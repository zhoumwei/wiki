"use strict";(self.webpackChunkmy_vuepress_site=self.webpackChunkmy_vuepress_site||[]).push([[5543],{577:(e,n,a)=>{a.r(n),a.d(n,{comp:()=>s,data:()=>t});var i=a(641);const l={},s=(0,a(6262).A)(l,[["render",function(e,n){return(0,i.uX)(),(0,i.CE)("div",null,[...n[0]||(n[0]=[(0,i.Fv)('<h1 id="llm-frequently-asked-questions" tabindex="-1"><a class="header-anchor" href="#llm-frequently-asked-questions"><span>LLM Frequently Asked Questions</span></a></h1><h2 id="table-of-contents" tabindex="-1"><a class="header-anchor" href="#table-of-contents"><span>Table of Contents</span></a></h2><ul><li><a href="#1-llm-basic-concepts">1. LLM Basic Concepts</a><ul><li><a href="#q1-what-is-a-large-language-model-llm">Q1: What is a Large Language Model (LLM)?</a></li><li><a href="#q2-llm-development-history">Q2: LLM Development History</a></li><li><a href="#q3-core-components-of-transformer-architecture">Q3: Core Components of Transformer Architecture</a></li></ul></li><li><a href="#2-llm-core-technologies">2. LLM Core Technologies</a><ul><li><a href="#q4-difference-between-pre-training-and-fine-tuning">Q4: Difference Between Pre-training and Fine-tuning</a></li><li><a href="#q5-what-is-prompt-engineering">Q5: What is Prompt Engineering?</a></li><li><a href="#q6-rlhf-reinforcement-learning-from-human-feedback-principle">Q6: RLHF (Reinforcement Learning from Human Feedback) Principle</a></li></ul></li><li><a href="#3-llm-application-practice">3. LLM Application Practice</a><ul><li><a href="#q7-challenges-of-llm-in-practical-applications">Q7: Challenges of LLM in Practical Applications</a></li><li><a href="#q8-how-to-evaluate-llm-performance">Q8: How to Evaluate LLM Performance?</a></li></ul></li><li><a href="#4-llm-development-tools-and-frameworks">4. LLM Development Tools and Frameworks</a><ul><li><a href="#q9-introduction-to-hugging-face-ecosystem">Q9: Introduction to Hugging Face Ecosystem</a></li><li><a href="#q10-how-to-use-hugging-face-for-model-inference">Q10: How to Use Hugging Face for Model Inference?</a></li></ul></li><li><a href="#5-llm-fine-tuning-and-optimization">5. LLM Fine-tuning and Optimization</a><ul><li><a href="#q11-what-are-parameter-efficient-fine-tuning-peft-methods">Q11: What are Parameter-Efficient Fine-tuning (PEFT) Methods?</a></li><li><a href="#q12-model-compression-and-acceleration-techniques">Q12: Model Compression and Acceleration Techniques?</a></li></ul></li><li><a href="#6-llm-application-cases">6. LLM Application Cases</a><ul><li><a href="#q13-llm-applications-in-different-fields">Q13: LLM Applications in Different Fields</a></li><li><a href="#q14-building-llm-based-application-systems">Q14: Building LLM-Based Application Systems</a></li></ul></li><li><a href="#7-llm-development-trends-and-learning-resources">7. LLM Development Trends and Learning Resources</a><ul><li><a href="#q15-future-development-directions-of-llm">Q15: Future Development Directions of LLM</a></li><li><a href="#q16-how-to-learn-llm-technology">Q16: How to Learn LLM Technology?</a></li></ul></li></ul><h2 id="_1-llm-basic-concepts" tabindex="-1"><a class="header-anchor" href="#_1-llm-basic-concepts"><span>1. LLM Basic Concepts</span></a></h2><h3 id="q1-what-is-a-large-language-model-llm" tabindex="-1"><a class="header-anchor" href="#q1-what-is-a-large-language-model-llm"><span>Q1: What is a Large Language Model (LLM)?</span></a></h3><p><strong>Answer:</strong> Large Language Model (LLM) is an artificial intelligence model based on deep learning that is trained on massive text data and can understand and generate human language.</p><p>Key characteristics:</p><ul><li><strong>Massive parameters</strong>: Usually contains billions or even hundreds of billions of parameters</li><li><strong>Self-supervised learning</strong>: Trained by predicting missing parts in text</li><li><strong>Versatility</strong>: Can handle various natural language tasks</li><li><strong>Context understanding</strong>: Can understand complex language structures and semantics</li></ul><h3 id="q2-llm-development-history" tabindex="-1"><a class="header-anchor" href="#q2-llm-development-history"><span>Q2: LLM Development History</span></a></h3><p><strong>Answer:</strong> LLM development has gone through several important stages:</p><ol><li><p><strong>Early Models</strong> (Around 2010)</p><ul><li>Word vector models like Word2Vec, GloVe</li><li>Sequence models like RNN, LSTM</li></ul></li><li><p><strong>Transformer Era</strong> (2017 to present)</p><ul><li>Google proposed Transformer architecture</li><li>Rise of BERT, GPT series models</li></ul></li><li><p><strong>Large Model Era</strong> (2020 to present)</p><ul><li>Billion-parameter models like GPT-3, PaLM</li><li>Conversational models like ChatGPT, Claude</li></ul></li></ol><h3 id="q3-core-components-of-transformer-architecture" tabindex="-1"><a class="header-anchor" href="#q3-core-components-of-transformer-architecture"><span>Q3: Core Components of Transformer Architecture</span></a></h3><p><strong>Answer:</strong> Core components of Transformer architecture include:</p><ol><li><p><strong>Self-Attention Mechanism</strong></p><ul><li>Calculates correlation between each position and other positions in the sequence</li><li>Allows the model to focus on different parts of the input sequence</li></ul></li><li><p><strong>Multi-Head Attention</strong></p><ul><li>Computes multiple attention heads in parallel</li><li>Captures different types of language relationships</li></ul></li><li><p><strong>Positional Encoding</strong></p><ul><li>Provides sequence order information to the model</li><li>Uses sine and cosine functions to encode positions</li></ul></li><li><p><strong>Feed-Forward Networks</strong></p><ul><li>Fully connected feed-forward networks</li><li>Processes each position independently</li></ul></li></ol><h2 id="_2-llm-core-technologies" tabindex="-1"><a class="header-anchor" href="#_2-llm-core-technologies"><span>2. LLM Core Technologies</span></a></h2><h3 id="q4-difference-between-pre-training-and-fine-tuning" tabindex="-1"><a class="header-anchor" href="#q4-difference-between-pre-training-and-fine-tuning"><span>Q4: Difference Between Pre-training and Fine-tuning?</span></a></h3><p><strong>Answer:</strong></p><p><strong>Pre-training</strong>:</p><ul><li>Train model on large-scale general data</li><li>Learn basic knowledge and patterns of language</li><li>Usually self-supervised learning tasks</li></ul><p><strong>Fine-tuning</strong>:</p><ul><li>Further train on small-scale data for specific tasks</li><li>Adapt model to specific application scenarios</li><li>Usually supervised learning tasks</li></ul><h3 id="q5-what-is-prompt-engineering" tabindex="-1"><a class="header-anchor" href="#q5-what-is-prompt-engineering"><span>Q5: What is Prompt Engineering?</span></a></h3><p><strong>Answer:</strong> Prompt Engineering is the technology of designing and optimizing prompts to guide LLM to generate desired outputs.</p><p>Key techniques:</p><ol><li><strong>Few-shot Learning</strong>: Provide few examples</li><li><strong>Chain-of-Thought</strong>: Guide model to show reasoning process</li><li><strong>Instruction Tuning</strong>: Optimize instruction format</li><li><strong>Role Playing</strong>: Set model role</li></ol><h3 id="q6-rlhf-reinforcement-learning-from-human-feedback-principle" tabindex="-1"><a class="header-anchor" href="#q6-rlhf-reinforcement-learning-from-human-feedback-principle"><span>Q6: RLHF (Reinforcement Learning from Human Feedback) Principle?</span></a></h3><p><strong>Answer:</strong> RLHF is a method to optimize LLM through human feedback:</p><ol><li><strong>Supervised Fine-tuning</strong>: Fine-tune model using manually annotated data</li><li><strong>Reward Model Training</strong>: Train reward model to evaluate generation quality</li><li><strong>Reinforcement Learning Optimization</strong>: Use PPO algorithm to optimize model parameters</li></ol><h2 id="_3-llm-application-practice" tabindex="-1"><a class="header-anchor" href="#_3-llm-application-practice"><span>3. LLM Application Practice</span></a></h2><h3 id="q7-challenges-of-llm-in-practical-applications" tabindex="-1"><a class="header-anchor" href="#q7-challenges-of-llm-in-practical-applications"><span>Q7: Challenges of LLM in Practical Applications?</span></a></h3><p><strong>Answer:</strong> LLM faces the following challenges in practical applications:</p><ol><li><strong>Computational Resources</strong>: Training and inference require massive computational resources</li><li><strong>Hallucination Problem</strong>: May generate false or inaccurate information</li><li><strong>Security</strong>: May be maliciously used to generate harmful content</li><li><strong>Controllability</strong>: Difficult to precisely control output content</li><li><strong>Timeliness</strong>: Knowledge cutoff at training data time point</li></ol><h3 id="q8-how-to-evaluate-llm-performance" tabindex="-1"><a class="header-anchor" href="#q8-how-to-evaluate-llm-performance"><span>Q8: How to Evaluate LLM Performance?</span></a></h3><p><strong>Answer:</strong> LLM performance evaluation can be conducted from multiple dimensions:</p><ol><li><strong>Language Understanding Ability</strong>: GLUE, SuperGLUE benchmark tests</li><li><strong>Generation Quality</strong>: BLEU, ROUGE automatic evaluation metrics</li><li><strong>Factual Accuracy</strong>: TruthfulQA factual testing</li><li><strong>Security</strong>: Adversarial testing and ethical evaluation</li><li><strong>Efficiency</strong>: Inference speed and resource consumption</li></ol><h2 id="_4-llm-development-tools-and-frameworks" tabindex="-1"><a class="header-anchor" href="#_4-llm-development-tools-and-frameworks"><span>4. LLM Development Tools and Frameworks</span></a></h2><h3 id="q9-introduction-to-hugging-face-ecosystem" tabindex="-1"><a class="header-anchor" href="#q9-introduction-to-hugging-face-ecosystem"><span>Q9: Introduction to Hugging Face Ecosystem?</span></a></h3><p><strong>Answer:</strong> Hugging Face is one of the most important open-source platforms in the LLM field:</p><p>Core components:</p><ol><li><p><strong>Transformers Library</strong>:</p><ul><li>Provides thousands of pre-trained models</li><li>Supports multiple modalities (text, image, audio)</li><li>Simple and easy-to-use API</li></ul></li><li><p><strong>Datasets Library</strong>:</p><ul><li>Large-scale dataset management tool</li><li>Supports multiple data formats</li><li>Efficient data processing capabilities</li></ul></li><li><p><strong>Tokenizers Library</strong>:</p><ul><li>Fast tokenization tool</li><li>Supports multiple tokenization algorithms</li><li>Seamless integration with Transformers</li></ul></li><li><p><strong>PEFT Library</strong>:</p><ul><li>Parameter-efficient fine-tuning methods</li><li>Technologies like LoRA, Adapter</li><li>Reduces fine-tuning costs</li></ul></li></ol><h3 id="q10-how-to-use-hugging-face-for-model-inference" tabindex="-1"><a class="header-anchor" href="#q10-how-to-use-hugging-face-for-model-inference"><span>Q10: How to Use Hugging Face for Model Inference?</span></a></h3><p><strong>Answer:</strong> Basic steps for using Hugging Face for model inference:</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipeline</span>\n<span class="line"></span>\n<span class="line"><span class="token comment"># 1. Create pipeline</span></span>\n<span class="line">generator <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">&#39;text-generation&#39;</span><span class="token punctuation">,</span> model<span class="token operator">=</span><span class="token string">&#39;gpt2&#39;</span><span class="token punctuation">)</span></span>\n<span class="line"></span>\n<span class="line"><span class="token comment"># 2. Execute inference</span></span>\n<span class="line">result <span class="token operator">=</span> generator<span class="token punctuation">(</span><span class="token string">&quot;Once upon a time&quot;</span><span class="token punctuation">,</span> max_length<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> num_return_sequences<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span></span>\n<span class="line"></span>\n<span class="line"><span class="token comment"># 3. Process results</span></span>\n<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">&#39;generated_text&#39;</span><span class="token punctuation">]</span><span class="token punctuation">)</span></span>\n<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> AutoModelForCausalLM</span>\n<span class="line"></span>\n<span class="line"><span class="token comment"># 1. Load model and tokenizer</span></span>\n<span class="line">tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">&quot;gpt2&quot;</span><span class="token punctuation">)</span></span>\n<span class="line">model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">&quot;gpt2&quot;</span><span class="token punctuation">)</span></span>\n<span class="line"></span>\n<span class="line"><span class="token comment"># 2. Encode input</span></span>\n<span class="line">input_ids <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token string">&quot;Hello, how are you?&quot;</span><span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">&quot;pt&quot;</span><span class="token punctuation">)</span></span>\n<span class="line"></span>\n<span class="line"><span class="token comment"># 3. Generate text</span></span>\n<span class="line">output <span class="token operator">=</span> model<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>input_ids<span class="token punctuation">,</span> max_length<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> num_beams<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> early_stopping<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></span>\n<span class="line"></span>\n<span class="line"><span class="token comment"># 4. Decode output</span></span>\n<span class="line">generated_text <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>output<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> skip_special_tokens<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></span>\n<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span>generated_text<span class="token punctuation">)</span></span>\n<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="_5-llm-fine-tuning-and-optimization" tabindex="-1"><a class="header-anchor" href="#_5-llm-fine-tuning-and-optimization"><span>5. LLM Fine-tuning and Optimization</span></a></h2><h3 id="q11-what-are-parameter-efficient-fine-tuning-peft-methods" tabindex="-1"><a class="header-anchor" href="#q11-what-are-parameter-efficient-fine-tuning-peft-methods"><span>Q11: What are Parameter-Efficient Fine-tuning (PEFT) Methods?</span></a></h3><p><strong>Answer:</strong> Parameter-efficient fine-tuning is a fine-tuning method that updates only a small number of parameters based on pre-trained models:</p><ol><li><p><strong>LoRA</strong> (Low-Rank Adaptation):</p><ul><li>Uses low-rank matrices to approximate weight updates</li><li>Dramatically reduces trainable parameter count</li><li>Maintains model performance</li></ul></li><li><p><strong>Adapter</strong>:</p><ul><li>Inserts small neural networks between model layers</li><li>Trains only Adapter module parameters</li><li>Easy to plug and combine</li></ul></li><li><p><strong>Prefix Tuning</strong>:</p><ul><li>Adds learnable prefix tokens before input</li><li>Fixes original model parameters</li><li>Suitable for generation tasks</li></ul></li><li><p><strong>Prompt Tuning</strong>:</p><ul><li>Optimizes input prompt representations</li><li>Keeps model parameters unchanged</li><li>Suitable for understanding tasks</li></ul></li></ol><h3 id="q12-model-compression-and-acceleration-techniques" tabindex="-1"><a class="header-anchor" href="#q12-model-compression-and-acceleration-techniques"><span>Q12: Model Compression and Acceleration Techniques?</span></a></h3><p><strong>Answer:</strong> Model compression and acceleration techniques include:</p><ol><li><p><strong>Quantization</strong>:</p><ul><li>Converts floating-point numbers to low-precision representation</li><li>Such as INT8, FP16 quantization</li><li>Reduces memory usage and computational overhead</li></ul></li><li><p><strong>Pruning</strong>:</p><ul><li>Removes unimportant weights or neurons</li><li>Structured pruning and unstructured pruning</li><li>Reduces parameters while maintaining model accuracy</li></ul></li><li><p><strong>Knowledge Distillation</strong>:</p><ul><li>Uses large models to guide small model training</li><li>Transfers &quot;dark knowledge&quot;</li><li>Obtains smaller and faster models</li></ul></li><li><p><strong>Model Architecture Optimization</strong>:</p><ul><li>Designs more efficient network structures</li><li>Such as MobileNet, EfficientNet</li><li>Balances performance and efficiency</li></ul></li></ol><h2 id="_6-llm-application-cases" tabindex="-1"><a class="header-anchor" href="#_6-llm-application-cases"><span>6. LLM Application Cases</span></a></h2><h3 id="q13-llm-applications-in-different-fields" tabindex="-1"><a class="header-anchor" href="#q13-llm-applications-in-different-fields"><span>Q13: LLM Applications in Different Fields?</span></a></h3><p><strong>Answer:</strong> LLM has wide applications in multiple fields:</p><ol><li><p><strong>Natural Language Processing</strong>:</p><ul><li>Machine translation, text summarization</li><li>Sentiment analysis, text classification</li><li>Question answering systems, dialogue systems</li></ul></li><li><p><strong>Code Generation and Understanding</strong>:</p><ul><li>Code assistants like GitHub Copilot</li><li>Code completion, bug detection</li><li>Code documentation generation</li></ul></li><li><p><strong>Content Creation</strong>:</p><ul><li>Article writing, creative generation</li><li>Marketing copy, social media content</li><li>Educational material generation</li></ul></li><li><p><strong>Scientific Research</strong>:</p><ul><li>Protein structure prediction</li><li>Drug discovery</li><li>Materials science</li></ul></li></ol><h3 id="q14-building-llm-based-application-systems" tabindex="-1"><a class="header-anchor" href="#q14-building-llm-based-application-systems"><span>Q14: Building LLM-Based Application Systems?</span></a></h3><p><strong>Answer:</strong> Building LLM-based application systems typically includes the following components:</p><ol><li><p><strong>Prompt Engineering Layer</strong>:</p><ul><li>Design effective prompt templates</li><li>Dynamically adjust prompt content</li><li>Handle context length limitations</li></ul></li><li><p><strong>Model Management Layer</strong>:</p><ul><li>Model version control</li><li>Performance monitoring and optimization</li><li>Cost control and scheduling</li></ul></li><li><p><strong>Data Processing Layer</strong>:</p><ul><li>Input data preprocessing</li><li>Output result post-processing</li><li>Data quality assurance</li></ul></li><li><p><strong>Application Interface Layer</strong>:</p><ul><li>API service interfaces</li><li>User interface integration</li><li>Third-party system integration</li></ul></li></ol><h2 id="_7-llm-development-trends-and-learning-resources" tabindex="-1"><a class="header-anchor" href="#_7-llm-development-trends-and-learning-resources"><span>7. LLM Development Trends and Learning Resources</span></a></h2><h3 id="q15-future-development-directions-of-llm" tabindex="-1"><a class="header-anchor" href="#q15-future-development-directions-of-llm"><span>Q15: Future Development Directions of LLM?</span></a></h3><p><strong>Answer:</strong> Future development directions of LLM include:</p><ol><li><strong>Model Efficiency Optimization</strong>: Model compression, quantization, distillation technologies</li><li><strong>Multimodal Fusion</strong>: Integration of vision, speech and other modalities</li><li><strong>Personalized Customization</strong>: Customization for specific users or domains</li><li><strong>Interpretability</strong>: Improving transparency of model decisions</li><li><strong>Green AI</strong>: Reducing energy consumption and improving environmental friendliness</li></ol><h3 id="q16-how-to-learn-llm-technology" tabindex="-1"><a class="header-anchor" href="#q16-how-to-learn-llm-technology"><span>Q16: How to Learn LLM Technology?</span></a></h3><p><strong>Answer:</strong> Recommended learning path for LLM technology:</p><ol><li><strong>Foundational Knowledge</strong>: Deep learning, natural language processing fundamentals</li><li><strong>Classic Papers</strong>: Core papers on Transformer, BERT, GPT</li><li><strong>Practice Projects</strong>: Experiment with tools like Hugging Face</li><li><strong>Open Source Community</strong>: Participate in open source projects, follow latest developments</li><li><strong>Continuous Learning</strong>: Track latest research from top conferences (NeurIPS, ICML)</li></ol><p><strong>Recommended Learning Resources</strong>:</p><ul><li><a href="https://huggingface.co/learn/llm-course/zh-CN/chapter1/1" target="_blank" rel="noopener noreferrer">Hugging Face Course</a>: Comprehensive LLM course</li><li><a href="https://learn.microsoft.com/zh-cn/shows/generative-ai-for-beginners/introduction-to-generative-ai-and-llms-generative-ai-for-beginners" target="_blank" rel="noopener noreferrer">Microsoft Generative AI for Beginners</a>: Microsoft&#39;s generative AI beginner course</li><li><a href="https://datawhalechina.github.io/llm-cookbook/#/" target="_blank" rel="noopener noreferrer">DataWhale LLM Cookbook</a>: Chinese LLM practice guide</li><li><a href="https://github.com/mlabonne/llm-course" target="_blank" rel="noopener noreferrer">ML Bonneau&#39;s LLM Course</a>: Practical LLM learning resources</li><li><a href="https://comfyai.app/about" target="_blank" rel="noopener noreferrer">Comfyai LLM Course</a>: Practical LLM learning resources</li></ul>',67)])])}]]),t=JSON.parse('{"path":"/tutorial/llm/llm-faq.html","title":"LLM Frequently Asked Questions","lang":"zh-CN","frontmatter":{},"headers":[{"level":2,"title":"Table of Contents","slug":"table-of-contents","link":"#table-of-contents","children":[]},{"level":2,"title":"1. LLM Basic Concepts","slug":"_1-llm-basic-concepts","link":"#_1-llm-basic-concepts","children":[{"level":3,"title":"Q1: What is a Large Language Model (LLM)?","slug":"q1-what-is-a-large-language-model-llm","link":"#q1-what-is-a-large-language-model-llm","children":[]},{"level":3,"title":"Q2: LLM Development History","slug":"q2-llm-development-history","link":"#q2-llm-development-history","children":[]},{"level":3,"title":"Q3: Core Components of Transformer Architecture","slug":"q3-core-components-of-transformer-architecture","link":"#q3-core-components-of-transformer-architecture","children":[]}]},{"level":2,"title":"2. LLM Core Technologies","slug":"_2-llm-core-technologies","link":"#_2-llm-core-technologies","children":[{"level":3,"title":"Q4: Difference Between Pre-training and Fine-tuning?","slug":"q4-difference-between-pre-training-and-fine-tuning","link":"#q4-difference-between-pre-training-and-fine-tuning","children":[]},{"level":3,"title":"Q5: What is Prompt Engineering?","slug":"q5-what-is-prompt-engineering","link":"#q5-what-is-prompt-engineering","children":[]},{"level":3,"title":"Q6: RLHF (Reinforcement Learning from Human Feedback) Principle?","slug":"q6-rlhf-reinforcement-learning-from-human-feedback-principle","link":"#q6-rlhf-reinforcement-learning-from-human-feedback-principle","children":[]}]},{"level":2,"title":"3. LLM Application Practice","slug":"_3-llm-application-practice","link":"#_3-llm-application-practice","children":[{"level":3,"title":"Q7: Challenges of LLM in Practical Applications?","slug":"q7-challenges-of-llm-in-practical-applications","link":"#q7-challenges-of-llm-in-practical-applications","children":[]},{"level":3,"title":"Q8: How to Evaluate LLM Performance?","slug":"q8-how-to-evaluate-llm-performance","link":"#q8-how-to-evaluate-llm-performance","children":[]}]},{"level":2,"title":"4. LLM Development Tools and Frameworks","slug":"_4-llm-development-tools-and-frameworks","link":"#_4-llm-development-tools-and-frameworks","children":[{"level":3,"title":"Q9: Introduction to Hugging Face Ecosystem?","slug":"q9-introduction-to-hugging-face-ecosystem","link":"#q9-introduction-to-hugging-face-ecosystem","children":[]},{"level":3,"title":"Q10: How to Use Hugging Face for Model Inference?","slug":"q10-how-to-use-hugging-face-for-model-inference","link":"#q10-how-to-use-hugging-face-for-model-inference","children":[]}]},{"level":2,"title":"5. LLM Fine-tuning and Optimization","slug":"_5-llm-fine-tuning-and-optimization","link":"#_5-llm-fine-tuning-and-optimization","children":[{"level":3,"title":"Q11: What are Parameter-Efficient Fine-tuning (PEFT) Methods?","slug":"q11-what-are-parameter-efficient-fine-tuning-peft-methods","link":"#q11-what-are-parameter-efficient-fine-tuning-peft-methods","children":[]},{"level":3,"title":"Q12: Model Compression and Acceleration Techniques?","slug":"q12-model-compression-and-acceleration-techniques","link":"#q12-model-compression-and-acceleration-techniques","children":[]}]},{"level":2,"title":"6. LLM Application Cases","slug":"_6-llm-application-cases","link":"#_6-llm-application-cases","children":[{"level":3,"title":"Q13: LLM Applications in Different Fields?","slug":"q13-llm-applications-in-different-fields","link":"#q13-llm-applications-in-different-fields","children":[]},{"level":3,"title":"Q14: Building LLM-Based Application Systems?","slug":"q14-building-llm-based-application-systems","link":"#q14-building-llm-based-application-systems","children":[]}]},{"level":2,"title":"7. LLM Development Trends and Learning Resources","slug":"_7-llm-development-trends-and-learning-resources","link":"#_7-llm-development-trends-and-learning-resources","children":[{"level":3,"title":"Q15: Future Development Directions of LLM?","slug":"q15-future-development-directions-of-llm","link":"#q15-future-development-directions-of-llm","children":[]},{"level":3,"title":"Q16: How to Learn LLM Technology?","slug":"q16-how-to-learn-llm-technology","link":"#q16-how-to-learn-llm-technology","children":[]}]}],"git":{"contributors":[{"name":"mingwzh","username":"mingwzh","email":"1127699551@qq.com","commits":1,"url":"https://github.com/mingwzh"}],"changelog":[{"hash":"889dca45bee9dd6823f04dc586ea5a3e17f6f61c","time":1769412347000,"email":"1127699551@qq.com","author":"mingwzh","message":"feat(tutorial): 添加LLM常见问题文档及示例布局代码"}]},"filePathRelative":"tutorial/llm/llm-faq.md"}')}}]);