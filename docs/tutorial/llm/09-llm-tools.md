# 09 Llm Tools   Index



# ğŸ’¡ è¿™èŠ‚è¯¾ä¼šå¸¦ç»™ä½ 

1. ç³»ç»Ÿæ€§ç»´æŠ¤ã€æµ‹è¯•ã€ç›‘æ§ä¸€ä¸ª LLM åº”ç”¨
2. å­¦ä¹ ä½¿ç”¨ä¸»æµçš„å·¥å…·å®Œæˆä¸Šè¿°å·¥ä½œ

## ğŸ“ è¿™èŠ‚è¯¾æ€ä¹ˆå­¦

ä»£ç èƒ½åŠ›è¦æ±‚ï¼š**ä¸­é«˜**ï¼ŒAI/æ•°å­¦åŸºç¡€è¦æ±‚ï¼š**ä½**

1. æœ‰ç¼–ç¨‹åŸºç¡€çš„åŒå­¦
   - ä»è½¯ä»¶å·¥ç¨‹è§’åº¦ä½“ä¼šä¸€ä¸ª AI åº”ç”¨çš„å¼€å‘ä¸ç»´æŠ¤æµç¨‹
2. æ²¡æœ‰ç¼–ç¨‹åŸºç¡€çš„åŒå­¦
   - äº†è§£ä¸€ä¸ª AI åº”ç”¨å¼€å‘ä¸ç»´æŠ¤è¿‡ç¨‹ä¸­æ¶‰åŠåˆ°çš„æŠ€æœ¯ä¸é—®é¢˜

## ç»´æŠ¤ä¸€ä¸ªç”Ÿäº§çº§çš„ LLM åº”ç”¨ï¼Œæˆ‘ä»¬éœ€è¦åšä»€ä¹ˆï¼Ÿ

1. å„ç§æŒ‡æ ‡ç›‘æ§ä¸ç»Ÿè®¡ï¼šè®¿é—®è®°å½•ã€å“åº”æ—¶é•¿ã€Token ç”¨é‡ã€è®¡è´¹ç­‰ç­‰
2. è°ƒè¯• Prompt
3. æµ‹è¯•/éªŒè¯ç³»ç»Ÿçš„ç›¸å…³è¯„ä¼°æŒ‡æ ‡
4. æ•°æ®é›†ç®¡ç†ï¼ˆä¾¿äºå›å½’æµ‹è¯•ï¼‰
5. Prompt ç‰ˆæœ¬ç®¡ç†ï¼ˆä¾¿äºå‡çº§/å›æ»šï¼‰

## é’ˆå¯¹ä»¥ä¸Šéœ€æ±‚ï¼Œæˆ‘ä»¬ä»‹ç»ä¸¤ä¸ªç”Ÿäº§çº§ LLM App ç»´æŠ¤å¹³å°

1. **LangFuse**: å¼€æº + SaaSï¼ˆå…è´¹/ä»˜è´¹ï¼‰ï¼ŒLangSmith å¹³æ›¿ï¼Œå¯é›†æˆ LangChain ä¹Ÿå¯ç›´æ¥å¯¹æ¥ OpenAI APIï¼›
2. **LangSmith**: LangChain çš„å®˜æ–¹å¹³å°ï¼ŒSaaS æœåŠ¡ï¼ˆå…è´¹/ä»˜è´¹ï¼‰ï¼Œéå¼€æºï¼Œä¼ä¸šç‰ˆæ”¯æŒç§æœ‰éƒ¨ç½²ï¼›

## 1ã€LangFuse

å¼€æºï¼Œæ”¯æŒ LangChain é›†æˆæˆ–åŸç”Ÿ OpenAI API é›†æˆ

å®˜æ–¹ç½‘ç«™ï¼šhttps://langfuse.com/

é¡¹ç›®åœ°å€ï¼šhttps://github.com/langfuse

æ–‡æ¡£åœ°å€ï¼šhttps://langfuse.com/docs

APIæ–‡æ¡£ï¼šhttps://api.reference.langfuse.com/

  - Python SDK: https://python.reference.langfuse.com/

  - JS SDK: https://js.reference.langfuse.com/

1. é€šè¿‡å®˜æ–¹äº‘æœåŠ¡ä½¿ç”¨ï¼š
   - æ³¨å†Œ: cloud.langfuse.com
   - åˆ›å»º API Key

```sh
LANGFUSE_SECRET_KEY="sk-lf-..."
LANGFUSE_PUBLIC_KEY="pk-lf-..."
LANGFUSE_HOST="https://cloud.langfuse.com" # EU æœåŠ¡å™¨
# LANGFUSE_HOST="https://us.cloud.langfuse.com" # US æœåŠ¡å™¨
```

2. é€šè¿‡ Docker æœ¬åœ°éƒ¨ç½²

```sh
# Clone repository
git clone https://github.com/langfuse/langfuse.git
cd langfuse

# Run server and db
docker compose up -d
```

```sh
# åœ¨è‡ªå·±éƒ¨ç½²çš„ç³»ç»Ÿä¸­ç”Ÿæˆä¸Šè¿°ä¸¤ä¸ª KEY
# å¹¶åœ¨ç¯å¢ƒå˜é‡ä¸­æŒ‡å®šæœåŠ¡åœ°å€

LANGFUSE_SECRET_KEY="sk-lf-..."
LANGFUSE_PUBLIC_KEY="pk-lf-.."
LANGFUSE_HOST="http://localhost:3000"

```

```python
# !pip install --upgrade langfuse
```

### 1.1ã€é€šè¿‡è£…é¥°å™¨è®°å½•

```python
from langfuse.decorators import observe, langfuse_context
from langfuse.openai import openai # OpenAI integration

@observe()
def run():
    return openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
          {"role": "user", "content": "å¯¹æˆ‘è¯´Hello, World!"}
        ],
    ).choices[0].message.content
 
print(run())
langfuse_context.flush()
```

**Output:**
```
Hello, World! Nice to meet you!
```

### 1.1.1ã€å‡ ä¸ªåŸºæœ¬æ¦‚å¿µ

- Trace ä¸€èˆ¬è¡¨ç¤ºç”¨æˆ·ä¸ç³»ç»Ÿçš„ä¸€æ¬¡äº¤äº’ï¼Œå…¶ä¸­è®°å½•è¾“å…¥ã€è¾“å‡ºï¼Œä¹ŸåŒ…æ‹¬è‡ªå®šä¹‰çš„ metadata æ¯”å¦‚ç”¨æˆ·åã€session id ç­‰ï¼›
- ä¸€ä¸ª trace å†…éƒ¨å¯ä»¥åŒ…å«å¤šä¸ªå­è¿‡ç¨‹ï¼Œè¿™é‡Œå« observarionsï¼›
- Observation å¯ä»¥æ˜¯å¤šä¸ªç±»å‹ï¼š
  - Event æ˜¯æœ€åŸºæœ¬çš„å•å…ƒï¼Œç”¨äºè®°å½•ä¸€ä¸ª trace ä¸­çš„æ¯ä¸ªäº‹ä»¶ï¼›
  - Span è¡¨ä¸€ä¸ª trace ä¸­çš„ä¸€ä¸ª"è€—æ—¶"çš„è¿‡ç¨‹ï¼›
  - Generation æ˜¯ç”¨äºè®°å½•ä¸ AI æ¨¡å‹äº¤äº’çš„ spanï¼Œä¾‹å¦‚ï¼šè°ƒç”¨ embedding æ¨¡å‹ã€è°ƒç”¨ LLMã€‚
- Observation å¯ä»¥åµŒå¥—ä½¿ç”¨ã€‚

![image](./assets/09-llm-tools/span.png)

### 1.1.2ã€`observe()` è£…é¥°å™¨çš„å‚æ•°

```python
def observe(
	self,
	*,
	name: Optional[str] = None, # Trace æˆ– Span çš„åç§°ï¼Œé»˜è®¤ä¸ºå‡½æ•°å
	as_type: Optional[Literal['generation']] = None, # å°†è®°å½•å®šä¹‰ä¸º Observation (LLM è°ƒç”¨ï¼‰
	capture_input: bool = True, # è®°å½•è¾“å…¥
	capture_output: bool = True, # è®°å½•è¾“å‡º
	transform_to_string: Optional[Callable[[Iterable], str]] = None # å°†è¾“å‡ºè½¬ä¸º string
) -> Callable[[~F], ~F]:
```

```python
from langfuse.decorators import observe, langfuse_context
from langfuse.openai import openai

@observe(name="HelloWorld")
def run():
    return openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
          {"role": "user", "content": "å¯¹æˆ‘è¯´Hello, World!"}
        ],
    ).choices[0].message.content
 
print(run())
langfuse_context.flush()
```

**Output:**
```
Hello, World! Nice to meet you!
```

### 1.1.3ã€é€šè¿‡ `langfuse_context` è®°å½• User IDã€Metadata ç­‰

```python
from langfuse.decorators import observe, langfuse_context
from langfuse.openai import openai # OpenAI integration
 
@observe()
def run():
    langfuse_context.update_current_trace(
        name="HelloWorld",
        user_id="wzr",
        tags=["test","demo"]
    )
    return openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
          {"role": "user", "content": "å¯¹æˆ‘è¯´Hello, World!"}
        ],
    ).choices[0].message.content
 
print(run())
langfuse_context.flush()
```

**Output:**
```
Hello, World! Nice to meet you.
```

### 1.2ã€é€šè¿‡ LangChain çš„å›è°ƒé›†æˆ

```python
# æ¸…ç†ç¯å¢ƒï¼Œé¿å…é‡å¤è®°å½•
del openai
```

```python
from langchain.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnablePassthrough

model = ChatOpenAI(model="gpt-3.5-turbo")

prompt = ChatPromptTemplate.from_messages([
    HumanMessagePromptTemplate.from_template("Say hello to {input}!")
])


# å®šä¹‰è¾“å‡ºè§£æå™¨
parser = StrOutputParser()

chain = (
    {"input": RunnablePassthrough()}
    | prompt
    | model
    | parser
)
```

```python
from langfuse.decorators import langfuse_context, observe

@observe()
def run():
    langfuse_context.update_current_trace(
            name="LangChainDemo",
            user_id="wzr",
        )
    
    # è·å–å½“å‰ LangChain å›è°ƒå¤„ç†å™¨
    langfuse_handler = langfuse_context.get_current_langchain_handler()
    
    return chain.invoke(input="AGIClass", config={"callbacks": [langfuse_handler]})

print(run())
langfuse_context.flush() # Langfuse å›ä¼ è®°å½•æ˜¯å¼‚æ­¥çš„ï¼Œå¯ä»¥é€šè¿‡ flush å¼ºåˆ¶æ›´æ–°
```

**Output:**
```
Hello AGIClass! How can I assist you today?
```

#### æ¢ä¸ªæ¨¡å‹è¯•è¯•

```python
# å…¶å®ƒæ¨¡å‹åˆ†è£…åœ¨ langchain_community åº•åŒ…ä¸­
from langchain_community.chat_models import QianfanChatEndpoint
from langchain_core.messages import HumanMessage
import os

ernie_model = QianfanChatEndpoint(
    qianfan_ak=os.getenv('ERNIE_CLIENT_ID'),
    qianfan_sk=os.getenv('ERNIE_CLIENT_SECRET')
)

chain = (
    {"input": RunnablePassthrough()}
    | prompt
    | ernie_model
    | parser
)

@observe()
def run():
    langfuse_context.update_current_trace(
            name="ErnieDemo",
            user_id="wzr",
        )
    
    # è·å–å½“å‰ LangChain å›è°ƒå¤„ç†å™¨
    langfuse_handler = langfuse_context.get_current_langchain_handler()
    
    return chain.invoke(input="ç‹å“ç„¶", config={"callbacks": [langfuse_handler]})

print(run())
langfuse_context.flush()
```

**Output:**
```
Langfuse was not able to parse the LLM model. The LLM call will be recorded without model name. Please create an issue: https://github.com/langfuse/langfuse/issues/new/choose
[INFO][2024-10-15 12:42:11.383] oauth.py:228 [t:140628858189632]: trying to refresh access_token for ak `cuTPS7***`
[INFO][2024-10-15 12:42:12.265] oauth.py:243 [t:140628858189632]: sucessfully refresh access_token
Hello, Wang Zhuoran!
```

### 1.3ã€æ„å»ºä¸€ä¸ªå®é™…åº”ç”¨

**AGI è¯¾å ‚è·Ÿè¯¾åŠ©æ‰‹**ï¼Œæ ¹æ®è¯¾ç¨‹å†…å®¹ï¼Œåˆ¤æ–­å­¦ç”Ÿé—®é¢˜æ˜¯å¦éœ€è¦è€å¸ˆè§£ç­”

1. åˆ¤æ–­è¯¥é—®é¢˜æ˜¯å¦éœ€è¦è€å¸ˆè§£ç­”ï¼Œå›å¤'Y'æˆ–'N'
2. åˆ¤æ–­è¯¥é—®é¢˜æ˜¯å¦å·²æœ‰åŒå­¦é—®è¿‡

```python
# æ„å»º PromptTemplate
from langchain.prompts import PromptTemplate

need_answer = PromptTemplate.from_template("""
*********
ä½ æ˜¯AIGCè¯¾ç¨‹çš„åŠ©æ•™ï¼Œä½ çš„å·¥ä½œæ˜¯ä»å­¦å‘˜çš„è¯¾å ‚äº¤æµä¸­é€‰æ‹©å‡ºéœ€è¦è€å¸ˆå›ç­”çš„é—®é¢˜ï¼ŒåŠ ä»¥æ•´ç†ä»¥äº¤ç»™è€å¸ˆå›ç­”ã€‚
 
è¯¾ç¨‹å†…å®¹:
{outlines}
*********
å­¦å‘˜è¾“å…¥:
{user_input}
*********
å¦‚æœè¿™æ˜¯ä¸€ä¸ªéœ€è¦è€å¸ˆç­”ç–‘çš„é—®é¢˜ï¼Œå›å¤Yï¼Œå¦åˆ™å›å¤Nã€‚
åªå›å¤Yæˆ–Nï¼Œä¸è¦å›å¤å…¶ä»–å†…å®¹ã€‚""")

check_duplicated = PromptTemplate.from_template("""
*********
å·²æœ‰æé—®åˆ—è¡¨:
[
{question_list}
]
*********
æ–°æé—®:
{user_input}
*********
å·²æœ‰æé—®åˆ—è¡¨æ˜¯å¦æœ‰å’Œæ–°æé—®ç±»ä¼¼çš„é—®é¢˜? å›å¤Yæˆ–N, Yè¡¨ç¤ºæœ‰ï¼ŒNè¡¨ç¤ºæ²¡æœ‰ã€‚
åªå›å¤Yæˆ–Nï¼Œä¸è¦å›å¤å…¶ä»–å†…å®¹ã€‚""")
```

```python
outlines = """
LangChain
æ¨¡å‹ I/O å°è£…
æ¨¡å‹çš„å°è£…
æ¨¡å‹çš„è¾“å…¥è¾“å‡º
PromptTemplate
OutputParser
æ•°æ®è¿æ¥å°è£…
æ–‡æ¡£åŠ è½½å™¨ï¼šDocument Loaders
æ–‡æ¡£å¤„ç†å™¨
å†…ç½®RAGï¼šRetrievalQA
è®°å¿†å°è£…ï¼šMemory
é“¾æ¶æ„ï¼šChain/LCEL
å¤§æ¨¡å‹æ—¶ä»£çš„è½¯ä»¶æ¶æ„ï¼šAgent
ReAct
SelfAskWithSearch
LangServe
LangChain.js
"""

question_list = [
    "LangChainå¯ä»¥å•†ç”¨å—",
    "LangChainå¼€æºå—",
]
```

```python
# åˆ›å»º chain
model = ChatOpenAI(temperature=0, seed=42)
parser = StrOutputParser()

need_answer_chain = (
    need_answer
    | model
    | parser
)

is_duplicated_chain = (
    check_duplicated
    | model
    | parser
)
```

### 1.3.1ã€ç”¨ Trace è®°å½•ä¸€ä¸ªå¤šæ¬¡è°ƒç”¨ LLM çš„è¿‡ç¨‹

```python
import uuid
from langfuse.decorators import langfuse_context, observe

# ä¸»æµç¨‹
@observe()
def verify_question(
    question: str,
    outlines: str,
    question_list: list,
    user_id: str,
) -> bool:
    langfuse_context.update_current_trace(
            name="AGIClassAssistant",
            user_id=user_id,
        )
    
    # get the langchain handler for the current trace
    langfuse_handler = langfuse_context.get_current_langchain_handler()
    # åˆ¤æ–­æ˜¯å¦éœ€è¦å›ç­”
    if need_answer_chain.invoke(
        {"user_input": question, "outlines": outlines},
        config={"callbacks": [langfuse_handler]}
    ) == 'Y':
        # åˆ¤æ–­æ˜¯å¦ä¸ºé‡å¤é—®é¢˜
        if is_duplicated_chain.invoke(
            {"user_input": question,
                "question_list": "\n".join(question_list)},
            config={"callbacks": [langfuse_handler]}
        ) == 'N':
            question_list.append(question)
            return True
    return False
```

```python
# å®é™…è°ƒç”¨
ret = verify_question(
    # "LangChainæ”¯æŒJavaå—",
    "è€å¸ˆå¥½",
    outlines,
    question_list,
    user_id="wzr",
)
print(ret)
langfuse_context.flush()
```

**Output:**
```
False
```

> âœ… **Tip:** ä¸Šé¢çš„å®ç°æ˜¯ä¸ºäº†æ¼”ç¤º  trace å’Œ span çš„æ¦‚å¿µã€‚å®é™…ä¸‹é¢çš„å®ç°æ–¹å¼æ›´ä¼˜ã€‚

```python
import numpy as np
import openai

cache = {}

@observe(as_type="generation")
def get_embeddings(text):
    '''å°è£… OpenAI çš„ Embedding æ¨¡å‹æ¥å£'''
    if text in cache: 
        # å¦‚æœå·²ç»åœ¨ç¼“å­˜ä¸­ï¼Œä¸å†é‡å¤è°ƒç”¨ï¼ˆèŠ‚çœæ—¶é—´ã€è´¹ç”¨ï¼‰
        return cache[text]
    data = openai.embeddings.create(
        input=[text], 
        model="text-embedding-3-small",
        dimensions=256
    ).data
    cache[text] = data[0].embedding
    return data[0].embedding

@observe()
def cos_sim(v, m):
    '''è®¡ç®—cosineç›¸ä¼¼åº¦'''
    score = np.dot(m, v)/(np.linalg.norm(m, axis=1)*np.linalg.norm(v))
    return score.tolist()

@observe()
def check_duplicated(query, existing, threshold=0.825):
    '''é€šè¿‡cosineç›¸ä¼¼åº¦é˜ˆå€¼åˆ¤æ–­æ˜¯å¦é‡å¤'''
    query_vec = np.array(get_embeddings(query))
    mat = np.array([item[1] for item in existing])
    cos = cos_sim(query_vec, mat)
    return max(cos) >= threshold

@observe()
def need_answer(question, outlints):
    '''åˆ¤æ–­æ˜¯å¦éœ€è¦å›ç­”'''
    langfuse_handler = langfuse_context.get_current_langchain_handler()
    return need_answer_chain.invoke(
        {"user_input": question, "outlines": outlines},
        config={"callbacks": [langfuse_handler]}
    ) == 'Y'
```

```python
# å‡è®¾ å·²æœ‰é—®é¢˜
question_list = [
    ("LangChainå¯ä»¥å•†ç”¨å—", get_embeddings("LangChainå¯ä»¥å•†ç”¨å—")),
    ("LangChainå¼€æºå—", get_embeddings("LangChainå¼€æºå—")),
]
```

```python
@observe()
def verify_question(
    question: str,
    outlines: str,
    question_list: list,
    user_id,
) -> bool:
    
    langfuse_context.update_current_trace(
        name="AGIClassAssistant2",
        user_id=user_id,
    )
    
    # åˆ¤æ–­æ˜¯å¦éœ€è¦å›ç­”
    if need_answer(question,outlines):
        # åˆ¤æ–­æ˜¯å¦é‡å¤
        if not check_duplicated(question, question_list):
            vec = cache[question]
            question_list.append((question,vec))
            return True
    return False
```

```python
ret = verify_question(
    # "LangChainæ”¯æŒJavaå—",
    "LangChainæœ‰å•†ç”¨è®¸å¯å—",
    outlines,
    question_list,
    user_id="wzr"
)
print(ret)
langfuse_context.flush()
```

**Output:**
```
False
```

### 1.3.2ã€ç”¨ Session è®°å½•ä¸€ä¸ªç”¨æˆ·çš„å¤šè½®å¯¹è¯

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import (
    AIMessage,  # ç­‰ä»·äºOpenAIæ¥å£ä¸­çš„assistant role
    HumanMessage,  # ç­‰ä»·äºOpenAIæ¥å£ä¸­çš„user role
    SystemMessage  # ç­‰ä»·äºOpenAIæ¥å£ä¸­çš„system role
)
from datetime import datetime
from langfuse.decorators import langfuse_context, observe

now = datetime.now()

llm = ChatOpenAI()

messages = [
    SystemMessage(content="ä½ æ˜¯AGIClassçš„è¯¾ç¨‹åŠ©ç†ã€‚"),
]

session_id = "chat-"+now.strftime("%d/%m/%Y %H:%M:%S")

@observe()
def chat_one_turn(user_input, user_id, turn_id):
    langfuse_context.update_current_trace(
        name=f"ChatTurn{turn_id}",
        user_id=user_id,
        session_id=session_id
    )
    langfuse_handler = langfuse_context.get_current_langchain_handler()
    messages.append(HumanMessage(content=user_input))
    response = llm.invoke(messages, config={"callbacks": [langfuse_handler]})
    messages.append(response)
    return response.content
```

```python
user_id="wzr"
turn_id = 0
while True:
    user_input = input("User: ")
    if user_input.strip() == "":
        break
    reply = chat_one_turn(user_input, user_id, turn_id)
    print("AI: "+reply)
    turn_id += 1
    langfuse_context.flush()
```

**Output:**
```
User:  ä½ å¥½
AI: ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ
User:  ä½ æ˜¯è°
AI: æˆ‘æ˜¯AGIClassçš„è¯¾ç¨‹åŠ©ç†ï¼Œæˆ‘å¯ä»¥å¸®åŠ©å›ç­”å…³äºè¯¾ç¨‹å†…å®¹å’Œå­¦ä¹ çš„é—®é¢˜ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ
User:
```

### 1.4ã€æ•°æ®é›†ä¸æµ‹è¯•

### 1.4.1ã€åœ¨çº¿æ ‡æ³¨

![image](./assets/09-llm-tools/annotation.png)

### 1.4.2ã€ä¸Šä¼ å·²æœ‰æ•°æ®é›†

```python
import json

# è°ƒæ•´æ•°æ®æ ¼å¼ {"input":{...},"expected_output":"label"}
data = []
with open('my_annotations.jsonl', 'r', encoding='utf-8') as fp:
    for line in fp:
        example = json.loads(line.strip())
        item = {
            "input": {
                "outlines": example["outlines"],
                "user_input": example["user_input"]
            },
            "expected_output": example["label"]
        }
        data.append(item)
```

```python
from langfuse import Langfuse
from langfuse.model import CreateDatasetRequest, CreateDatasetItemRequest
from tqdm import tqdm
import langfuse


dataset_name = "my-dataset"

# åˆå§‹åŒ–å®¢æˆ·ç«¯
langfuse=Langfuse()

# åˆ›å»ºæ•°æ®é›†ï¼Œå¦‚æœå·²å­˜åœ¨ä¸ä¼šé‡å¤åˆ›å»º
try:
    langfuse.create_dataset(
        name=dataset_name,
        # optional description
        description="My first dataset",
        # optional metadata
        metadata={
            "author": "wzr",
            "type": "demo"
        }
    )
except:
    pass

# è€ƒè™‘æ¼”ç¤ºè¿è¡Œé€Ÿåº¦ï¼Œåªä¸Šä¼ å‰50æ¡æ•°æ®
for item in tqdm(data[:50]):
    langfuse.create_dataset_item(
        dataset_name="my-dataset",
        input=item["input"],
        expected_output=item["expected_output"]
    )
```

**Output:**
```
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:11<00:00,  4.48it/s]
```

### 1.4.3ã€å®šä¹‰è¯„ä¼°å‡½æ•°

```python
def simple_evaluation(output, expected_output):
    return output == expected_output
```

### 1.4.4ã€è¿è¡Œæµ‹è¯•

Prompt æ¨¡æ¿ä¸ Chainï¼ˆLCELï¼‰

```python
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

need_answer = PromptTemplate.from_template("""
*********
ä½ æ˜¯AIGCè¯¾ç¨‹çš„åŠ©æ•™ï¼Œä½ çš„å·¥ä½œæ˜¯ä»å­¦å‘˜çš„è¯¾å ‚äº¤æµä¸­é€‰æ‹©å‡ºéœ€è¦è€å¸ˆå›ç­”çš„é—®é¢˜ï¼ŒåŠ ä»¥æ•´ç†ä»¥äº¤ç»™è€å¸ˆå›ç­”ã€‚
 
è¯¾ç¨‹å†…å®¹:
{outlines}
*********
å­¦å‘˜è¾“å…¥:
{user_input}
*********
å¦‚æœè¿™æ˜¯ä¸€ä¸ªéœ€è¦è€å¸ˆç­”ç–‘çš„é—®é¢˜ï¼Œå›å¤Yï¼Œå¦åˆ™å›å¤Nã€‚
åªå›å¤Yæˆ–Nï¼Œä¸è¦å›å¤å…¶ä»–å†…å®¹ã€‚""")

model = ChatOpenAI(temperature=0, seed=42)
parser = StrOutputParser()

chain_v1 = (
    need_answer
    | model
    | parser
)
```

åœ¨æ•°æ®é›†ä¸Šæµ‹è¯•æ•ˆæœ

```python
from concurrent.futures import ThreadPoolExecutor
import threading
from langfuse import Langfuse
from datetime import datetime

langfuse = Langfuse()
lock = threading.Lock()

def run_evaluation(chain, dataset_name, run_name):
    dataset = langfuse.get_dataset(dataset_name)

    def process_item(item):
        with lock:
            # æ³¨æ„ï¼šå¤šçº¿ç¨‹è°ƒç”¨æ­¤å¤„è¦åŠ é”ï¼Œå¦åˆ™ä¼šæœ‰idå†²çªå¯¼è‡´ä¸¢æ•°æ®
            handler = item.get_langchain_handler(run_name=run_name)

        # Assuming chain.invoke is a synchronous function
        output = chain.invoke(item.input, config={"callbacks": [handler]})

        # Assuming handler.root_span.score is a synchronous function
        handler.trace.score(
            name="accuracy",
            value=simple_evaluation(output, item.expected_output)
        )
        print('.', end='', flush=True)

    # for item in dataset.items:
    #    process_item(item)

    with ThreadPoolExecutor(max_workers=4) as executor:
        executor.map(process_item, dataset.items)
```

```python
run_evaluation(chain_v1, "my-dataset", "v1-"+datetime.now().strftime("%d/%m/%Y %H:%M:%S"))

# ä¿è¯å…¨éƒ¨æ•°æ®åŒæ­¥åˆ°äº‘ç«¯
langfuse_context.flush()
```

**Output:**
```
..................................................
```

### 1.4.5ã€Prompt è°ƒä¼˜ä¸å›å½’æµ‹è¯•

ä¼˜åŒ– Promptï¼šè¯•è¯•æ€ç»´é“¾ï¼ˆå›å¿†[ç¬¬äºŒè¯¾](../02-prompt/index.ipynb)ï¼‰

```python
from langchain.prompts import PromptTemplate

need_answer = PromptTemplate.from_template("""
*********
ä½ æ˜¯AIGCè¯¾ç¨‹çš„åŠ©æ•™ï¼Œä½ çš„å·¥ä½œæ˜¯ä»å­¦å‘˜çš„è¯¾å ‚äº¤æµä¸­é€‰æ‹©å‡ºéœ€è¦è€å¸ˆå›ç­”çš„é—®é¢˜ï¼ŒåŠ ä»¥æ•´ç†ä»¥äº¤ç»™è€å¸ˆå›ç­”ã€‚

ä½ çš„é€‰æ‹©éœ€è¦éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š
1 éœ€è¦è€å¸ˆå›ç­”çš„é—®é¢˜æ˜¯æŒ‡ä¸è¯¾ç¨‹å†…å®¹æˆ–AI/LLMç›¸å…³çš„æŠ€æœ¯é—®é¢˜ï¼›
2 è¯„è®ºæ€§çš„è§‚ç‚¹ã€é—²èŠã€è¡¨è¾¾æ¨¡ç³Šä¸æ¸…çš„å¥å­ï¼Œä¸éœ€è¦è€å¸ˆå›ç­”ï¼›
3 å­¦ç”Ÿè¾“å…¥ä¸æ„æˆç–‘é—®å¥çš„ï¼Œä¸éœ€è¦è€å¸ˆå›ç­”ï¼›
4 å­¦ç”Ÿé—®é¢˜ä¸­å¦‚æœç”¨â€œè¿™â€ã€â€œé‚£â€ç­‰ä»£è¯æŒ‡ä»£ï¼Œä¸ç®—è¡¨è¾¾æ¨¡ç³Šä¸æ¸…ï¼Œè¯·æ ¹æ®é—®é¢˜å†…å®¹åˆ¤æ–­æ˜¯å¦éœ€è¦è€å¸ˆå›ç­”ã€‚
 
è¯¾ç¨‹å†…å®¹:
{outlines}
*********
å­¦å‘˜è¾“å…¥:
{user_input}
*********
Analyse the student's input according to the lecture's contents and your criteria.
Output your analysis process step by step.
Finally, output a single letter Y or N in a separate line.
Y means that the input needs to be answered by the teacher.
N means that the input does not needs to be answered by the teacher.""")
```

```python
from langchain_core.output_parsers import BaseOutputParser
import re


class MyOutputParser(BaseOutputParser):
    """è‡ªå®šä¹‰parserï¼Œä»æ€ç»´é“¾ä¸­å–å‡ºæœ€åçš„Y/N"""

    def parse(self, text: str) -> str:
        matches = re.findall(r'[YN]', text)
        return matches[-1] if matches else 'N'
```

```python
chain_v2 = (
    need_answer
    | model
    | MyOutputParser()
)
```

å›å½’æµ‹è¯•

```python
run_evaluation(chain_v2, "my-dataset", "cot-"+datetime.now().strftime("%d/%m/%Y %H:%M:%S"))

# ä¿è¯å…¨éƒ¨æ•°æ®åŒæ­¥åˆ°äº‘ç«¯
langfuse_context.flush()
```

**Output:**
```
..................................................
```

### 1.5ã€Prompt ç‰ˆæœ¬ç®¡ç†

![image](./assets/09-llm-tools/prompt_management.png)

ç›®å‰åªæ”¯æŒ Langfuse è‡ªå·±çš„ SDK

```python
from langfuse import Langfuse

langfuse = Langfuse()

# æŒ‰åç§°åŠ è½½
prompt = langfuse.get_prompt("need_answer_v1")

# æŒ‰åç§°å’Œç‰ˆæœ¬å·åŠ è½½
prompt = langfuse.get_prompt("need_answer_v1", version=2)

# å¯¹æ¨¡æ¿ä¸­çš„å˜é‡èµ‹å€¼
compiled_prompt = prompt.compile(input="è€å¸ˆå¥½", outlines="test")

print(compiled_prompt)
```

**Output:**
```
*********
ä½ æ˜¯AIGCè¯¾ç¨‹çš„åŠ©æ•™ï¼Œä½ çš„å·¥ä½œæ˜¯ä»å­¦å‘˜çš„è¯¾å ‚äº¤æµä¸­é€‰æ‹©å‡ºéœ€è¦è€å¸ˆå›ç­”çš„é—®é¢˜ï¼ŒåŠ ä»¥æ•´ç†ä»¥äº¤ç»™è€å¸ˆå›ç­”ã€‚
 
è¯¾ç¨‹å†…å®¹:
test
*********
å­¦å‘˜è¾“å…¥:
è€å¸ˆå¥½
*********
å¦‚æœè¿™æ˜¯ä¸€ä¸ªéœ€è¦è€å¸ˆç­”ç–‘çš„é—®é¢˜ï¼Œå›å¤Yï¼Œå¦åˆ™å›å¤Nã€‚
åªå›å¤Yæˆ–Nï¼Œä¸è¦å›å¤å…¶ä»–å†…å®¹ã€‚
```

```python
# è·å– config

prompt = langfuse.get_prompt("need_answer_v1", version=5)

print(prompt.config)
```

**Output:**
```
{'temperature': 0}
```

### 1.6ã€å¦‚ä½•æ¯”è¾ƒä¸¤ä¸ªå¥å­çš„ç›¸ä¼¼æ€§ï¼šä¸€äº›ç»å…¸ NLP çš„è¯„æµ‹æ–¹æ³•ï¼ˆé€‰ï¼‰

1. **ç¼–è¾‘è·ç¦»**ï¼šä¹Ÿå«è±æ–‡æ–¯å¦è·ç¦»(Levenshtein),æ˜¯é’ˆå¯¹äºŒä¸ªå­—ç¬¦ä¸²çš„å·®å¼‚ç¨‹åº¦çš„é‡åŒ–é‡æµ‹ï¼Œé‡æµ‹æ–¹å¼æ˜¯çœ‹è‡³å°‘éœ€è¦å¤šå°‘æ¬¡çš„å¤„ç†æ‰èƒ½å°†ä¸€ä¸ªå­—ç¬¦ä¸²å˜æˆå¦ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚
   - å…·ä½“è®¡ç®—è¿‡ç¨‹æ˜¯ä¸€ä¸ªåŠ¨æ€è§„åˆ’ç®—æ³•ï¼šhttps://zhuanlan.zhihu.com/p/164599274
   - è¡¡é‡ä¸¤ä¸ªå¥å­çš„ç›¸ä¼¼åº¦æ—¶ï¼Œå¯ä»¥ä»¥è¯ä¸ºå•ä½è®¡ç®—
2. **BLEU Score**:
   - è®¡ç®—è¾“å‡ºä¸å‚ç…§å¥ä¹‹é—´çš„ n-gram å‡†ç¡®ç‡ï¼ˆn=1...4ï¼‰
   - å¯¹çŸ­è¾“å‡ºåšæƒ©ç½š
   - åœ¨æ•´ä¸ªæµ‹è¯•é›†ä¸Šå¹³å‡ä¸‹è¿°å€¼
   - å®Œæ•´è®¡ç®—å…¬å¼ï¼š$\mathrm{BLEU}_4=\min\left(1,\frac{output-length}{reference-length}\right)\left(\prod_{i=1}^4 precision_i\right)^{\frac{1}{4}}$
   - å‡½æ•°åº“ï¼šhttps://www.nltk.org/_modules/nltk/translate/bleu_score.html
3. **Rouge Score**:
   - Rouge-Nï¼šå°†æ¨¡å‹ç”Ÿæˆçš„ç»“æœå’Œæ ‡å‡†ç»“æœæŒ‰ N-gram æ‹†åˆ†åï¼Œåªè®¡ç®—å¬å›ç‡ï¼›
   - Rouge-L: åˆ©ç”¨äº†æœ€é•¿å…¬å…±å­åºåˆ—ï¼ˆLongest Common Sequenceï¼‰ï¼Œè®¡ç®—ï¼š$P=\frac{LCS(c,r)}{len(c)}$, $R=\frac{LCS(c,r)}{len(r)}$, $F=\frac{(1+\beta^2)PR}{R+\beta^2P}$
   - å‡½æ•°åº“ï¼šhttps://pypi.org/project/rouge-score/
   - å¯¹æ¯” BLEU ä¸ ROUGEï¼š
     - BLEU èƒ½è¯„ä¼°æµç•…åº¦ï¼Œä½†æŒ‡æ ‡åå‘äºè¾ƒçŸ­çš„ç¿»è¯‘ç»“æœï¼ˆbrevity penalty æ²¡æœ‰æƒ³è±¡ä¸­é‚£ä¹ˆå¼ºï¼‰
     - ROUGE ä¸ç®¡æµç•…åº¦ï¼Œæ‰€ä»¥åªé€‚åˆæ·±åº¦å­¦ä¹ çš„ç”Ÿæˆæ¨¡å‹ï¼šç»“æœéƒ½æ˜¯æµç•…çš„å‰æä¸‹ï¼ŒROUGE ååº”å‚ç…§å¥ä¸­å¤šå°‘å†…å®¹è¢«ç”Ÿæˆçš„å¥å­åŒ…å«ï¼ˆå¬å›ï¼‰
4. **METEOR**: å¦ä¸€ä¸ªä»æœºå™¨ç¿»è¯‘é¢†åŸŸå€Ÿé‰´çš„æŒ‡æ ‡ã€‚ä¸ BLEU ç›¸æ¯”ï¼ŒMETEOR è€ƒè™‘äº†æ›´å¤šçš„å› ç´ ï¼Œå¦‚åŒä¹‰è¯åŒ¹é…ã€è¯å¹²åŒ¹é…ã€è¯åºç­‰ï¼Œå› æ­¤å®ƒé€šå¸¸è¢«è®¤ä¸ºæ˜¯ä¸€ä¸ªæ›´å…¨é¢çš„è¯„ä»·æŒ‡æ ‡ã€‚
   - å¯¹è¯­è¨€å­¦å’Œè¯­ä¹‰è¯è¡¨æœ‰ä¾èµ–ï¼Œæ‰€ä»¥å¯¹è¯­è¨€ä¾èµ–å¼ºã€‚

> âœ… **Tip:** åˆ’é‡ç‚¹ï¼šæ­¤ç±»æ–¹æ³•å¸¸ç”¨äºå¯¹æ–‡æœ¬ç”Ÿæˆæ¨¡å‹çš„è‡ªåŠ¨åŒ–è¯„ä¼°ã€‚å®é™…ä½¿ç”¨ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸æ›´å…³æ³¨ç›¸å¯¹å˜åŒ–è€Œä¸æ˜¯ç»å¯¹å€¼ï¼ˆè°ƒä¼˜è¿‡ç¨‹ä¸­æŒ‡æ ‡æ˜¯ä¸æ˜¯åœ¨å˜å¥½ï¼‰ã€‚

### 1.7ã€åŸºäº LLM çš„æµ‹è¯•æ–¹æ³•

LangFuse æä¾›äº†åŸºäº LLM å’Œ Prompt çš„è‡ªåŠ¨æµ‹è¯•å·¥å…·ã€‚

å…·ä½“å‚è€ƒï¼šhttps://langfuse.com/docs/scores/model-based-evals

> âœ… **Tip:** åˆ’é‡ç‚¹ï¼šæ­¤ç±»æ–¹æ³•ï¼Œå¯¹äºç”¨äºè¯„ä¼°çš„ LLM è‡ªèº«èƒ½åŠ›æœ‰è¦æ±‚ã€‚éœ€æ ¹æ®å…·ä½“æƒ…å†µé€‰æ‹©ä½¿ç”¨ã€‚

### 1.8ã€ä¸ LlamaIndex é›†æˆ

```python
# !pip install --upgrade llama-index
```

```python
from llama_index.core import Settings
from llama_index.core.callbacks import CallbackManager
from langfuse.llama_index import LlamaIndexCallbackHandler

# å®šä¹‰ LangFuse çš„ CallbackHandler
langfuse_callback_handler = LlamaIndexCallbackHandler()

# ä¿®æ”¹ LlamaIndex çš„å…¨å±€è®¾å®š
Settings.callback_manager = CallbackManager([langfuse_callback_handler])
```

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.core.node_parser import SentenceSplitter
from llama_index.readers.file import PyMuPDFReader
from llama_index.core import Settings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

# æŒ‡å®šå…¨å±€llmä¸embeddingæ¨¡å‹
Settings.llm = OpenAI(temperature=0, model="gpt-4o")
Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small", dimensions=512)
Settings.transforms = [SentenceSplitter(chunk_size=300, chunk_overlap=100)]

# åŠ è½½ pdf æ–‡æ¡£
documents = SimpleDirectoryReader("./data", file_extractor={".pdf": PyMuPDFReader()}).load_data()

# æŒ‡å®š Vector Store ç”¨äº index
index = VectorStoreIndex.from_documents(documents)

# æ„å»ºå•è½® query engine
query_engine = index.as_query_engine()
```

```python
response = query_engine.query("llama2æœ‰å¤šå°‘å‚æ•°")

print(response)
```

**Output:**
```
Llama 2 æœ‰ 7Bã€13B å’Œ 70B å‚æ•°çš„å˜ä½“ã€‚
```

è‡ªå®šä¹‰ Trace å‚æ•°

```python
langfuse_callback_handler.set_trace_params(
    user_id="wzr",
    session_id="llamaindex-session",
    tags=["demo"]
  )
```

```python
response = query_engine.query("llama2å®‰å…¨å—")

print(response)
```

**Output:**
```
Llama 2 æ˜¯ä¸€ç§æ–°æŠ€æœ¯ï¼Œå…·æœ‰æ½œåœ¨çš„ä½¿ç”¨é£é™©ã€‚å°½ç®¡è¿›è¡Œäº†å®‰å…¨æ€§è¯„ä¼°ï¼Œä½†ç”±äºæç¤ºé›†çš„å±€é™æ€§ã€å®¡æŸ¥æŒ‡å—çš„ä¸»è§‚æ€§ä»¥åŠè¯„ä¼°è€…çš„ä¸»è§‚æ€§ï¼Œè¿™äº›è¯„ä¼°å¯èƒ½å­˜åœ¨åè§ã€‚å› æ­¤ï¼Œåœ¨éƒ¨ç½² Llama 2-Chat çš„åº”ç”¨ç¨‹åºä¹‹å‰ï¼Œå¼€å‘äººå‘˜åº”è¿›è¡Œé’ˆå¯¹å…¶ç‰¹å®šåº”ç”¨ç¨‹åºçš„å®‰å…¨æµ‹è¯•å’Œè°ƒæ•´ã€‚
```

æ›´å¤šæ¥å£ä¸å‚æ•°ï¼Œè¯·å‚è€ƒ[å®˜æ–¹æ–‡æ¡£](https://langfuse.com/docs/integrations/llama-index/get-started)ã€‚

## 2ã€LangSmith

> âš ï¸ **Note:** è¿è¡Œä»¥ä¸‹ä»£ç å‰ï¼Œè¯·å…ˆé‡å¯ä¸€ä¸‹ kernelï¼Œä»¥é‡ç½®æ‰€æœ‰é…ç½®ã€‚
    
![image](./assets/09-llm-tools/tips.png)

LangChain å®˜æ–¹çš„ SaaS æœåŠ¡ï¼Œä¸å¼€æºã€‚

å¹³å°å…¥å£ï¼šhttps://www.langchain.com/langsmith

æ–‡æ¡£åœ°å€ï¼šhttps://python.langchain.com/docs/langsmith/walkthrough

å°†ä½ çš„ LangChain åº”ç”¨ä¸ LangSmith é“¾æ¥ï¼Œéœ€è¦ï¼š

1. å®‰è£… LangSmith

```python
# %pip install --upgrade langsmith
```

2. æ³¨å†Œè´¦å·ï¼Œå¹¶ç”³è¯·ä¸€ä¸ª`LANGCHAIN_API_KEY`
3. åœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½®ä»¥ä¸‹å€¼

```shell
export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_PROJECT=YOUR_PROJECT_NAME #è‡ªå®šä¹‰é¡¹ç›®åç§°ï¼ˆå¯é€‰ï¼‰
export LANGCHAIN_API_KEY=LANGCHAIN_API_KEY # LangChain API Key
```

3. ç¨‹åºä¸­çš„è°ƒç”¨å°†è‡ªåŠ¨è¢«è®°å½•

```python
import os
from datetime import datetime
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = "HelloWorld-Demo"
```

### 2.1ã€åŸºæœ¬åŠŸèƒ½æ¼”ç¤º

1. Traces
2. LLM Calls
3. Monitor
4. Playground

![image](./assets/09-llm-tools/langsmith-example.png)

```python
from langchain.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnablePassthrough

model = ChatOpenAI(model="gpt-3.5-turbo")

prompt = ChatPromptTemplate.from_messages([
    HumanMessagePromptTemplate.from_template("Say hello to {input}!")
])


# å®šä¹‰è¾“å‡ºè§£æå™¨
parser = StrOutputParser()

chain = (
    {"input": RunnablePassthrough()}
    | prompt
    | model
    | parser
)
```

```python
chain.invoke("ç‹å“ç„¶")
```

**Output:**
```
'Hello ç‹å“ç„¶! Nice to meet you!'
```

### 2.2ã€æ•°æ®é›†ç®¡ç†ä¸æµ‹è¯•

### 2.2.1ã€åœ¨çº¿æ ‡æ³¨æ¼”ç¤º

![image](./assets/09-llm-tools/langsmith-annotation.png)

### 2.2.2ã€ä¸Šä¼ æ•°æ®é›†

```python
import json

data = []
with open('my_annotations.jsonl', 'r', encoding='utf-8') as fp:
    for line in fp:
        example = json.loads(line.strip())
        item = {
            "input": {
                "outlines": example["outlines"],
                "user_input": example["user_input"]
            },
            "expected_output": example["label"]
        }
        data.append(item)
```

```python
from langsmith import Client

client = Client()

dataset_name = "Assistant-"+datetime.now().strftime("%d/%m/%Y %H:%M:%S")

dataset = client.create_dataset(
    dataset_name,  # æ•°æ®é›†åç§°
    description="AGIClassçº¿ä¸Šè·Ÿè¯¾åŠ©æ‰‹çš„æ ‡æ³¨æ•°æ®",  # æ•°æ®é›†æè¿°
)

inputs, outputs = zip(
    *[({"input": item["input"]}, {"label": item["expected_output"]}) for item in data[:50]]
)
client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)
```

### 2.2.3ã€è¯„ä¼°å‡½æ•°

```python
from langsmith.schemas import Example, Run

def correct_label(root_run: Run, example: Example) -> dict:
    score = root_run.outputs.get("output") == example.outputs.get("label")
    return {"score": int(score), "key": "accuracy"}
```

### 2.2.4ã€è¿è¡Œæµ‹è¯•

```python
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser 

need_answer = PromptTemplate.from_template("""
*********
ä½ æ˜¯AIGCè¯¾ç¨‹çš„åŠ©æ•™ï¼Œä½ çš„å·¥ä½œæ˜¯ä»å­¦å‘˜çš„è¯¾å ‚äº¤æµä¸­é€‰æ‹©å‡ºéœ€è¦è€å¸ˆå›ç­”çš„é—®é¢˜ï¼ŒåŠ ä»¥æ•´ç†ä»¥äº¤ç»™è€å¸ˆå›ç­”ã€‚
 
è¯¾ç¨‹å†…å®¹:
{outlines}
*********
å­¦å‘˜è¾“å…¥:
{user_input}
*********
å¦‚æœè¿™æ˜¯ä¸€ä¸ªéœ€è¦è€å¸ˆç­”ç–‘çš„é—®é¢˜ï¼Œå›å¤Yï¼Œå¦åˆ™å›å¤Nã€‚
åªå›å¤Yæˆ–Nï¼Œä¸è¦å›å¤å…¶ä»–å†…å®¹ã€‚""")

model = ChatOpenAI(temperature=0, seed=42)
parser = StrOutputParser()

chain_v1 = need_answer | model | parser
```

```python
from langsmith.evaluation import evaluate

results = evaluate(
    lambda inputs: chain_v1.invoke(inputs["input"]),
    data=dataset_name,
    evaluators=[correct_label],
    experiment_prefix="Acc",
    description="æµ‹è¯•ChainV1",
)
```

**Output:**
```
/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
View the evaluation results for experiment: 'Acc-a48d31f8' at:
https://smith.langchain.com/o/97b8262a-9ab9-4b43-afeb-21ea05a90ba7/datasets/e7d27b2a-9f6a-4818-a716-14dbd0da3a05/compare?selectedSessions=e068df01-670a-431b-a5ee-3bbe552572a9
50it [00:08,  5.79it/s]
```

### 2.2.5ã€åŸºäº LLM çš„è¯„ä¼°å‡½æ•°

https://docs.smith.langchain.com/evaluation/faq/evaluator-implementations

## æ€»ç»“

ç®¡ç†ä¸€ä¸ª LLM åº”ç”¨çš„å…¨ç”Ÿå‘½å‘¨æœŸï¼Œéœ€è¦ç”¨åˆ°ä»¥ä¸‹å·¥å…·ï¼š

1. è°ƒè¯• Prompt çš„ Playground
2. æµ‹è¯•/éªŒè¯ç³»ç»Ÿçš„ç›¸å…³æŒ‡æ ‡
3. æ•°æ®é›†ç®¡ç†
4. å„ç§æŒ‡æ ‡ç›‘æ§ä¸ç»Ÿè®¡ï¼šè®¿é—®é‡ã€å“åº”æ—¶é•¿ã€Token è´¹ç­‰ç­‰

æ ¹æ®è‡ªå·±çš„æŠ€æœ¯æ ˆï¼Œé€‰æ‹©ï¼š

1. LangFuseï¼šå¼€æºå¹³å°ï¼Œæ”¯æŒ LangChainã€LlamaIndex å’ŒåŸç”Ÿ OpenAI API
2. LangSmith: LangChain çš„åŸå§‹ç®¡ç†å¹³å°

## ä½œä¸š

é€‰æ‹©ä¸€ä¸ªå·¥å…·å¹³å°ï¼Œå¯¹è‡ªå·±ä¹‹å‰å¼€å‘çš„ç³»ç»Ÿæˆ–æ¨¡å‹åšæ‰¹é‡æµ‹è¯•