# 07 Llamaindex   Index



# ğŸ’¡ è¿™èŠ‚è¯¾ä¼šå¸¦ç»™ä½ 

1. LlamaIndex çš„ç‰¹ç‚¹å’ŒåŸºæœ¬ç”¨æ³•
2. äº†è§£ LlamaIndex å†…ç½®çš„å·¥å…·
3. å¦‚ä½•ç”¨å¥½ SDK ç®€åŒ–åŸºäº LLM çš„åº”ç”¨å¼€å‘

å¼€å§‹ä¸Šè¯¾ï¼

## ğŸ“ è¿™èŠ‚è¯¾æ€ä¹ˆå­¦

ä»£ç èƒ½åŠ›è¦æ±‚ï¼š**ä¸­é«˜**ï¼ŒAI/æ•°å­¦åŸºç¡€è¦æ±‚ï¼š**æ— **

1. æœ‰ç¼–ç¨‹ä¸è½¯ä»¶å·¥ç¨‹åŸºç¡€çš„åŒå­¦
   - å…³æ³¨æ¥å£ä¸å®ç°ç»†èŠ‚ã€é«˜çº§æŠ€å·§ã€å¯æ‰©å±•æ€§
2. æ²¡æœ‰ç¼–ç¨‹æˆ–è½¯ä»¶å·¥ç¨‹åŸºç¡€çš„åŒå­¦
   - å°½é‡ç†è§£ SDK çš„æ¦‚å¿µå’Œä»·å€¼ï¼Œå°è¯•ä½“ä¼šä½¿ç”¨ SDK å‰åçš„å·®åˆ«ä¸æ„ä¹‰

## 1ã€å¤§è¯­è¨€æ¨¡å‹å¼€å‘æ¡†æ¶çš„ä»·å€¼æ˜¯ä»€ä¹ˆï¼Ÿ

_SDKï¼šSoftware Development Kitï¼Œå®ƒæ˜¯ä¸€ç»„è½¯ä»¶å·¥å…·å’Œèµ„æºçš„é›†åˆï¼Œæ—¨åœ¨å¸®åŠ©å¼€å‘è€…åˆ›å»ºã€æµ‹è¯•ã€éƒ¨ç½²å’Œç»´æŠ¤åº”ç”¨ç¨‹åºæˆ–è½¯ä»¶ã€‚_

æ‰€æœ‰å¼€å‘æ¡†æ¶ï¼ˆSDKï¼‰çš„æ ¸å¿ƒä»·å€¼ï¼Œéƒ½æ˜¯é™ä½å¼€å‘ã€ç»´æŠ¤æˆæœ¬ã€‚

å¤§è¯­è¨€æ¨¡å‹å¼€å‘æ¡†æ¶çš„ä»·å€¼ï¼Œæ˜¯è®©å¼€å‘è€…å¯ä»¥æ›´æ–¹ä¾¿åœ°å¼€å‘åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„åº”ç”¨ã€‚ä¸»è¦æä¾›ä¸¤ç±»å¸®åŠ©ï¼š

1. ç¬¬ä¸‰æ–¹èƒ½åŠ›æŠ½è±¡ã€‚æ¯”å¦‚ LLMã€å‘é‡æ•°æ®åº“ã€æœç´¢æ¥å£ç­‰
2. å¸¸ç”¨å·¥å…·ã€æ–¹æ¡ˆå°è£…
3. åº•å±‚å®ç°å°è£…ã€‚æ¯”å¦‚æµå¼æ¥å£ã€è¶…æ—¶é‡è¿ã€å¼‚æ­¥ä¸å¹¶è¡Œç­‰

å¥½çš„å¼€å‘æ¡†æ¶ï¼Œéœ€è¦å…·å¤‡ä»¥ä¸‹ç‰¹ç‚¹ï¼š

1. å¯é æ€§ã€é²æ£’æ€§é«˜
2. å¯ç»´æŠ¤æ€§é«˜
3. å¯æ‰©å±•æ€§é«˜
4. å­¦ä¹ æˆæœ¬ä½

ä¸¾äº›é€šä¿—çš„ä¾‹å­ï¼š

- ä¸å¤–éƒ¨åŠŸèƒ½è§£ä¾èµ–
  - æ¯”å¦‚å¯ä»¥éšæ„æ›´æ¢ LLM è€Œä¸ç”¨å¤§é‡é‡æ„ä»£ç 
  - æ›´æ¢ä¸‰æ–¹å·¥å…·ä¹ŸåŒç†
- ç»å¸¸å˜çš„éƒ¨åˆ†è¦åœ¨å¤–éƒ¨ç»´æŠ¤è€Œä¸æ˜¯æ”¾åœ¨ä»£ç é‡Œ
  - æ¯”å¦‚ Prompt æ¨¡æ¿
- å„ç§ç¯å¢ƒä¸‹éƒ½é€‚ç”¨
  - æ¯”å¦‚çº¿ç¨‹å®‰å…¨
- æ–¹ä¾¿è°ƒè¯•å’Œæµ‹è¯•
  - è‡³å°‘è¦èƒ½æ„Ÿè§‰åˆ°ç”¨äº†æ¯”ä¸ç”¨æ–¹ä¾¿å§
  - åˆæ³•çš„è¾“å…¥ä¸ä¼šå¼•å‘æ¡†æ¶å†…éƒ¨çš„æŠ¥é”™

> âœ… **Tip:** åˆ’é‡ç‚¹ï¼šé€‰å¯¹äº†æ¡†æ¶ï¼Œäº‹åŠåŠŸå€ï¼›åä¹‹ï¼Œäº‹å€åŠŸåŠã€‚

> â„¹ï¸ **Info:** ä»€ä¹ˆæ˜¯ SDK? https://aws.amazon.com/cn/what-is/sdk/

SDK å’Œ API çš„åŒºåˆ«æ˜¯ä»€ä¹ˆ? https://aws.amazon.com/cn/compare/the-difference-between-sdk-and-api/

#### ğŸŒ° ä¸¾ä¸ªä¾‹å­ï¼šä½¿ç”¨ SDKï¼Œ4 è¡Œä»£ç å®ç°ä¸€ä¸ªç®€æ˜“çš„ RAG ç³»ç»Ÿ

> âš ï¸ **Note:** è¿è¡Œæœ¬è¯¾ä»£ç å‰ï¼Œè¯·å…ˆé‡å¯ä¸€ä¸‹ kernelï¼Œä»¥é‡ç½®æ‰€æœ‰é…ç½®ã€‚
    
![image](./assets/07-llamaindex/tips.png)

```python
!pip install --upgrade llama-index
```

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

documents = SimpleDirectoryReader("./data").load_data()
index = VectorStoreIndex.from_documents(documents)

query_engine = index.as_query_engine()
```

```python
response = query_engine.query("llama2æœ‰å¤šå°‘å‚æ•°")
print(response)
```

**Output:**
```
Llama 2 ranges in scale from 7 billion to 70 billion parameters.
```

## 2ã€LlamaIndex ä»‹ç»

_ã€Œ LlamaIndex is a framework for building context-augmented LLM applications. Context augmentation refers to any use case that applies LLMs on top of your private or domain-specific data. ã€_

LlamaIndex æ˜¯ä¸€ä¸ªä¸ºå¼€å‘ã€Œä¸Šä¸‹æ–‡å¢å¼ºã€çš„å¤§è¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶ï¼ˆä¹Ÿå°±æ˜¯ SDKï¼‰ã€‚**ä¸Šä¸‹æ–‡å¢å¼º**ï¼Œæ³›æŒ‡ä»»ä½•åœ¨ç§æœ‰æˆ–ç‰¹å®šé¢†åŸŸæ•°æ®åŸºç¡€ä¸Šåº”ç”¨å¤§è¯­è¨€æ¨¡å‹çš„æƒ…å†µã€‚ä¾‹å¦‚ï¼š

![image](./assets/07-llamaindex/basic_rag.png)

- Question-Answering Chatbots (ä¹Ÿå°±æ˜¯ RAG)
- Document Understanding and Extraction ï¼ˆæ–‡æ¡£ç†è§£ä¸ä¿¡æ¯æŠ½å–ï¼‰

- Autonomous Agents that can perform research and take actions ï¼ˆæ™ºèƒ½ä½“åº”ç”¨ï¼‰

LlamaIndex æœ‰ Python å’Œ Typescript ä¸¤ä¸ªç‰ˆæœ¬ï¼ŒPython ç‰ˆçš„æ–‡æ¡£ç›¸å¯¹æ›´å®Œå–„ã€‚

- Python æ–‡æ¡£åœ°å€ï¼šhttps://docs.llamaindex.ai/en/stable/
- Python API æ¥å£æ–‡æ¡£ï¼šhttps://docs.llamaindex.ai/en/stable/api_reference/

- TS æ–‡æ¡£åœ°å€ï¼šhttps://ts.llamaindex.ai/

- TS API æ¥å£æ–‡æ¡£ï¼šhttps://ts.llamaindex.ai/api/

LlamaIndex æ˜¯ä¸€ä¸ªå¼€æºæ¡†æ¶ï¼ŒGithub é“¾æ¥ï¼šhttps://github.com/run-llama

### LlamaIndex çš„æ ¸å¿ƒæ¨¡å—

![LlamaIndex æ ¸å¿ƒæ¨¡å—](./assets/07-llamaindex/llamaindex.png)

### å®‰è£… LlamaIndex

1. Python

```
pip install llama-index
```

2. Typescript

```
# é€šè¿‡ npm å®‰è£…
npm install llamaindex

# é€šè¿‡ yarn å®‰è£…
yarn add llamaindex

# é€šè¿‡ pnpm å®‰è£…
pnpm add llamaindex
```

æœ¬è¯¾ç¨‹ä»¥ Python ç‰ˆä¸ºä¾‹è¿›è¡Œè®²è§£ã€‚

## 3ã€æ•°æ®åŠ è½½ï¼ˆLoadingï¼‰

### 3.1ã€åŠ è½½æœ¬åœ°æ•°æ®

`SimpleDirectoryReader` æ˜¯ä¸€ä¸ªç®€å•çš„æœ¬åœ°æ–‡ä»¶åŠ è½½å™¨ã€‚å®ƒä¼šéå†æŒ‡å®šç›®å½•ï¼Œå¹¶æ ¹æ®æ–‡ä»¶æ‰©å±•åè‡ªåŠ¨åŠ è½½æ–‡ä»¶ï¼ˆ**æ–‡æœ¬å†…å®¹**ï¼‰ã€‚

æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼š

- `.csv` - comma-separated values
- `.docx` - Microsoft Word
- `.epub` - EPUB ebook format
- `.hwp` - Hangul Word Processor
- `.ipynb` - Jupyter Notebook
- `.jpeg`, `.jpg` - JPEG image
- `.mbox` - MBOX email archive
- `.md` - Markdown
- `.mp3`, `.mp4` - audio and video
- `.pdf` - Portable Document Format
- `.png` - Portable Network Graphics
- `.ppt`, `.pptm`, `.pptx` - Microsoft PowerPoint

```python
import json
from pydantic.v1 import BaseModel

def show_json(data):
    """ç”¨äºå±•ç¤ºjsonæ•°æ®"""
    if isinstance(data, str):
        obj = json.loads(data)
        print(json.dumps(obj, indent=4, ensure_ascii=False))
    elif isinstance(data, dict) or isinstance(data, list):
        print(json.dumps(data, indent=4, ensure_ascii=False))
    elif issubclass(type(data), BaseModel):
        print(json.dumps(data.dict(), indent=4, ensure_ascii=False))

def show_list_obj(data):
    """ç”¨äºå±•ç¤ºä¸€ç»„å¯¹è±¡"""
    if isinstance(data, list):
        for item in data:
            show_json(item)
    else:
        raise ValueError("Input is not a list")
```

```python
from llama_index.core import SimpleDirectoryReader

reader = SimpleDirectoryReader(
        input_dir="./data", # ç›®æ ‡ç›®å½•
        recursive=False, # æ˜¯å¦é€’å½’éå†å­ç›®å½•
        required_exts=[".pdf"] # (å¯é€‰)åªè¯»å–æŒ‡å®šåç¼€çš„æ–‡ä»¶
    )
documents = reader.load_data()
```

```python
print(documents[0].text)
show_json(documents[0].json())
```

**Output:**
```
Llama 2: OpenFoundation andFine-Tuned ChatModels
Hugo Touvronâˆ—Louis Martinâ€ Kevin Stoneâ€ 
Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov SoumyaBatra
Prajjwal Bhargava Shruti Bhosale Dan Bikel LukasBlecher Cristian CantonFerrer MoyaChen
Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu BrianFuller
Cynthia Gao VedanujGoswami NamanGoyal AnthonyHartshorn Saghar Hosseini RuiHou
Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa IsabelKloumann ArtemKorenev
Punit Singh Koura Marie-AnneLachaux ThibautLavril Jenya Lee Diana Liskovich
Yinghai Lu YuningMao Xavier Martinet Todor Mihaylov PushkarMishra
Igor Molybog Yixin Nie AndrewPoulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi
Alan Schelten Ruan Silva EricMichael Smith Ranjan Subramanian XiaoqingEllenTan BinhTang
Ross Taylor AdinaWilliams JianXiang Kuan PuxinXu ZhengYan Iliyan Zarov YuchenZhang
Angela Fan MelanieKambadur SharanNarang Aurelien Rodriguez RobertStojnic
Sergey Edunov ThomasScialomâˆ—
GenAI, Meta
Abstract
In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned
large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.
Our fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our
models outperform open-source chat models on most benchmarks we tested, and based on
ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-
source models. We provide a detailed description of our approach to fine-tuning and safety
improvements of Llama 2-Chat in order to enable the community to build on our work and
contribute to the responsibledevelopmentof LLMs.
âˆ—Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com
â€ Second author
Contributions for all the authors can be found in Section A.1.arXiv:2307.09288v2  [cs.CL]  19 Jul 2023
{
    "id_": "045248bf-9292-4c32-a82c-abfb63ca0aaa",
    "embedding": null,
    "metadata": {
        "page_label": "1",
        "file_name": "llama2-extracted.pdf",
        "file_path": "/home/jovyan/lecture-notes/07-llamaindex/data/llama2-extracted.pdf",
        "file_type": "application/pdf",
        "file_size": 401338,
        "creation_date": "2024-06-14",
        "last_modified_date": "2024-06-14"
    },
    "excluded_embed_metadata_keys": [
        "file_name",
        "file_type",
        "file_size",
        "creation_date",
        "last_modified_date",
        "last_accessed_date"
    ],
    "excluded_llm_metadata_keys": [
        "file_name",
        "file_type",
        "file_size",
        "creation_date",
        "last_modified_date",
        "last_accessed_date"
    ],
    "relationships": {},
    "text": "Llama 2: OpenFoundation andFine-Tuned ChatModels\nHugo Touvronâˆ—Louis Martinâ€ Kevin Stoneâ€ \nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov SoumyaBatra\nPrajjwal Bhargava Shruti Bhosale Dan Bikel LukasBlecher Cristian CantonFerrer MoyaChen\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu BrianFuller\nCynthia Gao VedanujGoswami NamanGoyal AnthonyHartshorn Saghar Hosseini RuiHou\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa IsabelKloumann ArtemKorenev\nPunit Singh Koura Marie-AnneLachaux ThibautLavril Jenya Lee Diana Liskovich\nYinghai Lu YuningMao Xavier Martinet Todor Mihaylov PushkarMishra\nIgor Molybog Yixin Nie AndrewPoulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\nAlan Schelten Ruan Silva EricMichael Smith Ranjan Subramanian XiaoqingEllenTan BinhTang\nRoss Taylor AdinaWilliams JianXiang Kuan PuxinXu ZhengYan Iliyan Zarov YuchenZhang\nAngela Fan MelanieKambadur SharanNarang Aurelien Rodriguez RobertStojnic\nSergey Edunov ThomasScialomâˆ—\nGenAI, Meta\nAbstract\nIn this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\nOur fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our\nmodels outperform open-source chat models on most benchmarks we tested, and based on\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-\nsource models. We provide a detailed description of our approach to fine-tuning and safety\nimprovements of Llama 2-Chat in order to enable the community to build on our work and\ncontribute to the responsibledevelopmentof LLMs.\nâˆ—Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\nâ€ Second author\nContributions for all the authors can be found in Section A.1.arXiv:2307.09288v2  [cs.CL]  19 Jul 2023",
    "mimetype": "text/plain",
    "start_char_idx": null,
    "end_char_idx": null,
    "text_template": "{metadata_str}\n\n{content}",
    "metadata_template": "{key}: {value}",
    "metadata_seperator": "\n",
    "class_name": "Document"
}
```

> âš ï¸ **Note:** æ³¨æ„ï¼šå¯¹å›¾åƒã€è§†é¢‘ã€è¯­éŸ³ç±»æ–‡ä»¶ï¼Œé»˜è®¤ä¸ä¼šè‡ªåŠ¨æå–å…¶ä¸­æ–‡å­—ã€‚å¦‚éœ€æå–ï¼Œå‚è€ƒä¸‹é¢ä»‹ç»çš„ Data Connectorsã€‚

é»˜è®¤çš„ `PDFReader` æ•ˆæœå¹¶ä¸ç†æƒ³ï¼Œæˆ‘ä»¬å¯ä»¥æ›´æ¢æ–‡ä»¶åŠ è½½å™¨

```python
# !pip install pymupdf
```

```python
from llama_index.core import SimpleDirectoryReader
from llama_index.readers.file import PyMuPDFReader

reader = SimpleDirectoryReader(
        input_dir="./data", # ç›®æ ‡ç›®å½•
        recursive=False, # æ˜¯å¦é€’å½’éå†å­ç›®å½•
        required_exts=[".pdf"], # (å¯é€‰)åªè¯»å–æŒ‡å®šåç¼€çš„æ–‡ä»¶
        file_extractor={".pdf": PyMuPDFReader()} # æŒ‡å®šç‰¹å®šçš„æ–‡ä»¶åŠ è½½å™¨
    )

documents = reader.load_data()

print(documents[0].text)
```

**Output:**
```
Llama 2: Open Foundation and Fine-Tuned Chat Models
Hugo Touvronâˆ—
Louis Martinâ€ 
Kevin Stoneâ€ 
Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra
Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen
Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller
Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou
Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev
Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich
Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra
Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi
Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang
Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang
Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic
Sergey Edunov
Thomas Scialomâˆ—
GenAI, Meta
Abstract
In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned
large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.
Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our
models outperform open-source chat models on most benchmarks we tested, and based on
our human evaluations for helpfulness and safety, may be a suitable substitute for closed-
source models. We provide a detailed description of our approach to fine-tuning and safety
improvements of Llama 2-Chat in order to enable the community to build on our work and
contribute to the responsible development of LLMs.
âˆ—Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com
â€ Second author
Contributions for all the authors can be found in Section A.1.
arXiv:2307.09288v2  [cs.CL]  19 Jul 2023
```

æ›´å¤šçš„ PDF åŠ è½½å™¨è¿˜æœ‰ [`SmartPDFLoader`](https://llamahub.ai/l/readers/llama-index-readers-smart-pdf-loader?from=readers) å’Œ [`LlamaParse`](https://llamahub.ai/l/readers/llama-index-readers-llama-parse?from=readers), äºŒè€…éƒ½æä¾›äº†æ›´ä¸°å¯Œçš„è§£æèƒ½åŠ›ï¼ŒåŒ…æ‹¬è§£æç« èŠ‚ä¸æ®µè½ç»“æ„ç­‰ã€‚ä½†ä¸æ˜¯ 100%å‡†ç¡®ï¼Œå¶æœ‰æ–‡å­—ä¸¢å¤±æˆ–é”™ä½æƒ…å†µï¼Œå»ºè®®æ ¹æ®è‡ªèº«éœ€æ±‚è¯¦ç»†æµ‹è¯•è¯„ä¼°ã€‚

### 3.2ã€Data Connectors

ç”¨äºå¤„ç†æ›´ä¸°å¯Œçš„æ•°æ®ç±»å‹ï¼Œå¹¶å°†å…¶è¯»å–ä¸º `Document` çš„å½¢å¼ï¼ˆtext + metadataï¼‰ã€‚

[//]: # (ä¾‹å¦‚ï¼šåŠ è½½ä¸€ä¸ª[é£ä¹¦æ–‡æ¡£]&#40;https://agiclass.feishu.cn/docx/FULadzkWmovlfkxSgLPcE4oWnPf&#41;ã€‚ï¼ˆé£ä¹¦æ–‡æ¡£ API è®¿é—®æƒé™ç”³è¯·ï¼Œè¯·å‚è€ƒæ­¤[è¯´æ˜æ–‡æ¡£]&#40;é£ä¹¦æ–‡æ¡£ç›¸å…³æƒé™ç”³è¯·.pdf&#41;ï¼‰)

```python
# !pip install llama-index-readers-feishu-docs
```

```python
from llama_index.readers.feishu_docs import FeishuDocsReader

# è§è¯´æ˜æ–‡æ¡£
app_id = "cli_a6f1c0fa1fd9d00b"
app_secret = "dMXCTy8DGaty2xn8I858ZbFDFvcqgiep"

# https://agiclass.feishu.cn/docx/FULadzkWmovlfkxSgLPcE4oWnPf
# é“¾æ¥æœ€åçš„ "FULadzkWmovlfkxSgLPcE4oWnPf" ä¸ºæ–‡æ¡£ ID 
doc_ids = ["FULadzkWmovlfkxSgLPcE4oWnPf"]

# å®šä¹‰é£ä¹¦æ–‡æ¡£åŠ è½½å™¨
loader = FeishuDocsReader(app_id, app_secret)

# åŠ è½½æ–‡æ¡£
documents = loader.load_data(document_ids=doc_ids)

# æ˜¾ç¤ºå‰1000å­—ç¬¦
print(documents[0].text[:1000])
```

**Output:**
```
AI å¤§æ¨¡å‹å…¨æ ˆå·¥ç¨‹å¸ˆåŸ¹å…»è®¡åˆ’ - AGIClass.ai

ç”± AGI è¯¾å ‚æ¨å‡ºçš„ç¤¾ç¾¤å‹ä¼šå‘˜åˆ¶è¯¾ç¨‹ï¼Œä¼ æˆå¤§æ¨¡å‹çš„åŸç†ã€åº”ç”¨å¼€å‘æŠ€æœ¯å’Œè¡Œä¸šè®¤çŸ¥ï¼ŒåŠ©ä½ æˆä¸º ChatGPT æµªæ½®ä¸­çš„è¶…çº§ä¸ªä½“
ä»€ä¹ˆæ˜¯ AI å¤§æ¨¡å‹å…¨æ ˆå·¥ç¨‹å¸ˆï¼Ÿ
ã€ŒAI å¤§æ¨¡å‹å…¨æ ˆå·¥ç¨‹å¸ˆã€ç®€ç§°ã€ŒAI å…¨æ ˆã€ï¼Œæ˜¯ä¸€ä¸ªäººå°±èƒ½å€ŸåŠ© AIï¼Œè®¾è®¡ã€å¼€å‘å’Œè¿è¥åŸºäº AI çš„å¤§æ¨¡å‹åº”ç”¨çš„è¶…çº§ä¸ªä½“ã€‚
AI å…¨æ ˆéœ€è¦æ‡‚ä¸šåŠ¡ã€æ‡‚ AIã€æ‡‚ç¼–ç¨‹ï¼Œä¸€ä¸ªäººå°±æ˜¯ä¸€ä¸ªå›¢é˜Ÿï¼Œå•æªåŒ¹é©¬åˆ›é€ è´¢å¯Œã€‚
åœ¨æŠ€æœ¯å‹å…¬å¸ï¼ŒAI å…¨æ ˆæœ€æ‡‚ AIï¼Œç¬é—´ç«™ä¸ŠæŠ€æœ¯é¡¶å³°ã€‚
åœ¨éæŠ€æœ¯å‹å…¬å¸ï¼ŒAI å…¨æ ˆè¿æ¥å…¶ä»–å‘˜å·¥å’Œ AIï¼Œæå‡æ•´ä¸ªå…¬å¸çš„æ•ˆç‡ã€‚
åœ¨å…¬å¸å¤–ï¼ŒAI å…¨æ ˆæ¥é¡¹ç›®ï¼Œç‹¬ç«‹å¼€å‘å˜ç°å°å·¥å…·ï¼Œèµšå–ä¸°åšå‰¯ä¸šæ”¶å…¥ã€‚
é€‚åˆäººç¾¤
å­¦ä¹ æœ¬è¯¾ç¨‹ï¼Œå¯ä»¥åœ¨ä¸‹è¿°ç›®æ ‡ä¸­ä¸‰é€‰ä¸€ï¼š
æˆä¸º AI å…¨æ ˆï¼šæ‡‚ä¸šåŠ¡ã€æ‡‚ AI ä¹Ÿæ‡‚ç¼–ç¨‹ã€‚å¤§é‡ä½¿ç”¨ AIï¼Œè‡ªå·±å®Œæˆ AI åº”ç”¨ä»ç­–åˆ’ã€å¼€å‘åˆ°è½åœ°çš„å…¨è¿‡ç¨‹ã€‚åŒ…æ‹¬å•†ä¸šåˆ†æã€éœ€æ±‚åˆ†æã€äº§å“è®¾è®¡ã€å¼€å‘ã€æµ‹è¯•ã€å¸‚åœºæ¨å¹¿å’Œè¿è¥ç­‰
æˆä¸ºä¸šåŠ¡å‘ AI å…¨æ ˆï¼šæ‡‚ä¸šåŠ¡ä¹Ÿæ‡‚ AIï¼Œä¸ç¨‹åºå‘˜åˆä½œï¼Œä¸€èµ·å®Œæˆ AI åº”ç”¨ä»ç­–åˆ’ã€å¼€å‘åˆ°è½åœ°çš„å…¨è¿‡ç¨‹
æˆä¸ºç¼–ç¨‹å‘ AI å…¨æ ˆï¼šæ‡‚ç¼–ç¨‹ä¹Ÿæ‡‚ AIï¼Œä¸ä¸šåŠ¡äººå‘˜åˆä½œï¼Œä¸€èµ·å®Œæˆ AI åº”ç”¨ä»ç­–åˆ’ã€å¼€å‘åˆ°è½åœ°çš„å…¨è¿‡ç¨‹
æ‡‚è‡³å°‘ä¸€é—¨ç¼–ç¨‹è¯­è¨€ï¼Œå¹¶æœ‰è¿‡çœŸå®é¡¹ç›®å¼€å‘ç»éªŒçš„è½¯ä»¶å¼€å‘â¼¯ç¨‹å¸ˆã€â¾¼çº§â¼¯ç¨‹å¸ˆã€æŠ€æœ¯æ€»ç›‘ã€ç ”å‘ç»ç†ã€æ¶æ„å¸ˆã€æµ‹è¯•â¼¯ç¨‹å¸ˆã€æ•°æ®å·¥ç¨‹å¸ˆã€è¿ç»´å·¥ç¨‹å¸ˆç­‰ï¼Œå»ºè®®ä»¥ã€ŒAI å…¨æ ˆã€ä¸ºç›®æ ‡ã€‚å³ä¾¿å¯¹å•†ä¸šã€äº§å“ã€å¸‚åœºç­‰çš„å­¦ä¹ è¾¾ä¸åˆ°æœ€ä½³ï¼Œä½†å·²æŒæ¡çš„ç»éªŒå’Œè®¤çŸ¥ä¹Ÿæœ‰åŠ©äºæˆä¸ºæœ‰ç«äº‰åŠ›çš„ã€Œç¼–ç¨‹å‘AI å…¨æ ˆã€ã€‚
ä¸æ‡‚ç¼–ç¨‹çš„äº§å“ç»ç†ã€éœ€æ±‚åˆ†æå¸ˆã€åˆ›ä¸šè€…ã€è€æ¿ã€è§£å†³æ–¹æ¡ˆå·¥ç¨‹å¸ˆã€é¡¹ç›®ç»ç†ã€è¿è¥ã€å¸‚åœºã€é”€å”®ã€è®¾è®¡å¸ˆç­‰ï¼Œå»ºè®®ä¼˜å…ˆé€‰æ‹©ã€Œä¸šåŠ¡å‘ AI å…¨æ ˆã€ä¸ºç›®æ ‡ã€‚åœ¨è¯¾ç¨‹æä¾›çš„æŠ€æœ¯ç¯å¢ƒé‡Œç†é™¶ï¼Œæé«˜æŠ€æœ¯é¢†åŸŸçš„åˆ¤æ–­åŠ›ï¼Œæœªæ¥å¯ä»¥å’ŒæŠ€æœ¯äººå‘˜æ›´æµç•…åœ°æ²Ÿé€šåä½œã€‚å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œå¦‚æœèƒ½å–„ç”¨ AI å­¦ä¹ ç¼–ç¨‹ã€è¾…åŠ©ç¼–ç¨‹ï¼Œå°±å¯ä»¥å‘ã€ŒAI å…¨æ ˆã€è¿ˆè¿›ã€‚
å¸ˆèµ„åŠ›é‡
é¦–å¸­è®²å¸ˆ - ç‹å“ç„¶


image.png


å“ˆå°”æ»¨å·¥ä¸šå¤§å­¦æœ¬ç¡•ï¼Œè‹±å›½ UCL åšå£«ï¼Œå›½é™…çŸ¥åå­¦è€…ã€ä¼ä¸šå®¶ï¼Œå¸ˆä»ç»Ÿè®¡æœºå™¨å­¦ä¹ ç†è®ºå¥ åŸºäººä¹‹ä¸€ John Shawe-Taylor æ•™æˆï¼Œæ˜¯æœ€æ—©ä»äº‹äººæœºå¯¹è¯ç ”ç©¶çš„åè£”å­¦è€…ä¹‹ä¸€ï¼Œè‡³ä»Šå·²è¶… 20 å¹´ã€‚
ä»–å°±æ˜¯ AI å…¨æ ˆï¼Œä»åœ¨ç ”å‘ä¸€çº¿ï¼Œå•äººé”€å”®ã€å”®å‰ã€å¼€å‘ã€å®æ–½å…¨æµç¨‹äº¤ä»˜å¤šä¸ªæ•°ç™¾ä¸‡é‡‘é¢ AI é¡¹ç›®ï¼Œå…¨æ ˆå®æˆ˜ç»éªŒ
```

> â„¹ï¸ **Info:** æ›´å¤š Data Connectors
    
        å†…ç½®çš„æ–‡ä»¶åŠ è½½å™¨
        è¿æ¥ä¸‰æ–¹æœåŠ¡çš„æ•°æ®åŠ è½½å™¨ï¼Œä¾‹å¦‚æ•°æ®åº“
        æ›´å¤šåŠ è½½å™¨å¯ä»¥åœ¨ LlamaHub ä¸Šæ‰¾åˆ°

## 4ã€æ–‡æœ¬åˆ‡åˆ†ä¸è§£æï¼ˆChunkingï¼‰

ä¸ºæ–¹ä¾¿æ£€ç´¢ï¼Œæˆ‘ä»¬é€šå¸¸æŠŠ `Document` åˆ‡åˆ†ä¸º `Node`ã€‚

åœ¨ LlamaIndex ä¸­ï¼Œ`Node` è¢«å®šä¹‰ä¸ºä¸€ä¸ªæ–‡æœ¬çš„ã€Œchunkã€ã€‚

### 4.1ã€ä½¿ç”¨ TextSplitters å¯¹æ–‡æœ¬åšåˆ‡åˆ†

ä¾‹å¦‚ï¼š`TokenTextSplitter` æŒ‰æŒ‡å®š token æ•°åˆ‡åˆ†æ–‡æœ¬

```python
from llama_index.core import Document
from llama_index.core.node_parser import TokenTextSplitter

node_parser = TokenTextSplitter(
    chunk_size=100,  # æ¯ä¸ª chunk çš„æœ€å¤§é•¿åº¦
    chunk_overlap=50  # chunk ä¹‹é—´é‡å é•¿åº¦ 
)

nodes = node_parser.get_nodes_from_documents(
    documents, show_progress=False
)
```

```python
show_json(nodes[0].json())
show_json(nodes[1].json())
```

**Output:**
```
{
    "id_": "0a1eee9a-635a-4391-8b74-75bf3c648f0e",
    "embedding": null,
    "metadata": {
        "document_id": "FULadzkWmovlfkxSgLPcE4oWnPf"
    },
    "excluded_embed_metadata_keys": [],
    "excluded_llm_metadata_keys": [],
    "relationships": {
        "1": {
            "node_id": "9cd5baeb-1c59-448a-9c95-df7fb634b6bb",
            "node_type": "4",
            "metadata": {
                "document_id": "FULadzkWmovlfkxSgLPcE4oWnPf"
            },
            "hash": "aa5f32dabade1da0e01c23cb16e160b624565d09a4a2a07fb7f7fd4d45aac88a",
            "class_name": "RelatedNodeInfo"
        },
        "3": {
            "node_id": "1a1aa20f-a88a-49a6-a0c6-fd13700022e4",
            "node_type": "1",
            "metadata": {},
            "hash": "654c6cbdd5a23946a84e84e6f3a474de2a442191b2be2d817ba7f04286b1a980",
            "class_name": "RelatedNodeInfo"
        }
    },
    "text": "AI å¤§æ¨¡å‹å…¨æ ˆå·¥ç¨‹å¸ˆåŸ¹å…»è®¡åˆ’ - AGIClass.ai\n\nç”± AGI è¯¾å ‚æ¨å‡ºçš„ç¤¾ç¾¤å‹ä¼šå‘˜åˆ¶è¯¾ç¨‹ï¼Œä¼ æˆå¤§æ¨¡å‹çš„åŸç†ã€åº”ç”¨å¼€å‘æŠ€æœ¯å’Œè¡Œä¸šè®¤çŸ¥ï¼ŒåŠ©ä½ æˆä¸º",
    "mimetype": "text/plain",
    "start_char_idx": 0,
    "end_char_idx": 76,
    "text_template": "{metadata_str}\n\n{content}",
    "metadata_template": "{key}: {value}",
    "metadata_seperator": "\n",
    "class_name": "TextNode"
}
{
    "id_": "1a1aa20f-a88a-49a6-a0c6-fd13700022e4",
    "embedding": null,
    "metadata": {
        "document_id": "FULadzkWmovlfkxSgLPcE4oWnPf"
    },
    "excluded_embed_metadata_keys": [],
    "excluded_llm_metadata_keys": [],
    "relationships": {
        "1": {
            "node_id": "9cd5baeb-1c59-448a-9c95-df7fb634b6bb",
            "node_type": "4",
            "metadata": {
                "document_id": "FULadzkWmovlfkxSgLPcE4oWnPf"
            },
            "hash": "aa5f32dabade1da0e01c23cb16e160b624565d09a4a2a07fb7f7fd4d45aac88a",
            "class_name": "RelatedNodeInfo"
        },
        "2": {
            "node_id": "0a1eee9a-635a-4391-8b74-75bf3c648f0e",
            "node_type": "1",
            "metadata": {
                "document_id": "FULadzkWmovlfkxSgLPcE4oWnPf"
            },
            "hash": "b08e60a1cf7fa55aa8c010d792766208dcbb34e58aeead16dca005eab4e1df8f",
            "class_name": "RelatedNodeInfo"
        },
        "3": {
            "node_id": "54c6a119-d914-4690-aa3c-1275d55efe5a",
            "node_type": "1",
            "metadata": {},
            "hash": "06d6c13287ff7e2f033a1aae487198dbfdec3d954aab0fd9b4866ce833200afb",
            "class_name": "RelatedNodeInfo"
        }
    },
    "text": "AGI è¯¾å ‚æ¨å‡ºçš„ç¤¾ç¾¤å‹ä¼šå‘˜åˆ¶è¯¾ç¨‹ï¼Œä¼ æˆå¤§æ¨¡å‹çš„åŸç†ã€åº”ç”¨å¼€å‘æŠ€æœ¯å’Œè¡Œä¸šè®¤çŸ¥ï¼ŒåŠ©ä½ æˆä¸º ChatGPT æµªæ½®ä¸­çš„è¶…çº§ä¸ªä½“\nä»€ä¹ˆæ˜¯ AI",
    "mimetype": "text/plain",
    "start_char_idx": 33,
    "end_char_idx": 100,
    "text_template": "{metadata_str}\n\n{content}",
    "metadata_template": "{key}: {value}",
    "metadata_seperator": "\n",
    "class_name": "TextNode"
}
```

LlamaIndex æä¾›äº†ä¸°å¯Œçš„ `TextSplitter`ï¼Œä¾‹å¦‚ï¼š

- [`SentenceSplitter`](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/sentence_splitter/)ï¼šåœ¨åˆ‡åˆ†æŒ‡å®šé•¿åº¦çš„ chunk åŒæ—¶å°½é‡ä¿è¯å¥å­è¾¹ç•Œä¸è¢«åˆ‡æ–­ï¼›
- [`CodeSplitter`](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/code/)ï¼šæ ¹æ® ASTï¼ˆç¼–è¯‘å™¨çš„æŠ½è±¡å¥æ³•æ ‘ï¼‰åˆ‡åˆ†ä»£ç ï¼Œä¿è¯ä»£ç åŠŸèƒ½ç‰‡æ®µå®Œæ•´ï¼›
- [`SemanticSplitterNodeParser`](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/semantic_splitter/)ï¼šæ ¹æ®è¯­ä¹‰ç›¸å…³æ€§å¯¹å°†æ–‡æœ¬åˆ‡åˆ†ä¸ºç‰‡æ®µã€‚

### 4.2ã€ä½¿ç”¨ NodeParsers å¯¹æœ‰ç»“æ„çš„æ–‡æ¡£åšè§£æ

ä¾‹å¦‚ï¼š`MarkdownNodeParser`è§£æ markdown æ–‡æ¡£

```python
from llama_index.readers.file import FlatReader
from llama_index.core.node_parser import MarkdownNodeParser
from pathlib import Path

md_docs = FlatReader().load_data(Path("./data/ChatALL.md"))
parser = MarkdownNodeParser()
nodes = parser.get_nodes_from_documents(md_docs)
```

```python
show_json(nodes[2].json())
show_json(nodes[3].json())
```

**Output:**
```
{
    "id_": "5c2eb25f-08ac-4b2e-9050-bc6efa90304a",
    "embedding": null,
    "metadata": {
        "filename": "ChatALL.md",
        "extension": ".md",
        "Header_2": "åŠŸèƒ½"
    },
    "excluded_embed_metadata_keys": [],
    "excluded_llm_metadata_keys": [],
    "relationships": {
        "1": {
            "node_id": "f3be7692-141b-41a6-8b3f-58402aaf36c1",
            "node_type": "4",
            "metadata": {
                "filename": "ChatALL.md",
                "extension": ".md"
            },
            "hash": "45b9149e0039c1ef7fbbd74f96923875505cc77916de48734ba7767f6a16a87e",
            "class_name": "RelatedNodeInfo"
        },
        "2": {
            "node_id": "3722d2c3-6638-453c-8fe1-f2af97fc8452",
            "node_type": "1",
            "metadata": {
                "filename": "ChatALL.md",
                "extension": ".md",
                "Header_2": "å±å¹•æˆªå›¾"
            },
            "hash": "f6065ad5e9929bc7ee14e3c4cc2d29c06788501df8887476c30b279ba8ffd594",
            "class_name": "RelatedNodeInfo"
        },
        "3": {
            "node_id": "c5dcf1d3-7a6a-48b0-b6d8-06457b182ac5",
            "node_type": "1",
            "metadata": {
                "Header_2": "åŠŸèƒ½",
                "Header_3": "è¿™æ˜¯ä½ å—ï¼Ÿ"
            },
            "hash": "f54ac07d417fbcbd606e7cdd3de28c30804e2213218dec2e6157d5037a23e289",
            "class_name": "RelatedNodeInfo"
        }
    },
    "text": "åŠŸèƒ½\n\nåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ AI æœºå™¨äººéå¸¸ç¥å¥‡ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„è¡Œä¸ºå¯èƒ½æ˜¯éšæœºçš„ï¼Œä¸åŒçš„æœºå™¨äººåœ¨ä¸åŒçš„ä»»åŠ¡ä¸Šè¡¨ç°ä¹Ÿæœ‰å·®å¼‚ã€‚å¦‚æœä½ æƒ³è·å¾—æœ€ä½³ä½“éªŒï¼Œä¸è¦ä¸€ä¸ªä¸€ä¸ªå°è¯•ã€‚ChatALLï¼ˆä¸­æ–‡åï¼šé½å¨ï¼‰å¯ä»¥æŠŠä¸€æ¡æŒ‡ä»¤åŒæ—¶å‘ç»™å¤šä¸ª AIï¼Œå¸®åŠ©æ‚¨å‘ç°æœ€å¥½çš„å›ç­”ã€‚ä½ éœ€è¦åšçš„åªæ˜¯[ä¸‹è½½ã€å®‰è£…](https://github.com/sunner/ChatALL/releases)å’Œæé—®ã€‚",
    "mimetype": "text/plain",
    "start_char_idx": 459,
    "end_char_idx": 650,
    "text_template": "{metadata_str}\n\n{content}",
    "metadata_template": "{key}: {value}",
    "metadata_seperator": "\n",
    "class_name": "TextNode"
}
{
    "id_": "c5dcf1d3-7a6a-48b0-b6d8-06457b182ac5",
    "embedding": null,
    "metadata": {
        "filename": "ChatALL.md",
        "extension": ".md",
        "Header_2": "åŠŸèƒ½",
        "Header_3": "è¿™æ˜¯ä½ å—ï¼Ÿ"
    },
    "excluded_embed_metadata_keys": [],
    "excluded_llm_metadata_keys": [],
    "relationships": {
        "1": {
            "node_id": "f3be7692-141b-41a6-8b3f-58402aaf36c1",
            "node_type": "4",
            "metadata": {
                "filename": "ChatALL.md",
                "extension": ".md"
            },
            "hash": "45b9149e0039c1ef7fbbd74f96923875505cc77916de48734ba7767f6a16a87e",
            "class_name": "RelatedNodeInfo"
        },
        "2": {
            "node_id": "5c2eb25f-08ac-4b2e-9050-bc6efa90304a",
            "node_type": "1",
            "metadata": {
                "filename": "ChatALL.md",
                "extension": ".md",
                "Header_2": "åŠŸèƒ½"
            },
            "hash": "90172566aa1795d0f9ac33c954d0b98fde63bf9176950d0ea38e87e4ab6563ed",
            "class_name": "RelatedNodeInfo"
        },
        "3": {
            "node_id": "7d959c37-6ff9-4bfe-b499-c509a24088cb",
            "node_type": "1",
            "metadata": {
                "Header_2": "åŠŸèƒ½",
                "Header_3": "æ”¯æŒçš„ AI"
            },
            "hash": "1b2b11abec9fc74b725b6c344f37d44736e8e991a3eebdbcfa4ab682506c7b2e",
            "class_name": "RelatedNodeInfo"
        }
    },
    "text": "è¿™æ˜¯ä½ å—ï¼Ÿ\n\nChatALL çš„å…¸å‹ç”¨æˆ·æ˜¯ï¼š\n\n- ğŸ¤ **å¤§æ¨¡å‹é‡åº¦ç©å®¶**ï¼Œå¸Œæœ›ä»å¤§æ¨¡å‹æ‰¾åˆ°æœ€å¥½çš„ç­”æ¡ˆï¼Œæˆ–è€…æœ€å¥½çš„åˆ›ä½œ\n- ğŸ¤“**å¤§æ¨¡å‹ç ”ç©¶è€…**ï¼Œç›´è§‚æ¯”è¾ƒå„ç§å¤§æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸçš„ä¼˜åŠ£\n- ğŸ˜**å¤§æ¨¡å‹åº”ç”¨å¼€å‘è€…**ï¼Œå¿«é€Ÿè°ƒè¯• promptï¼Œå¯»æ‰¾è¡¨ç°æœ€ä½³çš„åŸºç¡€æ¨¡å‹",
    "mimetype": "text/plain",
    "start_char_idx": 656,
    "end_char_idx": 788,
    "text_template": "{metadata_str}\n\n{content}",
    "metadata_template": "{key}: {value}",
    "metadata_seperator": "\n",
    "class_name": "TextNode"
}
```

æ›´å¤šçš„ `NodeParser` åŒ…æ‹¬ [`HTMLNodeParser`](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/html/)ï¼Œ[`JSONNodeParser`](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/json/)ç­‰ç­‰ã€‚

## 5ã€ç´¢å¼•ï¼ˆIndexingï¼‰ä¸æ£€ç´¢ï¼ˆRetrievalï¼‰

**åŸºç¡€æ¦‚å¿µ**ï¼šåœ¨ã€Œæ£€ç´¢ã€ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œã€Œç´¢å¼•ã€å³`index`ï¼Œ é€šå¸¸æ˜¯æŒ‡ä¸ºäº†å®ç°å¿«é€Ÿæ£€ç´¢è€Œè®¾è®¡çš„ç‰¹å®šã€Œæ•°æ®ç»“æ„ã€ã€‚

ç´¢å¼•çš„å…·ä½“åŸç†ä¸å®ç°ä¸æ˜¯æœ¬è¯¾ç¨‹çš„æ•™å­¦é‡ç‚¹ï¼Œæ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥å‚è€ƒï¼š[ä¼ ç»Ÿç´¢å¼•](https://en.wikipedia.org/wiki/Search_engine_indexing)ã€[å‘é‡ç´¢å¼•](https://medium.com/kx-systems/vector-indexing-a-roadmap-for-vector-databases-65866f07daf5)

### 5.1ã€å‘é‡æ£€ç´¢

1. `SimpleVectorStore` ç›´æ¥åœ¨å†…å­˜ä¸­æ„å»ºä¸€ä¸ª Vector Store å¹¶å»ºç´¢å¼•

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.core.node_parser import TokenTextSplitter
from llama_index.readers.file import PyMuPDFReader

# åŠ è½½ pdf æ–‡æ¡£
documents = SimpleDirectoryReader(
    "./data", 
    required_exts=[".pdf"],
    file_extractor={".pdf": PyMuPDFReader()}
).load_data()

# å®šä¹‰ Node Parser
node_parser = TokenTextSplitter(chunk_size=300, chunk_overlap=100)

# åˆ‡åˆ†æ–‡æ¡£
nodes = node_parser.get_nodes_from_documents(documents)

# æ„å»º index
index = VectorStoreIndex(nodes)

# è·å– retriever
vector_retriever = index.as_retriever(
    similarity_top_k=2 # è¿”å›2ä¸ªç»“æœ
)

# æ£€ç´¢
results = vector_retriever.retrieve("Llama2æœ‰å¤šå°‘å‚æ•°")

print(results[0].text)
print()
print(results[1].text)
```

**Output:**
```
an updated version of Llama 1, trained on a new mix of publicly available data. We also
increased the size of the pretraining corpus by 40%, doubled the context length of the model, and
adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with
7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper
but are not releasing.Â§
2. Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release
variants of this model with 7B, 13B, and 70B parameters as well.
We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,
Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021;
Solaiman et al., 2023). Testing conducted to date has been in English and has not â€” and could not â€” cover
all scenarios. Therefore, before deploying any applications of

Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich
Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra
Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi
Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang
Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang
Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic
Sergey Edunov
Thomas Scialomâˆ—
GenAI, Meta
Abstract
In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned
large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.
Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our
models outperform open-source chat models on most benchmarks we tested, and based on
our human evaluations for helpfulness and safety, may be a suitable substitute for
```

> âš ï¸ **Note:** LlamaIndex é»˜è®¤çš„ Embedding æ¨¡å‹æ˜¯ OpenAIEmbedding(model="text-embedding-ada-002")ã€‚
å¦‚ä½•æ›¿æ¢æŒ‡å®šçš„ Embedding æ¨¡å‹è§åé¢ç« èŠ‚è¯¦è§£ã€‚

2. ä½¿ç”¨è‡ªå®šä¹‰çš„ Vector Storeï¼Œä»¥ `Qdrant` ä¸ºä¾‹ï¼š

```python
!pip install llama-index-vector-stores-qdrant
```

```python
from llama_index.core.indices.vector_store.base import VectorStoreIndex
from llama_index.vector_stores.qdrant import QdrantVectorStore
from llama_index.core import StorageContext

from qdrant_client import QdrantClient
from qdrant_client.models import VectorParams, Distance

client = QdrantClient(location=":memory:")
collection_name = "demo"
collection = client.create_collection(
    collection_name=collection_name,
    vectors_config=VectorParams(size=1536, distance=Distance.COSINE)
)

vector_store = QdrantVectorStore(client=client, collection_name=collection_name)
# storage: æŒ‡å®šå­˜å‚¨ç©ºé—´
storage_context = StorageContext.from_defaults(vector_store=vector_store)

# åˆ›å»º indexï¼šé€šè¿‡ Storage Context å…³è”åˆ°è‡ªå®šä¹‰çš„ Vector Store
index = VectorStoreIndex(nodes, storage_context=storage_context)

# è·å– retriever
vector_retriever = index.as_retriever(similarity_top_k=2)

# æ£€ç´¢
results = vector_retriever.retrieve("Llama2æœ‰å¤šå°‘å‚æ•°")

print(results[0])
print(results[1])
```

**Output:**
```
Node ID: bd3ff0be-13b9-4f7c-bac3-71ef38978766
Text: an updated version of Llama 1, trained on a new mix of publicly
available data. We also increased the size of the pretraining corpus
by 40%, doubled the context length of the model, and adopted grouped-
query attention (Ainslie et al., 2023). We are releasing variants of
Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B
variants,...
Score:  0.790

Node ID: 18282125-5744-4da8-afe6-32b208304571
Text: Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana
Liskovich Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar
Mishra Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi
Rungta Kalyan Saladi Alan Schelten Ruan Silva Eric Michael Smith
Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang Ross Taylor Adina
Williams Jian Xiang...
Score:  0.789
```

### 5.2ã€æ›´å¤šç´¢å¼•ä¸æ£€ç´¢æ–¹å¼

LlamaIndex å†…ç½®äº†ä¸°å¯Œçš„æ£€ç´¢æœºåˆ¶ï¼Œä¾‹å¦‚ï¼š

- å…³é”®å­—æ£€ç´¢

  - [`BM25Retriever`](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/bm25/)ï¼šåŸºäº tokenizer å®ç°çš„ BM25 ç»å…¸æ£€ç´¢ç®—æ³•
  - [`KeywordTableGPTRetriever`](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/keyword/#llama_index.core.indices.keyword_table.retrievers.KeywordTableGPTRetriever)ï¼šä½¿ç”¨ GPT æå–æ£€ç´¢å…³é”®å­—
  - [`KeywordTableSimpleRetriever`](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/keyword/#llama_index.core.indices.keyword_table.retrievers.KeywordTableSimpleRetriever)ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ£€ç´¢å…³é”®å­—
  - [`KeywordTableRAKERetriever`](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/keyword/#llama_index.core.indices.keyword_table.retrievers.KeywordTableRAKERetriever)ï¼šä½¿ç”¨[`RAKE`](https://pypi.org/project/rake-nltk/)ç®—æ³•æå–æ£€ç´¢å…³é”®å­—ï¼ˆæœ‰è¯­è¨€é™åˆ¶ï¼‰

- RAG-Fusion [`QueryFusionRetriever`](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/query_fusion/)

- è¿˜æ”¯æŒ [KnowledgeGraph](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/knowledge_graph/)ã€[SQL](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/sql/#llama_index.core.retrievers.SQLRetriever)ã€[Text-to-SQL](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/sql/#llama_index.core.retrievers.NLSQLRetriever) ç­‰ç­‰

### 5.3ã€Ingestion Pipeline è‡ªå®šä¹‰æ•°æ®å¤„ç†æµç¨‹

LlamaIndex é€šè¿‡ `Transformations` å®šä¹‰ä¸€ä¸ªæ•°æ®ï¼ˆ`Documents`ï¼‰çš„å¤šæ­¥å¤„ç†çš„æµç¨‹ï¼ˆPipelineï¼‰ã€‚
è¿™ä¸ª Pipeline çš„ä¸€ä¸ªæ˜¾è‘—ç‰¹ç‚¹æ˜¯ï¼Œ**å®ƒçš„æ¯ä¸ªå­æ­¥éª¤æ˜¯å¯ä»¥ç¼“å­˜ï¼ˆcacheï¼‰çš„**ï¼Œå³å¦‚æœè¯¥å­æ­¥éª¤çš„è¾“å…¥ä¸å¤„ç†æ–¹æ³•ä¸å˜ï¼Œé‡å¤è°ƒç”¨æ—¶ä¼šç›´æ¥ä»ç¼“å­˜ä¸­è·å–ç»“æœï¼Œè€Œæ— éœ€é‡æ–°æ‰§è¡Œè¯¥å­æ­¥éª¤ï¼Œè¿™æ ·å³èŠ‚çœæ—¶é—´ä¹Ÿä¼šèŠ‚çœ token ï¼ˆå¦‚æœå­æ­¥éª¤æ¶‰åŠå¤§æ¨¡å‹è°ƒç”¨ï¼‰ã€‚

```python
import time

class Timer:
    def __enter__(self):
        self.start = time.time()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.end = time.time()
        self.interval = self.end - self.start
        print(f"è€—æ—¶ {self.interval*1000} ms")
```

```python
from llama_index.core.indices.vector_store.base import VectorStoreIndex
from llama_index.vector_stores.qdrant import QdrantVectorStore
from llama_index.core import StorageContext

from qdrant_client import QdrantClient
from qdrant_client.models import VectorParams, Distance

from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.extractors import TitleExtractor
from llama_index.core.ingestion import IngestionPipeline
from llama_index.core import VectorStoreIndex
from llama_index.readers.file import PyMuPDFReader
import nest_asyncio
nest_asyncio.apply() # åªåœ¨Jupyterç¬”è®°ç¯å¢ƒä¸­éœ€è¦æ­¤æ“ä½œï¼Œå¦åˆ™ä¼šæŠ¥é”™

client = QdrantClient(location=":memory:")
collection_name = "ingestion_demo"

collection = client.create_collection(
    collection_name=collection_name,
    vectors_config=VectorParams(size=1536, distance=Distance.COSINE)
)

# åˆ›å»º Vector Store
vector_store = QdrantVectorStore(client=client, collection_name=collection_name)

pipeline = IngestionPipeline(
    transformations=[
        SentenceSplitter(chunk_size=300, chunk_overlap=100), # æŒ‰å¥å­åˆ‡åˆ†
        TitleExtractor(), # åˆ©ç”¨ LLM å¯¹æ–‡æœ¬ç”Ÿæˆæ ‡é¢˜
        OpenAIEmbedding(), # å°†æ–‡æœ¬å‘é‡åŒ–
    ],
    vector_store=vector_store,
)

documents = SimpleDirectoryReader(
    "./data", 
    required_exts=[".pdf"],
    file_extractor={".pdf": PyMuPDFReader()}
).load_data()

# è®¡æ—¶
with Timer():
    # Ingest directly into a vector db
    pipeline.run(documents=documents)

# åˆ›å»ºç´¢å¼•
index = VectorStoreIndex.from_vector_store(vector_store)

# è·å– retriever
vector_retriever = index.as_retriever(similarity_top_k=1)

# æ£€ç´¢
results = vector_retriever.retrieve("Llama2æœ‰å¤šå°‘å‚æ•°")

print(results[0])
```

**Output:**
```
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.99it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  5.62it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  5.85it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.68it/s]
è€—æ—¶ 8875.431299209595 ms
Node ID: b48b5c44-9287-4056-bdae-f8dc1393d44d
Text: Llama 2, an updated version of Llama 1, trained on a new mix of
publicly available data. We also increased the size of the pretraining
corpus by 40%, doubled the context length of the model, and adopted
grouped-query attention (Ainslie et al., 2023). We are releasing
variants of Llama 2 with 7B, 13B, and 70B parameters. We have also
trained 34B ...
Score:  0.783
```

æœ¬åœ°ä¿å­˜ `IngestionPipeline` çš„ç¼“å­˜

```python
pipeline.persist("./pipeline_storage")
```

```python
new_pipeline = IngestionPipeline(
    transformations=[
        SentenceSplitter(chunk_size=300, chunk_overlap=100),
        TitleExtractor(),
    ],
)

# åŠ è½½ç¼“å­˜
new_pipeline.load("./pipeline_storage")

with Timer():
    nodes = new_pipeline.run(documents=documents)
```

**Output:**
```
è€—æ—¶ 3.515005111694336 ms
```

æ­¤å¤–ï¼Œä¹Ÿå¯ä»¥ç”¨è¿œç¨‹çš„ Redis æˆ– MongoDB ç­‰å­˜å‚¨ `IngestionPipeline` çš„ç¼“å­˜ï¼Œå…·ä½“å‚è€ƒå®˜æ–¹æ–‡æ¡£ï¼š[Remote Cache Management](https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/#remote-cache-management)ã€‚

`IngestionPipeline` ä¹Ÿæ”¯æŒå¼‚æ­¥å’Œå¹¶å‘è°ƒç”¨ï¼Œè¯·å‚è€ƒå®˜æ–¹æ–‡æ¡£ï¼š[Async Support](https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/#async-support)ã€[Parallel Processing](https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/#parallel-processing)ã€‚

### 5.4ã€æ£€ç´¢åå¤„ç†

LlamaIndex çš„ `Node Postprocessors` æä¾›äº†ä¸€ç³»åˆ—æ£€ç´¢åå¤„ç†æ¨¡å—ã€‚

ä¾‹å¦‚ï¼šæˆ‘ä»¬å¯ä»¥ç”¨ä¸åŒæ¨¡å‹å¯¹æ£€ç´¢åçš„ `Nodes` åšé‡æ’åº

```python
# è·å– retriever
vector_retriever = index.as_retriever(similarity_top_k=5)

# æ£€ç´¢
nodes = vector_retriever.retrieve("Llama2 æœ‰å•†ç”¨è®¸å¯å—?")

for i, node in enumerate(nodes):
    print(f"[{i}] {node.text}\n")
```

**Output:**
```
[0] Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also
increased the size of the pretraining corpus by 40%, doubled the context length of the model, and
adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with
7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper
but are not releasing.Â§
2. Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release
variants of this model with 7B, 13B, and 70B parameters as well.
We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,
Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021;
Solaiman et al., 2023).

[1] We release
variants of this model with 7B, 13B, and 70B parameters as well.
We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,
Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021;
Solaiman et al., 2023). Testing conducted to date has been in English and has not â€” and could not â€” cover
all scenarios. Therefore, before deploying any applications of Llama 2-Chat, developers should perform
safety testing and tuning tailored to their specific applications of the model. We provide a responsible use
guideÂ¶ and code examplesâ€– to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of
our responsible release strategy can be found in Section 5.3.

[2] Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich
Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra
Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi
Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang
Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang
Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic
Sergey Edunov
Thomas Scialomâˆ—
GenAI, Meta
Abstract
In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned
large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.
Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases.

[3] These closed product LLMs are heavily fine-tuned to align with human
preferences, which greatly enhances their usability and safety. This step can require significant costs in
compute and human annotation, and is often not transparent or easily reproducible, limiting progress within
the community to advance AI alignment research.
In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and
Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,
Llama 2-Chat models generally perform better than existing open-source models. They also appear to
be on par with some of the closed-source models, at least on the human evaluations we performed (see
Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data
annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,
this paper contributes a thorough description of our fine-tuning methodology and approach to improving
LLM safety.

[4] Figure 3: Safety human evaluation results for Llama 2-Chat compared to other open-source and closed-
source models. Human raters judged model generations for safety violations across ~2,000 adversarial
prompts consisting of both single and multi-turn prompts. More details can be found in Section 4.4. It is
important to caveat these safety results with the inherent bias of LLM evaluations due to limitations of the
prompt set, subjectivity of the review guidelines, and subjectivity of individual raters. Additionally, these
safety evaluations are performed using content standards that are likely to be biased towards the Llama
2-Chat models.
We are releasing the following models to the general public for research and commercial useâ€¡:
1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also
increased the size of the pretraining corpus by 40%, doubled the context length of the model, and
adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with
7B, 13B, and 70B parameters.
```

> âŒ **Warning:** ä»¥ä¸‹ä»£ç ä¸è¦åœ¨æœåŠ¡å™¨ä¸Šè¿è¡Œï¼Œä¼šæ­»æœºï¼
å¯ä¸‹è½½å·¦ä¾§ rag_demo.py çš„å®Œæ•´ä¾‹å­åœ¨è‡ªå·±æœ¬åœ°è¿è¡Œã€‚

```python
from llama_index.core.postprocessor import SentenceTransformerRerank

# æ£€ç´¢åæ’åºæ¨¡å‹
postprocessor = SentenceTransformerRerank(
    model="BAAI/bge-reranker-large", top_n=2
)

nodes = postprocessor.postprocess_nodes(nodes, query_str="Llama2 æœ‰å•†ç”¨è®¸å¯å—?")

for i, node in enumerate(nodes):
    print(f"[{i}] {node.text}")
```

**Output:**
```
/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
/home/jovyan/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[0] Figure 3: Safety human evaluation results for Llama 2-Chat compared to other open-source and closed-
source models. Human raters judged model generations for safety violations across ~2,000 adversarial
prompts consisting of both single and multi-turn prompts. More details can be found in Section 4.4. It is
important to caveat these safety results with the inherent bias of LLM evaluations due to limitations of the
prompt set, subjectivity of the review guidelines, and subjectivity of individual raters. Additionally, these
safety evaluations are performed using content standards that are likely to be biased towards the Llama
2-Chat models.
We are releasing the following models to the general public for research and commercial useâ€¡:
1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also
increased the size of the pretraining corpus by 40%, doubled the context length of the model, and
adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with
7B, 13B, and 70B parameters.
[1] Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also
increased the size of the pretraining corpus by 40%, doubled the context length of the model, and
adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with
7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper
but are not releasing.Â§
2. Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release
variants of this model with 7B, 13B, and 70B parameters as well.
We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,
Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021;
Solaiman et al., 2023).
```

æ›´å¤šçš„ Rerank åŠå…¶å®ƒåå¤„ç†æ–¹æ³•ï¼Œå‚è€ƒå®˜æ–¹æ–‡æ¡£ï¼š[Node Postprocessor Modules](https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/node_postprocessors/)

## 6ã€ç”Ÿæˆå›å¤ï¼ˆQA & Chatï¼‰

### 6.1ã€å•è½®é—®ç­”ï¼ˆQuery Engineï¼‰

```python
qa_engine = index.as_query_engine()
response = qa_engine.query("Llama2 æœ‰å¤šå°‘å‚æ•°?")

print(response)
```

**Output:**
```
Llama 2æœ‰7B, 13B, å’Œ70Bå‚æ•°ã€‚
```

#### æµå¼è¾“å‡º

```python
qa_engine = index.as_query_engine(streaming=True)
response = qa_engine.query("Llama2 æœ‰å¤šå°‘å‚æ•°?")
response.print_response_stream()
```

**Output:**
```
Llama 2æœ‰7B, 13B, å’Œ70Bå‚æ•°ã€‚
```

### 6.2ã€å¤šè½®å¯¹è¯ï¼ˆChat Engineï¼‰

```python
chat_engine = index.as_chat_engine()
response = chat_engine.chat("Llama2 æœ‰å¤šå°‘å‚æ•°?")
print(response)
```

**Output:**
```
Llama2 æœ‰7B, 13B, å’Œ70Bå‚æ•°ã€‚
```

```python
response = chat_engine.chat("How many at most?")
print(response)
```

**Output:**
```
Llama2 æœ€å¤šæœ‰70Bå‚æ•°ã€‚
```

#### æµå¼è¾“å‡º

```python
chat_engine = index.as_chat_engine()
streaming_response = chat_engine.stream_chat("Llama 2æœ‰å¤šå°‘å‚æ•°?")
# streaming_response.print_response_stream()
for token in streaming_response.response_gen:
    print(token, end="", flush=True)
```

**Output:**
```
Llama 2æœ‰7Bã€13Bå’Œ70Bå‚æ•°å˜ä½“ã€‚
```

## 7ã€åº•å±‚æ¥å£ï¼šPromptã€LLM ä¸ Embedding

### 7.1ã€Prompt æ¨¡æ¿

#### `PromptTemplate` å®šä¹‰æç¤ºè¯æ¨¡æ¿

```python
from llama_index.core import PromptTemplate

prompt = PromptTemplate("å†™ä¸€ä¸ªå…³äº{topic}çš„ç¬‘è¯")

prompt.format(topic="å°æ˜")
```

**Output:**
```
'å†™ä¸€ä¸ªå…³äºå°æ˜çš„ç¬‘è¯'
```

#### `ChatPromptTemplate` å®šä¹‰å¤šè½®æ¶ˆæ¯æ¨¡æ¿

```python
from llama_index.core.llms import ChatMessage, MessageRole
from llama_index.core import ChatPromptTemplate

chat_text_qa_msgs = [
    ChatMessage(
        role=MessageRole.SYSTEM,
        content="ä½ å«{name}ï¼Œä½ å¿…é¡»æ ¹æ®ç”¨æˆ·æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚",
    ),
    ChatMessage(
        role=MessageRole.USER, 
        content=(
            "å·²çŸ¥ä¸Šä¸‹æ–‡ï¼š\n" \
            "{context}\n\n" \
            "é—®é¢˜ï¼š{question}"
        )
    ),
]
text_qa_template = ChatPromptTemplate(chat_text_qa_msgs)

print(
    text_qa_template.format(
        name="ç“œç“œ",
        context="è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•",
        question="è¿™æ˜¯ä»€ä¹ˆ"
    )
)
```

**Output:**
```
system: ä½ å«ç“œç“œï¼Œä½ å¿…é¡»æ ¹æ®ç”¨æˆ·æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚
user: å·²çŸ¥ä¸Šä¸‹æ–‡ï¼š
è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•

é—®é¢˜ï¼šè¿™æ˜¯ä»€ä¹ˆ
assistant:
```

### 7.2ã€è¯­è¨€æ¨¡å‹

```python
from llama_index.llms.openai import OpenAI

llm = OpenAI(temperature=0, model="gpt-4o")
```

```python
response = llm.complete(prompt.format(topic="å°æ˜"))

print(response.text)
```

**Output:**
```
å°æ˜æœ‰ä¸€å¤©å»å‚åŠ ä¸€ä¸ªæ™ºåŠ›ç«èµ›ï¼Œä¸»æŒäººé—®ä»–ï¼šâ€œå°æ˜ï¼Œè¯·ç”¨â€˜å› ä¸ºâ€™å’Œâ€˜æ‰€ä»¥â€™é€ ä¸€ä¸ªå¥å­ã€‚â€

å°æ˜æƒ³äº†æƒ³ï¼Œè¯´ï¼šâ€œå› ä¸ºä»Šå¤©æˆ‘æ²¡å¸¦ä½œä¸šï¼Œæ‰€ä»¥æˆ‘æ¥å‚åŠ æ¯”èµ›äº†ã€‚â€
```

```python
response = llm.complete(
    text_qa_template.format(
        name="ç“œç“œ",
        context="è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•",
        question="ä½ æ˜¯è°ï¼Œæˆ‘ä»¬åœ¨å¹²å˜›"
    )
)

print(response.text)
```

**Output:**
```
æˆ‘æ˜¯ç“œç“œï¼Œä½ çš„æ™ºèƒ½åŠ©æ‰‹ã€‚æ ¹æ®ä½ æä¾›çš„ä¸Šä¸‹æ–‡ï¼Œæˆ‘ä»¬æ­£åœ¨è¿›è¡Œä¸€ä¸ªæµ‹è¯•ã€‚è¯·è®©æˆ‘çŸ¥é“å¦‚æœä½ æœ‰å…¶ä»–é—®é¢˜æˆ–éœ€è¦è¿›ä¸€æ­¥çš„å¸®åŠ©ï¼
```

#### è®¾ç½®å…¨å±€ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹

```python
from llama_index.core import Settings

Settings.llm = OpenAI(temperature=0, model="gpt-4o")
```

é™¤ OpenAI å¤–ï¼ŒLlamaIndex å·²é›†æˆå¤šä¸ªå¤§è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬äº‘æœåŠ¡ API å’Œæœ¬åœ°éƒ¨ç½² APIï¼Œè¯¦è§å®˜æ–¹æ–‡æ¡£ï¼š[Available LLM integrations](https://docs.llamaindex.ai/en/stable/module_guides/models/llms/modules/)

### 7.3ã€Embedding æ¨¡å‹

```python
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core import Settings

# å…¨å±€è®¾å®š
Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small", dimensions=512)
```

LlamaIndex åŒæ ·é›†æˆäº†å¤šç§ Embedding æ¨¡å‹ï¼ŒåŒ…æ‹¬äº‘æœåŠ¡ API å’Œå¼€æºæ¨¡å‹ï¼ˆHuggingFaceï¼‰ç­‰ï¼Œè¯¦è§[å®˜æ–¹æ–‡æ¡£](https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/)ã€‚

## 8ã€åŸºäº LlamaIndex å®ç°ä¸€ä¸ªåŠŸèƒ½è¾ƒå®Œæ•´çš„ RAG ç³»ç»Ÿ

åŠŸèƒ½è¦æ±‚ï¼š

- åŠ è½½æŒ‡å®šç›®å½•çš„æ–‡ä»¶
- æ”¯æŒ RAG-Fusion
- ä½¿ç”¨ Qdrant å‘é‡æ•°æ®åº“ï¼Œå¹¶æŒä¹…åŒ–åˆ°æœ¬åœ°
- æ”¯æŒæ£€ç´¢åæ’åº
- æ”¯æŒå¤šè½®å¯¹è¯

[//]: # (> âŒ **Warning:** ä»¥ä¸‹ä»£ç ä¸è¦åœ¨æœåŠ¡å™¨ä¸Šè¿è¡Œï¼Œä¼šæ­»æœºï¼å¯ä¸‹è½½å·¦ä¾§ rag_demo.py åœ¨è‡ªå·±æœ¬åœ°è¿è¡Œã€‚)

```python
from qdrant_client import QdrantClient
from qdrant_client.models import VectorParams, Distance

EMBEDDING_DIM = 512
COLLECTION_NAME = "full_demo"
PATH = "./qdrant_db"

client = QdrantClient(path=PATH)
```

```python
from llama_index.core import VectorStoreIndex, KeywordTableIndex, SimpleDirectoryReader
from llama_index.vector_stores.qdrant import QdrantVectorStore
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.ingestion import IngestionPipeline
from llama_index.readers.file import PyMuPDFReader
from llama_index.core import Settings
from llama_index.core import StorageContext
from llama_index.core.postprocessor import SentenceTransformerRerank
from llama_index.core.retrievers import QueryFusionRetriever
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.chat_engine import CondenseQuestionChatEngine
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
import time

# 1. æŒ‡å®šå…¨å±€llmä¸embeddingæ¨¡å‹
Settings.llm = OpenAI(temperature=0, model="gpt-4o")
Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small", dimensions=EMBEDDING_DIM)
# 2. æŒ‡å®šå…¨å±€æ–‡æ¡£å¤„ç†çš„ Ingestion Pipeline
Settings.transformations = [SentenceSplitter(chunk_size=300, chunk_overlap=100)]

# 3. åŠ è½½æœ¬åœ°æ–‡æ¡£
documents = SimpleDirectoryReader("./data", file_extractor={".pdf": PyMuPDFReader()}).load_data()

if client.collection_exists(collection_name=COLLECTION_NAME):
    client.delete_collection(collection_name=COLLECTION_NAME)

# 4. åˆ›å»º collection
client.create_collection(
    collection_name=COLLECTION_NAME,
    vectors_config=VectorParams(size=EMBEDDING_DIM, distance=Distance.COSINE)
)

# 5. åˆ›å»º Vector Store
vector_store = QdrantVectorStore(client=client, collection_name=COLLECTION_NAME)

# 6. æŒ‡å®š Vector Store çš„ Storage ç”¨äº index
storage_context = StorageContext.from_defaults(vector_store=vector_store)
index = VectorStoreIndex.from_documents(
    documents, storage_context=storage_context
)

# 7. å®šä¹‰æ£€ç´¢åæ’åºæ¨¡å‹
reranker = SentenceTransformerRerank(
    model="BAAI/bge-reranker-large", top_n=2
)

# 8. å®šä¹‰ RAG Fusion æ£€ç´¢å™¨
fusion_retriever = QueryFusionRetriever(
    [index.as_retriever()],
    similarity_top_k=5, # æ£€ç´¢å¬å› top k ç»“æœ
    num_queries=3,  # ç”Ÿæˆ query æ•°
    use_async=False,
    # query_gen_prompt="...",  # å¯ä»¥è‡ªå®šä¹‰ query ç”Ÿæˆçš„ prompt æ¨¡æ¿
)

# 9. æ„å»ºå•è½® query engine
query_engine = RetrieverQueryEngine.from_args(
    fusion_retriever,
    node_postprocessors=[reranker]
)

# 10. å¯¹è¯å¼•æ“
chat_engine = CondenseQuestionChatEngine.from_defaults(
    query_engine=query_engine, 
    # condense_question_prompt=... # å¯ä»¥è‡ªå®šä¹‰ chat message prompt æ¨¡æ¿
)
```

```python
while True:
    question=input("User:")
    if question.strip() == "":
        break
    response = chat_engine.chat(question)
    print(f"AI: {response}")
```

**Output:**
```
User: llama2æœ‰å¤šå°‘å‚æ•°
AI: Llama 2 æœ‰ 7Bã€13B å’Œ 70B å‚æ•°çš„å˜ä½“ã€‚
User: æœ€å¤šå¤šå°‘
AI: Llama 2 çš„å˜ä½“ä¸­å‚æ•°æœ€å¤šçš„æ˜¯ 70Bã€‚
User:
```

## LlamaIndex çš„æ›´å¤šåŠŸèƒ½

- æ™ºèƒ½ä½“ï¼ˆAgentï¼‰å¼€å‘æ¡†æ¶ï¼šhttps://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/
- RAG çš„è¯„æµ‹ï¼šhttps://docs.llamaindex.ai/en/stable/module_guides/evaluating/
- è¿‡ç¨‹ç›‘æ§ï¼šhttps://docs.llamaindex.ai/en/stable/module_guides/observability/

ä»¥ä¸Šå†…å®¹æ¶‰åŠè¾ƒå¤šèƒŒæ™¯çŸ¥è¯†ï¼Œæš‚æ—¶ä¸åœ¨æœ¬è¯¾å±•å¼€ï¼Œç›¸å…³çŸ¥è¯†ä¼šåœ¨åé¢è¯¾ç¨‹ä¸­é€ä¸€è¯¦ç»†è®²è§£ã€‚

æ­¤å¤–ï¼ŒLlamaIndex é’ˆå¯¹ç”Ÿäº§çº§çš„ RAG ç³»ç»Ÿä¸­é‡åˆ°çš„å„ä¸ªæ–¹é¢çš„ç»†èŠ‚é—®é¢˜ï¼Œæ€»ç»“äº†å¾ˆå¤šé«˜ç«¯æŠ€å·§ï¼ˆ[Advanced Topics](https://docs.llamaindex.ai/en/stable/optimizing/production_rag/)ï¼‰ï¼Œå¯¹å®æˆ˜å¾ˆæœ‰å‚è€ƒä»·å€¼ï¼Œéå¸¸æ¨èæœ‰èƒ½åŠ›çš„åŒå­¦é˜…è¯»ã€‚

## ä½œä¸š

1. åŸºäº LlamaIndex å®ç°ä¸€ä¸ªè‡ªå·± RAG ç³»ç»Ÿã€‚
2. æŸ¥é˜… LlamaIndex æ–‡æ¡£å’Œç½‘ä¸Šç›¸å…³èµ„æ–™ï¼Œæ€è€ƒå¦‚ä½•è®©ç¬¬ 8 èŠ‚ä¸­çš„ç³»ç»Ÿæ”¯æŒå…³é”®å­—ä¸å‘é‡çš„æ··åˆæ£€ç´¢ï¼ˆé€‰ï¼‰