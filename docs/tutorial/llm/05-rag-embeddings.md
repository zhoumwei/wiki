# 05 Rag Embeddings   Index



## ğŸ’¡ è¿™èŠ‚è¯¾ä¼šå¸¦ç»™ä½ 

1. å¦‚ä½•ç”¨ä½ çš„å‚åŸŸæ•°æ®è¡¥å…… LLM çš„èƒ½åŠ›
2. å¦‚ä½•æ„å»ºä½ çš„å‚åŸŸï¼ˆå‘é‡ï¼‰çŸ¥è¯†åº“
3. æ­å»ºä¸€å¥—å®Œæ•´ RAG ç³»ç»Ÿéœ€è¦å“ªäº›æ¨¡å—
4. æ­å»º RAG ç³»ç»Ÿæ—¶æ›´å¤šçš„æœ‰ç”¨æŠ€å·§

å¼€å§‹ä¸Šè¯¾ï¼

## ğŸ“ è¿™èŠ‚è¯¾æ€ä¹ˆå­¦

ä»£ç èƒ½åŠ›è¦æ±‚ï¼š**ä¸­**ï¼ŒAI/æ•°å­¦åŸºç¡€è¦æ±‚ï¼š**ä½**

1. æœ‰ç¼–ç¨‹åŸºç¡€çš„åŒå­¦
   - å…³æ³¨æŠ€æœ¯åŸç†å’Œæ–‡æœ¬å¤„ç†ä¸­å¸¸è§çš„é—®é¢˜ä¸æŠ€å·§
2. æ²¡æœ‰ç¼–ç¨‹åŸºç¡€çš„åŒå­¦
   - å…³æ³¨å®è§‚åŸç†å’Œæ€è·¯
   - **æ€è·¯æ›´é‡è¦ï¼Œä»£ç åªæ˜¯å®ç°æƒ³æ³•çš„å·¥å…·**

## ä¸€ã€æ¾„æ¸…ä¸€ä¸ªæ¦‚å¿µ

RAG **ä¸è¦** å‚è€ƒä¸‹é¢è¿™å¼ å›¾ï¼ï¼ï¼

![image](./assets/05-rag-embeddings/rag-paper.png)

è¿™å¼ å›¾æºè‡ªä¸€ä¸ª[ç ”ç©¶å·¥ä½œ](https://arxiv.org/pdf/2005.11401.pdf)

- æ­¤è®ºæ–‡ç¬¬ä¸€æ¬¡æå‡º RAG è¿™ä¸ªå«æ³•
- åœ¨ç ”ç©¶ä¸­ï¼Œä½œè€…å°è¯•å°†æ£€ç´¢å’Œç”Ÿæˆåšåœ¨ä¸€ä¸ªæ¨¡å‹ä½“ç³»ä¸­

**ä½†æ˜¯ï¼Œå®é™…ç”Ÿäº§ä¸­ï¼ŒRAG ä¸æ˜¯è¿™ä¹ˆåšçš„ï¼ï¼ï¼**

## äºŒã€ä»€ä¹ˆæ˜¯æ£€ç´¢å¢å¼ºçš„ç”Ÿæˆæ¨¡å‹ï¼ˆRAGï¼‰

### 2.1ã€LLM å›ºæœ‰çš„å±€é™æ€§

1. LLM çš„çŸ¥è¯†ä¸æ˜¯å®æ—¶çš„
2. LLM å¯èƒ½ä¸çŸ¥é“ä½ ç§æœ‰çš„é¢†åŸŸ/ä¸šåŠ¡çŸ¥è¯†

![image](./assets/05-rag-embeddings/gpt-llama2.png)

### 2.2ã€æ£€ç´¢å¢å¼ºç”Ÿæˆ

RAGï¼ˆRetrieval Augmented Generationï¼‰é¡¾åæ€ä¹‰ï¼Œé€šè¿‡**æ£€ç´¢**çš„æ–¹æ³•æ¥å¢å¼º**ç”Ÿæˆæ¨¡å‹**çš„èƒ½åŠ›ã€‚

<video src="/wiki/tutorial/llm/assets/05-rag-embeddings/RAG.mp4" controls="controls" width=800px style="margin-left: 0px"></video>

> âœ… **Tip:** ç±»æ¯”ï¼šä½ å¯ä»¥æŠŠè¿™ä¸ªè¿‡ç¨‹æƒ³è±¡æˆå¼€å·è€ƒè¯•ã€‚è®© LLM å…ˆç¿»ä¹¦ï¼Œå†å›ç­”é—®é¢˜ã€‚

## ä¸‰ã€RAG ç³»ç»Ÿçš„åŸºæœ¬æ­å»ºæµç¨‹

å…ˆçœ‹æ•ˆæœï¼šhttp://localhost:9999/

æ­å»ºè¿‡ç¨‹ï¼š

1. æ–‡æ¡£åŠ è½½ï¼Œå¹¶æŒ‰ä¸€å®šæ¡ä»¶**åˆ‡å‰²**æˆç‰‡æ®µ
2. å°†åˆ‡å‰²çš„æ–‡æœ¬ç‰‡æ®µçŒå…¥**æ£€ç´¢å¼•æ“**
3. å°è£…**æ£€ç´¢æ¥å£**
4. æ„å»º**è°ƒç”¨æµç¨‹**ï¼šQuery -> æ£€ç´¢ -> Prompt -> LLM -> å›å¤

### 3.1ã€æ–‡æ¡£çš„åŠ è½½ä¸åˆ‡å‰²

```python
# !pip install --upgrade openai
```

```python
# å®‰è£… pdf è§£æåº“
# !pip install pdfminer.six
```

```python
from pdfminer.high_level import extract_pages
from pdfminer.layout import LTTextContainer
```

```python
def extract_text_from_pdf(filename, page_numbers=None, min_line_length=1):
    '''ä» PDF æ–‡ä»¶ä¸­ï¼ˆæŒ‰æŒ‡å®šé¡µç ï¼‰æå–æ–‡å­—'''
    paragraphs = []
    buffer = ''
    full_text = ''
    # æå–å…¨éƒ¨æ–‡æœ¬
    for i, page_layout in enumerate(extract_pages(filename)):
        # å¦‚æœæŒ‡å®šäº†é¡µç èŒƒå›´ï¼Œè·³è¿‡èŒƒå›´å¤–çš„é¡µ
        if page_numbers is not None and i not in page_numbers:
            continue
        for element in page_layout:
            if isinstance(element, LTTextContainer):
                full_text += element.get_text() + '\n'
    # æŒ‰ç©ºè¡Œåˆ†éš”ï¼Œå°†æ–‡æœ¬é‡æ–°ç»„ç»‡æˆæ®µè½
    lines = full_text.split('\n')
    for text in lines:
        if len(text) >= min_line_length:
            buffer += (' '+text) if not text.endswith('-') else text.strip('-')
        elif buffer:
            paragraphs.append(buffer)
            buffer = ''
    if buffer:
        paragraphs.append(buffer)
    return paragraphs
```

```python
paragraphs = extract_text_from_pdf("llama2.pdf", min_line_length=10)
```

```python
for para in paragraphs[:4]:
    print(para+"\n")
```

**Output:**
```
Llama 2: Open Foundation and Fine-Tuned Chat Models

 Hugo Touvronâˆ— Louis Martinâ€  Kevin Stoneâ€  Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic Sergey Edunov Thomas Scialomâˆ—

 GenAI, Meta

 In this work, we develop and release Llama 2, a collection of pretrained and ï¬ne-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our ï¬ne-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based onour human evaluations for helpfulness and safety, may be a suitable substitute for closed source models. We provide a detailed description of our approach to ï¬ne-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.
```

### 3.2ã€æ£€ç´¢å¼•æ“

å…ˆçœ‹ä¸€ä¸ªæœ€åŸºç¡€çš„å®ç°

### å®‰è£… ES å®¢æˆ·ç«¯

> âœ… **Tip:** ä¸ºäº†å®éªŒå®¤æ€§èƒ½
    ä»¥ä¸‹å®‰è£…åŒ…å·²ç»å†…ç½®å®éªŒå¹³å°

#!pip install elasticsearch7

### å®‰è£… NLTKï¼ˆæ–‡æœ¬å¤„ç†æ–¹æ³•åº“ï¼‰

#!pip install nltk

```python
from elasticsearch7 import Elasticsearch, helpers
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk
import re

import warnings
warnings.simplefilter("ignore")  # å±è”½ ES çš„ä¸€äº›Warnings

# å®éªŒå®¤å¹³å°å·²ç»å†…ç½®
nltk.download('punkt')  # è‹±æ–‡åˆ‡è¯ã€è¯æ ¹ã€åˆ‡å¥ç­‰æ–¹æ³•
nltk.download('stopwords')  # è‹±æ–‡åœç”¨è¯åº“
nltk.download('punkt_tab')
```

**Output:**
```
[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
[nltk_data] Downloading package punkt_tab to /home/jovyan/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
True
```

```python
def to_keywords(input_string):
    '''ï¼ˆè‹±æ–‡ï¼‰æ–‡æœ¬åªä¿ç•™å…³é”®å­—'''
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢æ‰€æœ‰éå­—æ¯æ•°å­—çš„å­—ç¬¦ä¸ºç©ºæ ¼
    no_symbols = re.sub(r'[^a-zA-Z0-9\s]', ' ', input_string)
    word_tokens = word_tokenize(no_symbols)
    # åŠ è½½åœç”¨è¯è¡¨
    stop_words = set(stopwords.words('english'))
    ps = PorterStemmer()
    # å»åœç”¨è¯ï¼Œå–è¯æ ¹
    filtered_sentence = [ps.stem(w)
                         for w in word_tokens if not w.lower() in stop_words]
    return ' '.join(filtered_sentence)
```

> â„¹ï¸ **Info:** æ­¤å¤„ to_keywords ä¸ºé’ˆå¯¹è‹±æ–‡çš„å®ç°ï¼Œé’ˆå¯¹ä¸­æ–‡çš„å®ç°è¯·å‚è€ƒ chinese_utils.py

å°†æ–‡æœ¬çŒå…¥æ£€ç´¢å¼•æ“

```python
import os, time

# å¼•å…¥é…ç½®æ–‡ä»¶
ELASTICSEARCH_BASE_URL = os.getenv('ELASTICSEARCH_BASE_URL')
ELASTICSEARCH_PASSWORD = os.getenv('ELASTICSEARCH_PASSWORD')
ELASTICSEARCH_NAME= os.getenv('ELASTICSEARCH_NAME')

# tips: å¦‚æœæƒ³åœ¨æœ¬åœ°è¿è¡Œï¼Œè¯·åœ¨ä¸‹é¢ä¸€è¡Œ print(ELASTICSEARCH_BASE_URL) è·å–çœŸå®çš„é…ç½®

# 1. åˆ›å»ºElasticsearchè¿æ¥
es = Elasticsearch(
    hosts=[ELASTICSEARCH_BASE_URL],  # æœåŠ¡åœ°å€ä¸ç«¯å£
    http_auth=(ELASTICSEARCH_NAME, ELASTICSEARCH_PASSWORD),  # ç”¨æˆ·åï¼Œå¯†ç 
)

# 2. å®šä¹‰ç´¢å¼•åç§°
index_name = "teacher_demo_index111"

# 3. å¦‚æœç´¢å¼•å·²å­˜åœ¨ï¼Œåˆ é™¤å®ƒï¼ˆä»…ä¾›æ¼”ç¤ºï¼Œå®é™…åº”ç”¨æ—¶ä¸éœ€è¦è¿™æ­¥ï¼‰
if es.indices.exists(index=index_name):
    es.indices.delete(index=index_name)

# 4. åˆ›å»ºç´¢å¼•
es.indices.create(index=index_name)

# 5. çŒåº“æŒ‡ä»¤
actions = [
    {
        "_index": index_name,
        "_source": {
            "keywords": to_keywords(para),
            "text": para
        }
    }
    for para in paragraphs
]

# 6. æ–‡æœ¬çŒåº“
helpers.bulk(es, actions)

# çŒåº“æ˜¯å¼‚æ­¥çš„
time.sleep(2)
```

å®ç°å…³é”®å­—æ£€ç´¢

```python
def search(query_string, top_n=3):
    # ES çš„æŸ¥è¯¢è¯­è¨€
    search_query = {
        "match": {
            "keywords": to_keywords(query_string)
        }
    }
    res = es.search(index=index_name, query=search_query, size=top_n)
    return [hit["_source"]["text"] for hit in res["hits"]["hits"]]
```

```python
results = search("how many parameters does llama 2 have?", 2)
for r in results:
    print(r+"\n")
```

**Output:**
```
1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.Â§

 In this work, we develop and release Llama 2, a collection of pretrained and ï¬ne-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our ï¬ne-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based onour human evaluations for helpfulness and safety, may be a suitable substitute for closed source models. We provide a detailed description of our approach to ï¬ne-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.
```

### 3.3ã€LLM æ¥å£å°è£…

```python
from openai import OpenAI
import os
# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv())  # è¯»å–æœ¬åœ° .env æ–‡ä»¶ï¼Œé‡Œé¢å®šä¹‰äº† OPENAI_API_KEY

client = OpenAI()
```

```python
def get_completion(prompt, model="gpt-3.5-turbo"):
    '''å°è£… openai æ¥å£'''
    messages = [{"role": "user", "content": prompt}]
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0,  # æ¨¡å‹è¾“å‡ºçš„éšæœºæ€§ï¼Œ0 è¡¨ç¤ºéšæœºæ€§æœ€å°
    )
    return response.choices[0].message.content
```

### 3.4ã€Prompt æ¨¡æ¿

```python
def build_prompt(prompt_template, **kwargs):
    '''å°† Prompt æ¨¡æ¿èµ‹å€¼'''
    inputs = {}
    for k, v in kwargs.items():
        if isinstance(v, list) and all(isinstance(elem, str) for elem in v):
            val = '\n\n'.join(v)
        else:
            val = v
        inputs[k] = val
    return prompt_template.format(**inputs)
```

```python
prompt_template = """
ä½ æ˜¯ä¸€ä¸ªé—®ç­”æœºå™¨äººã€‚
ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ä¸‹è¿°ç»™å®šçš„å·²çŸ¥ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

å·²çŸ¥ä¿¡æ¯:
{context}

ç”¨æˆ·é—®ï¼š
{query}

å¦‚æœå·²çŸ¥ä¿¡æ¯ä¸åŒ…å«ç”¨æˆ·é—®é¢˜çš„ç­”æ¡ˆï¼Œæˆ–è€…å·²çŸ¥ä¿¡æ¯ä¸è¶³ä»¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œè¯·ç›´æ¥å›å¤"æˆ‘æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜"ã€‚
è¯·ä¸è¦è¾“å‡ºå·²çŸ¥ä¿¡æ¯ä¸­ä¸åŒ…å«çš„ä¿¡æ¯æˆ–ç­”æ¡ˆã€‚
è¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
"""
```

### 3.5ã€RAG Pipeline åˆæ¢

```python
user_query = "how many parameters does llama 2 have?"

# 1. æ£€ç´¢
search_results = search(user_query, 2)

# 2. æ„å»º Prompt
prompt = build_prompt(prompt_template, context=search_results, query=user_query)
print("===Prompt===")
print(prompt)

# 3. è°ƒç”¨ LLM
response = get_completion(prompt)

print("===å›å¤===")
print(response)
```

**Output:**
```
===Prompt===

ä½ æ˜¯ä¸€ä¸ªé—®ç­”æœºå™¨äººã€‚
ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ä¸‹è¿°ç»™å®šçš„å·²çŸ¥ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

å·²çŸ¥ä¿¡æ¯:
 1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.Â§

 In this work, we develop and release Llama 2, a collection of pretrained and ï¬ne-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our ï¬ne-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based onour human evaluations for helpfulness and safety, may be a suitable substitute for closed source models. We provide a detailed description of our approach to ï¬ne-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.

ç”¨æˆ·é—®ï¼š
how many parameters does llama 2 have?

å¦‚æœå·²çŸ¥ä¿¡æ¯ä¸åŒ…å«ç”¨æˆ·é—®é¢˜çš„ç­”æ¡ˆï¼Œæˆ–è€…å·²çŸ¥ä¿¡æ¯ä¸è¶³ä»¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œè¯·ç›´æ¥å›å¤"æˆ‘æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜"ã€‚
è¯·ä¸è¦è¾“å‡ºå·²çŸ¥ä¿¡æ¯ä¸­ä¸åŒ…å«çš„ä¿¡æ¯æˆ–ç­”æ¡ˆã€‚
è¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

===å›å¤===
Llama 2æœ‰7B, 13Bå’Œ70Bå‚æ•°ã€‚
```

> â„¹ï¸ **Info:** æ‰©å±•é˜…è¯»ï¼š

Elasticsearchï¼ˆç®€ç§°ESï¼‰æ˜¯ä¸€ä¸ªå¹¿æ³›åº”ç”¨çš„å¼€æºæœç´¢å¼•æ“: https://www.elastic.co/
å…³äºESçš„å®‰è£…ã€éƒ¨ç½²ç­‰çŸ¥è¯†ï¼Œç½‘ä¸Šå¯ä»¥æ‰¾åˆ°å¤§é‡èµ„æ–™ï¼Œä¾‹å¦‚: https://juejin.cn/post/7104875268166123528
å…³äºç»å…¸ä¿¡æ¯æ£€ç´¢æŠ€æœ¯çš„æ›´å¤šç»†èŠ‚ï¼Œå¯ä»¥å‚è€ƒ: https://nlp.stanford.edu/IR-book/information-retrieval-book.html

### 3.6ã€å…³é”®å­—æ£€ç´¢çš„å±€é™æ€§

åŒä¸€ä¸ªè¯­ä¹‰ï¼Œç”¨è¯ä¸åŒï¼Œå¯èƒ½å¯¼è‡´æ£€ç´¢ä¸åˆ°æœ‰æ•ˆçš„ç»“æœ

```python
# user_query="Does llama 2 have a chat version?"
user_query = "Does llama 2 have a conversational variant?"

search_results = search(user_query, 2)

for res in search_results:
    print(res+"\n")
```

**Output:**
```
1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.Â§

 variants of this model with 7B, 13B, and 70B parameters as well.
```

## å››ã€å‘é‡æ£€ç´¢

### 4.1ã€ä»€ä¹ˆæ˜¯å‘é‡

å‘é‡æ˜¯ä¸€ç§æœ‰å¤§å°å’Œæ–¹å‘çš„æ•°å­¦å¯¹è±¡ã€‚å®ƒå¯ä»¥è¡¨ç¤ºä¸ºä»ä¸€ä¸ªç‚¹åˆ°å¦ä¸€ä¸ªç‚¹çš„æœ‰å‘çº¿æ®µã€‚ä¾‹å¦‚ï¼ŒäºŒç»´ç©ºé—´ä¸­çš„å‘é‡å¯ä»¥è¡¨ç¤ºä¸º $(x,y)$ï¼Œè¡¨ç¤ºä»åŸç‚¹ $(0,0)$ åˆ°ç‚¹ $(x,y)$ çš„æœ‰å‘çº¿æ®µã€‚
<br />
![image](./assets/05-rag-embeddings/vector.png)
<br />
ä»¥æ­¤ç±»æ¨ï¼Œæˆ‘å¯ä»¥ç”¨ä¸€ç»„åæ ‡ $(x_0, x_1, \ldots, x_{N-1})$ è¡¨ç¤ºä¸€ä¸ª $N$ ç»´ç©ºé—´ä¸­çš„å‘é‡ï¼Œ$N$ å«å‘é‡çš„ç»´åº¦ã€‚

### 4.1.1ã€æ–‡æœ¬å‘é‡ï¼ˆText Embeddingsï¼‰

1. å°†æ–‡æœ¬è½¬æˆä¸€ç»„ $N$ ç»´æµ®ç‚¹æ•°ï¼Œå³**æ–‡æœ¬å‘é‡**åˆå« Embeddings
2. å‘é‡ä¹‹é—´å¯ä»¥è®¡ç®—è·ç¦»ï¼Œè·ç¦»è¿œè¿‘å¯¹åº”**è¯­ä¹‰ç›¸ä¼¼åº¦**å¤§å°

<br />
![image](./assets/05-rag-embeddings/embeddings.png)
<br />

### 4.1.2ã€æ–‡æœ¬å‘é‡æ˜¯æ€ä¹ˆå¾—åˆ°çš„ï¼ˆé€‰ï¼‰

1. æ„å»ºç›¸å…³ï¼ˆæ­£ç«‹ï¼‰ä¸ä¸ç›¸å…³ï¼ˆè´Ÿä¾‹ï¼‰çš„å¥å­å¯¹å„¿æ ·æœ¬
2. è®­ç»ƒåŒå¡”å¼æ¨¡å‹ï¼Œè®©æ­£ä¾‹é—´çš„è·ç¦»å°ï¼Œè´Ÿä¾‹é—´çš„è·ç¦»å¤§

ä¾‹å¦‚ï¼š

![image](./assets/05-rag-embeddings/sbert.png)

> â„¹ï¸ **Info:** æ‰©å±•é˜…è¯»ï¼šhttps://www.sbert.net

### 4.2ã€å‘é‡é—´çš„ç›¸ä¼¼åº¦è®¡ç®—

![image](./assets/05-rag-embeddings/sim.png)

```python
import numpy as np
from numpy import dot
from numpy.linalg import norm
```

```python
def cos_sim(a, b):
    '''ä½™å¼¦è·ç¦» -- è¶Šå¤§è¶Šç›¸ä¼¼'''
    return dot(a, b)/(norm(a)*norm(b))


def l2(a, b):
    '''æ¬§æ°è·ç¦» -- è¶Šå°è¶Šç›¸ä¼¼'''
    x = np.asarray(a)-np.asarray(b)
    return norm(x)
```

```python
def get_embeddings(texts, model="text-embedding-ada-002", dimensions=None):
    '''å°è£… OpenAI çš„ Embedding æ¨¡å‹æ¥å£'''
    if model == "text-embedding-ada-002":
        dimensions = None
    if dimensions:
        data = client.embeddings.create(
            input=texts, model=model, dimensions=dimensions).data
    else:
        data = client.embeddings.create(input=texts, model=model).data
    return [x.embedding for x in data]
```

```python
test_query = ["æµ‹è¯•æ–‡æœ¬"]
vec = get_embeddings(test_query)[0]
print(f"Total dimension: {len(vec)}")
print(f"First 10 elements: {vec[:10]}")
```

**Output:**
```
Total dimension: 1536
First 10 elements: [-0.007280634716153145, -0.006147929932922125, -0.010664181783795357, 0.001484171487390995, -0.010678750462830067, 0.029253656044602394, -0.01976952701807022, 0.005444996990263462, -0.01687038503587246, -0.01207733154296875]
```

```python
# query = "å›½é™…äº‰ç«¯"

# ä¸”èƒ½æ”¯æŒè·¨è¯­è¨€
query = "global conflicts"

documents = [
    "è”åˆå›½å°±è‹ä¸¹è¾¾å°”å¯Œå°”åœ°åŒºå¤§è§„æ¨¡æš´åŠ›äº‹ä»¶å‘å‡ºè­¦å‘Š",
    "åœŸè€³å…¶ã€èŠ¬å…°ã€ç‘å…¸ä¸åŒ—çº¦ä»£è¡¨å°†ç»§ç»­å°±ç‘å…¸â€œå…¥çº¦â€é—®é¢˜è¿›è¡Œè°ˆåˆ¤",
    "æ—¥æœ¬å²é˜œå¸‚é™†ä¸Šè‡ªå«é˜Ÿå°„å‡»åœºå†…å‘ç”Ÿæªå‡»äº‹ä»¶ 3äººå—ä¼¤",
    "å›½å®¶æ¸¸æ³³ä¸­å¿ƒï¼ˆæ°´ç«‹æ–¹ï¼‰ï¼šæ¢å¤æ¸¸æ³³ã€å¬‰æ°´ä¹å›­ç­‰æ°´ä¸Šé¡¹ç›®è¿è¥",
    "æˆ‘å›½é¦–æ¬¡åœ¨ç©ºé—´ç«™å¼€å±•èˆ±å¤–è¾å°„ç”Ÿç‰©å­¦æš´éœ²å®éªŒ",
]

query_vec = get_embeddings([query])[0]
doc_vecs = get_embeddings(documents)

print("Queryä¸è‡ªå·±çš„ä½™å¼¦è·ç¦»: {:.2f}".format(cos_sim(query_vec, query_vec)))
print("Queryä¸Documentsçš„ä½™å¼¦è·ç¦»:")
for vec in doc_vecs:
    print(cos_sim(query_vec, vec))

print()

print("Queryä¸è‡ªå·±çš„æ¬§æ°è·ç¦»: {:.2f}".format(l2(query_vec, query_vec)))
print("Queryä¸Documentsçš„æ¬§æ°è·ç¦»:")
for vec in doc_vecs:
    print(l2(query_vec, vec))
```

**Output:**
```
Queryä¸è‡ªå·±çš„ä½™å¼¦è·ç¦»: 1.00
Queryä¸Documentsçš„ä½™å¼¦è·ç¦»:
0.7622749944010915
0.7563038106493584
0.7426665802579038
0.7079273699608006
0.7254355321045072

Queryä¸è‡ªå·±çš„æ¬§æ°è·ç¦»: 0.00
Queryä¸Documentsçš„æ¬§æ°è·ç¦»:
0.6895288502682277
0.6981349637998769
0.7174028746492277
0.7642939833636829
0.7410323668625171
```

### 4.3ã€å‘é‡æ•°æ®åº“

å‘é‡æ•°æ®åº“ï¼Œæ˜¯ä¸“é—¨ä¸ºå‘é‡æ£€ç´¢è®¾è®¡çš„ä¸­é—´ä»¶

```python
# !pip install chromadb
```

```python
# ç”±äºå­¦ç”Ÿç«¯ä¸æ•™å¸ˆç«¯ç¯å¢ƒçš„åŒºåˆ«
# å¯¹pysqliteçš„å…¼å®¹å¤„ç†
import os
if os.environ.get('CUR_ENV_IS_STUDENT',False):
    import sys
    __import__('pysqlite3')
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
```

```python
# ä¸ºäº†æ¼”ç¤ºæ–¹ä¾¿ï¼Œæˆ‘ä»¬åªå–ä¸¤é¡µï¼ˆç¬¬ä¸€ç« ï¼‰
paragraphs = extract_text_from_pdf(
    "llama2.pdf",
    page_numbers=[2, 3],
    min_line_length=10
)
```

```python
import chromadb
from chromadb.config import Settings


class MyVectorDBConnector:
    def __init__(self, collection_name, embedding_fn):
        chroma_client = chromadb.Client(Settings(allow_reset=True))

        # ä¸ºäº†æ¼”ç¤ºï¼Œå®é™…ä¸éœ€è¦æ¯æ¬¡ reset()
        chroma_client.reset()

        # åˆ›å»ºä¸€ä¸ª collection
        self.collection = chroma_client.get_or_create_collection(
            name=collection_name)
        self.embedding_fn = embedding_fn

    def add_documents(self, documents):
        '''å‘ collection ä¸­æ·»åŠ æ–‡æ¡£ä¸å‘é‡'''
        self.collection.add(
            embeddings=self.embedding_fn(documents),  # æ¯ä¸ªæ–‡æ¡£çš„å‘é‡
            documents=documents,  # æ–‡æ¡£çš„åŸæ–‡
            ids=[f"id{i}" for i in range(len(documents))]  # æ¯ä¸ªæ–‡æ¡£çš„ id
        )

    def search(self, query, top_n):
        '''æ£€ç´¢å‘é‡æ•°æ®åº“'''
        results = self.collection.query(
            query_embeddings=self.embedding_fn([query]),
            n_results=top_n
        )
        return results
```

```python
# åˆ›å»ºä¸€ä¸ªå‘é‡æ•°æ®åº“å¯¹è±¡
vector_db = MyVectorDBConnector("demo", get_embeddings)
# å‘å‘é‡æ•°æ®åº“ä¸­æ·»åŠ æ–‡æ¡£
vector_db.add_documents(paragraphs)
```

```python
user_query = "Llama 2æœ‰å¤šå°‘å‚æ•°"
# user_query = "Does Llama 2 have a conversational variant"
results = vector_db.search(user_query, 2)
```

```python
for para in results['documents'][0]:
    print(para+"\n")
```

**Output:**
```
1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.Â§

 In this work, we develop and release Llama 2, a family of pretrained and ï¬ne-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models. They also appear to be on par with some of the closed-source models, at least on the human evaluations we performed (see Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-speciï¬c data annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally, this paper contributes a thorough description of our ï¬ne-tuning methodology and approach to improving LLM safety. We hope that this openness will enable the community to reproduce ï¬ne-tuned LLMs and continue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge.
```

> âœ… **Tip:** æ¾„æ¸…å‡ ä¸ªå…³é”®æ¦‚å¿µï¼š
    å‘é‡æ•°æ®åº“çš„æ„ä¹‰æ˜¯å¿«é€Ÿçš„æ£€ç´¢ï¼›
    å‘é‡æ•°æ®åº“æœ¬èº«ä¸ç”Ÿæˆå‘é‡ï¼Œå‘é‡æ˜¯ç”± Embedding æ¨¡å‹äº§ç”Ÿçš„ï¼›
    å‘é‡æ•°æ®åº“ä¸ä¼ ç»Ÿçš„å…³ç³»å‹æ•°æ®åº“æ˜¯äº’è¡¥çš„ï¼Œä¸æ˜¯æ›¿ä»£å…³ç³»ï¼Œåœ¨å®é™…åº”ç”¨ä¸­æ ¹æ®å®é™…éœ€æ±‚ç»å¸¸åŒæ—¶ä½¿ç”¨ã€‚

### 4.3.1ã€å‘é‡æ•°æ®åº“æœåŠ¡

Server ç«¯

```sh
chroma run --path /db_path
```

Client ç«¯

```python
import chromadb
chroma_client = chromadb.HttpClient(host='localhost', port=8000)
```

### 4.3.2ã€ä¸»æµå‘é‡æ•°æ®åº“åŠŸèƒ½å¯¹æ¯”

![image](./assets/05-rag-embeddings/vectordb.png)

- FAISS: Meta å¼€æºçš„å‘é‡æ£€ç´¢å¼•æ“ https://github.com/facebookresearch/faiss
- Pinecone: å•†ç”¨å‘é‡æ•°æ®åº“ï¼Œåªæœ‰äº‘æœåŠ¡ https://www.pinecone.io/
- Milvus: å¼€æºå‘é‡æ•°æ®åº“ï¼ŒåŒæ—¶æœ‰äº‘æœåŠ¡ https://milvus.io/
- Weaviate: å¼€æºå‘é‡æ•°æ®åº“ï¼ŒåŒæ—¶æœ‰äº‘æœåŠ¡ https://weaviate.io/
- Qdrant: å¼€æºå‘é‡æ•°æ®åº“ï¼ŒåŒæ—¶æœ‰äº‘æœåŠ¡ https://qdrant.tech/
- PGVector: Postgres çš„å¼€æºå‘é‡æ£€ç´¢å¼•æ“ https://github.com/pgvector/pgvector
- RediSearch: Redis çš„å¼€æºå‘é‡æ£€ç´¢å¼•æ“ https://github.com/RediSearch/RediSearch
- ElasticSearch ä¹Ÿæ”¯æŒå‘é‡æ£€ç´¢ https://www.elastic.co/enterprise-search/vector-search

### 4.4ã€åŸºäºå‘é‡æ£€ç´¢çš„ RAG

```python
class RAG_Bot:
    def __init__(self, vector_db, llm_api, n_results=2):
        self.vector_db = vector_db
        self.llm_api = llm_api
        self.n_results = n_results

    def chat(self, user_query):
        # 1. æ£€ç´¢
        search_results = self.vector_db.search(user_query, self.n_results)

        # 2. æ„å»º Prompt
        prompt = build_prompt(
            prompt_template, context=search_results['documents'][0], query=user_query)

        # 3. è°ƒç”¨ LLM
        response = self.llm_api(prompt)
        return response
```

```python
# åˆ›å»ºä¸€ä¸ªRAGæœºå™¨äºº
bot = RAG_Bot(
    vector_db,
    llm_api=get_completion
)

user_query = "llama 2æœ‰å¤šå°‘å‚æ•°?"

response = bot.chat(user_query)

print(response)
```

**Output:**
```
llama 2æœ‰7B, 13Bå’Œ70Bå‚æ•°ã€‚
```

### 4.5ã€å¦‚æœæƒ³è¦æ¢ä¸ªå›½äº§æ¨¡å‹

```python
import json
import requests
import os

# é€šè¿‡é‰´æƒæ¥å£è·å– access token


def get_access_token():
    """
    ä½¿ç”¨ AKï¼ŒSK ç”Ÿæˆé‰´æƒç­¾åï¼ˆAccess Tokenï¼‰
    :return: access_tokenï¼Œæˆ–æ˜¯None(å¦‚æœé”™è¯¯)
    """
    url = "https://aip.baidubce.com/oauth/2.0/token"
    params = {
        "grant_type": "client_credentials",
        "client_id": os.getenv('ERNIE_CLIENT_ID'),
        "client_secret": os.getenv('ERNIE_CLIENT_SECRET')
    }

    return str(requests.post(url, params=params).json().get("access_token"))

# è°ƒç”¨æ–‡å¿ƒåƒå¸† è°ƒç”¨ BGE Embedding æ¥å£


def get_embeddings_bge(prompts):
    url = "https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/embeddings/bge_large_en?access_token=" + get_access_token()
    payload = json.dumps({
        "input": prompts
    })
    headers = {'Content-Type': 'application/json'}

    response = requests.request(
        "POST", url, headers=headers, data=payload).json()
    data = response["data"]
    return [x["embedding"] for x in data]


# è°ƒç”¨æ–‡å¿ƒ4.0å¯¹è¯æ¥å£
def get_completion_ernie(prompt):

    url = "https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/completions_pro?access_token=" + get_access_token()
    payload = json.dumps({
        "messages": [
            {
                "role": "user",
                "content": prompt
            }
        ]
    })

    headers = {'Content-Type': 'application/json'}

    response = requests.request(
        "POST", url, headers=headers, data=payload).json()

    return response["result"]
```

```python
# åˆ›å»ºä¸€ä¸ªå‘é‡æ•°æ®åº“å¯¹è±¡
new_vector_db = MyVectorDBConnector(
    "demo_ernie",
    embedding_fn=get_embeddings_bge
)
# å‘å‘é‡æ•°æ®åº“ä¸­æ·»åŠ æ–‡æ¡£
new_vector_db.add_documents(paragraphs)

# åˆ›å»ºä¸€ä¸ªRAGæœºå™¨äºº
new_bot = RAG_Bot(
    new_vector_db,
    llm_api=get_completion_ernie
)
```

```python
user_query = "how many parameters does llama 2 have?"

response = new_bot.chat(user_query)

print(response)
```

**Output:**
```
Llama 2å…·æœ‰7Bã€13Bå’Œ70Bå‚æ•°ã€‚
```

### 4.6ã€OpenAI æ–°å‘å¸ƒçš„ä¸¤ä¸ª Embedding æ¨¡å‹

2024 å¹´ 1 æœˆ 25 æ—¥ï¼ŒOpenAI æ–°å‘å¸ƒäº†ä¸¤ä¸ª Embedding æ¨¡å‹

- text-embedding-3-large
- text-embedding-3-small

å…¶æœ€å¤§ç‰¹ç‚¹æ˜¯ï¼Œæ”¯æŒè‡ªå®šä¹‰çš„ç¼©çŸ­å‘é‡ç»´åº¦ï¼Œä»è€Œåœ¨å‡ ä¹ä¸å½±å“æœ€ç»ˆæ•ˆæœçš„æƒ…å†µä¸‹é™ä½å‘é‡æ£€ç´¢ä¸ç›¸ä¼¼åº¦è®¡ç®—çš„å¤æ‚åº¦ã€‚

é€šä¿—çš„è¯´ï¼š**è¶Šå¤§è¶Šå‡†ã€è¶Šå°è¶Šå¿«ã€‚** å®˜æ–¹å…¬å¸ƒçš„è¯„æµ‹ç»“æœ:

![image](./assets/05-rag-embeddings/mteb.png)

æ³¨ï¼š[MTEB](https://huggingface.co/blog/mteb) æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡å¤šä»»åŠ¡çš„ Embedding æ¨¡å‹å…¬å¼€è¯„æµ‹é›†

```python
model = "text-embedding-3-large"
dimensions = 128

# query = "å›½é™…äº‰ç«¯"

# ä¸”èƒ½æ”¯æŒè·¨è¯­è¨€
query = "global conflicts"

documents = [
    "è”åˆå›½å°±è‹ä¸¹è¾¾å°”å¯Œå°”åœ°åŒºå¤§è§„æ¨¡æš´åŠ›äº‹ä»¶å‘å‡ºè­¦å‘Š",
    "åœŸè€³å…¶ã€èŠ¬å…°ã€ç‘å…¸ä¸åŒ—çº¦ä»£è¡¨å°†ç»§ç»­å°±ç‘å…¸â€œå…¥çº¦â€é—®é¢˜è¿›è¡Œè°ˆåˆ¤",
    "æ—¥æœ¬å²é˜œå¸‚é™†ä¸Šè‡ªå«é˜Ÿå°„å‡»åœºå†…å‘ç”Ÿæªå‡»äº‹ä»¶ 3äººå—ä¼¤",
    "å›½å®¶æ¸¸æ³³ä¸­å¿ƒï¼ˆæ°´ç«‹æ–¹ï¼‰ï¼šæ¢å¤æ¸¸æ³³ã€å¬‰æ°´ä¹å›­ç­‰æ°´ä¸Šé¡¹ç›®è¿è¥",
    "æˆ‘å›½é¦–æ¬¡åœ¨ç©ºé—´ç«™å¼€å±•èˆ±å¤–è¾å°„ç”Ÿç‰©å­¦æš´éœ²å®éªŒ",
]

query_vec = get_embeddings([query], model=model, dimensions=dimensions)[0]
doc_vecs = get_embeddings(documents, model=model, dimensions=dimensions)

print("å‘é‡ç»´åº¦: {}".format(len(query_vec)))

print()

print("Queryä¸Documentsçš„ä½™å¼¦è·ç¦»:")
for vec in doc_vecs:
    print(cos_sim(query_vec, vec))

print()

print("Queryä¸Documentsçš„æ¬§æ°è·ç¦»:")
for vec in doc_vecs:
    print(l2(query_vec, vec))
```

**Output:**
```
å‘é‡ç»´åº¦: 128

Queryä¸Documentsçš„ä½™å¼¦è·ç¦»:
0.3341465461188056
0.35421753696892283
0.31378006817856735
0.21377702309282426
0.12870207920908178

Queryä¸Documentsçš„æ¬§æ°è·ç¦»:
1.1539960610822753
1.136470458688705
1.1715118547835923
1.2539720770077676
1.3200742239193948
```

> â„¹ï¸ **Info:** æ‰©å±•é˜…è¯»ï¼šè¿™ç§å¯å˜é•¿åº¦çš„ Embedding æŠ€æœ¯èƒŒåçš„åŸç†å«åš Matryoshka Representation Learning

## äº”ã€å®æˆ˜ RAG ç³»ç»Ÿçš„è¿›é˜¶çŸ¥è¯†

### 5.1ã€æ–‡æœ¬åˆ†å‰²çš„ç²’åº¦

**ç¼ºé™·**

1. ç²’åº¦å¤ªå¤§å¯èƒ½å¯¼è‡´æ£€ç´¢ä¸ç²¾å‡†ï¼Œç²’åº¦å¤ªå°å¯èƒ½å¯¼è‡´ä¿¡æ¯ä¸å…¨é¢
2. é—®é¢˜çš„ç­”æ¡ˆå¯èƒ½è·¨è¶Šä¸¤ä¸ªç‰‡æ®µ

```python
# åˆ›å»ºä¸€ä¸ªå‘é‡æ•°æ®åº“å¯¹è±¡
vector_db = MyVectorDBConnector("demo_text_split", get_embeddings)
# å‘å‘é‡æ•°æ®åº“ä¸­æ·»åŠ æ–‡æ¡£
vector_db.add_documents(paragraphs)

# åˆ›å»ºä¸€ä¸ªRAGæœºå™¨äºº
bot = RAG_Bot(
    vector_db,
    llm_api=get_completion
)
```

```python
# user_query = "llama 2æœ‰å•†ç”¨è®¸å¯åè®®å—"
user_query="llama 2 chatæœ‰å¤šå°‘å‚æ•°"
search_results = vector_db.search(user_query, 2)

for doc in search_results['documents'][0]:
    print(doc+"\n")

print("====å›å¤====")
bot.chat(user_query)
```

**Output:**
```
In this work, we develop and release Llama 2, a family of pretrained and ï¬ne-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models. They also appear to be on par with some of the closed-source models, at least on the human evaluations we performed (see Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-speciï¬c data annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally, this paper contributes a thorough description of our ï¬ne-tuning methodology and approach to improving LLM safety. We hope that this openness will enable the community to reproduce ï¬ne-tuned LLMs and continue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge.

 2. Llama 2-Chat, a ï¬ne-tuned version of Llama 2 that is optimized for dialogue use cases. We release

====å›å¤====
'llama 2 chatæœ‰70Bä¸ªå‚æ•°ã€‚'
```

```python
for p in paragraphs:
    print(p+"\n")
```

**Output:**
```
Figure 1: Helpfulness human evaluation results for Llama 2-Chat compared to other open-source and closed-source models. Human raters compared model generations on ~4k prompts consisting of both single and multi-turn prompts. The 95% conï¬dence intervals for this evaluation are between 1% and 2%. More details in Section 3.4.2. While reviewing these results, it is important to note that human evaluations can be noisy due to limitations of the prompt set, subjectivity of the review guidelines, subjectivity of individual raters, and the inherent diï¬ƒculty of comparing generations.

 Figure 2: Win-rate % for helpfulness andsafety between commercial-licensed baselines and Llama 2-Chat, according to GPT 4. To complement the human evaluation, we used a more capable model, not subject to our own guidance. Green area indicates our model is better according to GPT-4. To remove ties, we used win/(win + loss). The orders in which the model responses are presented to GPT-4 are randomly swapped to alleviate bias.

 1 Introduction

 Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in complex reasoning tasks requiring expert knowledge across a wide range of ï¬elds, including in specialized domains such as programming and creative writing. They enable interaction with humans through intuitive chat interfaces, which has led to rapid and widespread adoption among the general public.

 The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training methodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data, followed by alignment with human preferences via techniques such as Reinforcement Learning with Human Feedback (RLHF). Although the training methodology is simple, high computational requirements have limited the development of LLMs to a few players. There have been public releases of pretrained LLMs (such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that match the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla (Hoï¬€mann et al., 2022), but none of these models are suitable substitutes for closed â€œproductâ€ LLMs, such as ChatGPT, BARD, and Claude. These closed product LLMs are heavily ï¬ne-tuned to align with human preferences, which greatly enhances their usability and safety. This step can require signiï¬cant costs in compute and human annotation, and is often not transparent or easily reproducible, limiting progress within the community to advance AI alignment research.

 In this work, we develop and release Llama 2, a family of pretrained and ï¬ne-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models. They also appear to be on par with some of the closed-source models, at least on the human evaluations we performed (see Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-speciï¬c data annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally, this paper contributes a thorough description of our ï¬ne-tuning methodology and approach to improving LLM safety. We hope that this openness will enable the community to reproduce ï¬ne-tuned LLMs and continue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge.

Figure 3: Safety human evaluation results for Llama 2-Chat compared to other open-source and closed source models. Human raters judged model generations for safety violations across ~2,000 adversarial prompts consisting of both single and multi-turn prompts. More details can be found in Section 4.4. It is important to caveat these safety results with the inherent bias of LLM evaluations due to limitations of the prompt set, subjectivity of the review guidelines, and subjectivity of individual raters. Additionally, these safety evaluations are performed using content standards that are likely to be biased towards the Llama 2-Chat models.

 We are releasing the following models to the general public for research and commercial useâ€¡:

 1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.Â§

 2. Llama 2-Chat, a ï¬ne-tuned version of Llama 2 that is optimized for dialogue use cases. We release

 variants of this model with 7B, 13B, and 70B parameters as well.

 We believe that the open release of LLMs, when done safely, will be a net beneï¬t to society. Like all LLMs, Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021; Solaiman et al., 2023). Testing conducted to date has been in English and has not â€” and could not â€” cover all scenarios. Therefore, before deploying any applications of Llama 2-Chat, developers should perform safety testing and tuning tailored to their speciï¬c applications of the model. We provide a responsible use guideÂ¶ and code examplesâ€– to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of our responsible release strategy can be found in Section 5.3.

 The remainder of this paper describes our pretraining methodology (Section 2), ï¬ne-tuning methodology (Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related work (Section 6), and conclusions (Section 7).

 â€¡https://ai.meta.com/resources/models-and-libraries/llama/ Â§We are delaying the release of the 34B model due to a lack of time to suï¬ƒciently red team. Â¶https://ai.meta.com/llama â€–https://github.com/facebookresearch/llama
```

**æ”¹è¿›**: æŒ‰ä¸€å®šç²’åº¦ï¼Œéƒ¨åˆ†é‡å å¼çš„åˆ‡å‰²æ–‡æœ¬ï¼Œä½¿ä¸Šä¸‹æ–‡æ›´å®Œæ•´

```python
from nltk.tokenize import sent_tokenize
import json


def split_text(paragraphs, chunk_size=300, overlap_size=100):
    '''æŒ‰æŒ‡å®š chunk_size å’Œ overlap_size äº¤å å‰²æ–‡æœ¬'''
    sentences = [s.strip() for p in paragraphs for s in sent_tokenize(p)]
    chunks = []
    i = 0
    while i < len(sentences):
        chunk = sentences[i]
        overlap = ''
        prev_len = 0
        prev = i - 1
        # å‘å‰è®¡ç®—é‡å éƒ¨åˆ†
        while prev >= 0 and len(sentences[prev])+len(overlap) <= overlap_size:
            overlap = sentences[prev] + ' ' + overlap
            prev -= 1
        chunk = overlap+chunk
        next = i + 1
        # å‘åè®¡ç®—å½“å‰chunk
        while next < len(sentences) and len(sentences[next])+len(chunk) <= chunk_size:
            chunk = chunk + ' ' + sentences[next]
            next += 1
        chunks.append(chunk)
        i = next
    return chunks
```

> â„¹ï¸ **Info:** æ­¤å¤„ sent_tokenize ä¸ºé’ˆå¯¹è‹±æ–‡çš„å®ç°ï¼Œé’ˆå¯¹ä¸­æ–‡çš„å®ç°è¯·å‚è€ƒ chinese_utils.py

```python
chunks = split_text(paragraphs, 300, 100)
```

```python
# åˆ›å»ºä¸€ä¸ªå‘é‡æ•°æ®åº“å¯¹è±¡
vector_db = MyVectorDBConnector("demo_text_split", get_embeddings)
# å‘å‘é‡æ•°æ®åº“ä¸­æ·»åŠ æ–‡æ¡£
vector_db.add_documents(chunks)
# åˆ›å»ºä¸€ä¸ªRAGæœºå™¨äºº
bot = RAG_Bot(
    vector_db,
    llm_api=get_completion
)
```

```python
# user_query = "llama 2æœ‰å•†ç”¨è®¸å¯åè®®å—"
user_query="llama 2 chatæœ‰å¤šå°‘å‚æ•°"

search_results = vector_db.search(user_query, 2)
for doc in search_results['documents'][0]:
    print(doc+"\n")

response = bot.chat(user_query)
print("====å›å¤====")
print(response)
```

**Output:**
```
2. Llama 2-Chat, a ï¬ne-tuned version of Llama 2 that is optimized for dialogue use cases. We release variants of this model with 7B, 13B, and 70B parameters as well. We believe that the open release of LLMs, when done safely, will be a net beneï¬t to society.

In this work, we develop and release Llama 2, a family of pretrained and ï¬ne-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models.

====å›å¤====
llama 2 chatæœ‰7B, 13Bå’Œ70Bå‚æ•°ã€‚
```

### 5.2ã€æ£€ç´¢åæ’åºï¼ˆé€‰ï¼‰

**é—®é¢˜**: æœ‰æ—¶ï¼Œæœ€åˆé€‚çš„ç­”æ¡ˆä¸ä¸€å®šæ’åœ¨æ£€ç´¢çš„æœ€å‰é¢

```python
user_query = "how safe is llama 2"
search_results = vector_db.search(user_query, 5)

for doc in search_results['documents'][0]:
    print(doc+"\n")

response = bot.chat(user_query)
print("====å›å¤====")
print(response)
```

**Output:**
```
We believe that the open release of LLMs, when done safely, will be a net beneï¬t to society. Like all LLMs, Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021; Solaiman et al., 2023).

We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge. Figure 3: Safety human evaluation results for Llama 2-Chat compared to other open-source and closed source models.

In this work, we develop and release Llama 2, a family of pretrained and ï¬ne-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models.

Additionally, these safety evaluations are performed using content standards that are likely to be biased towards the Llama 2-Chat models. We are releasing the following models to the general public for research and commercial useâ€¡: 1.

We provide a responsible use guideÂ¶ and code examplesâ€– to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of our responsible release strategy can be found in Section 5.3.

====å›å¤====
æ ¹æ®å·²çŸ¥ä¿¡æ¯ä¸­æåˆ°çš„å®‰å…¨äººç±»è¯„ä¼°ç»“æœï¼ŒLlama 2-Chatåœ¨å®‰å…¨æ€§æ–¹é¢ç›¸å¯¹äºå…¶ä»–å¼€æºå’Œé—­æºæ¨¡å‹è¡¨ç°è‰¯å¥½ã€‚
```

**æ–¹æ¡ˆ**:

1. æ£€ç´¢æ—¶è¿‡æ‹›å›ä¸€éƒ¨åˆ†æ–‡æœ¬
2. é€šè¿‡ä¸€ä¸ªæ’åºæ¨¡å‹å¯¹ query å’Œ document é‡æ–°æ‰“åˆ†æ’åº

![image](./assets/05-rag-embeddings/sbert-rerank.png)

> âŒ **Warning:** ä»¥ä¸‹ä»£ç ä¸è¦åœ¨æœåŠ¡å™¨ä¸Šè¿è¡Œï¼Œä¼šæ­»æœºï¼å¯ä¸‹è½½å·¦ä¾§ rank.py åœ¨è‡ªå·±æœ¬åœ°è¿è¡Œã€‚

> âš ï¸ **Note:** å¤‡æ³¨ï¼š
ç”±äº huggingface è¢«å¢™ï¼Œæˆ‘ä»¬å·²ç»ä¸ºæ‚¨å‡†å¤‡å¥½äº†æœ¬ç« ç›¸å…³æ¨¡å‹ã€‚è¯·ç‚¹å‡»ä»¥ä¸‹ç½‘ç›˜é“¾æ¥è¿›è¡Œä¸‹è½½ï¼š
    
é“¾æ¥: https://pan.baidu.com/s/1X0kfNKasvWqCLUEEyAvO-Q?pwd=3v6y æå–ç : 3v6y


```python
# !pip install sentence_transformers
```

```python
from sentence_transformers import CrossEncoder

# model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', max_length=512) # è‹±æ–‡ï¼Œæ¨¡å‹è¾ƒå°
model = CrossEncoder('BAAI/bge-reranker-large', max_length=512) # å¤šè¯­è¨€ï¼Œå›½äº§ï¼Œæ¨¡å‹è¾ƒå¤§
```

```python
user_query = "how safe is llama 2"
# user_query = "llama 2å®‰å…¨æ€§å¦‚ä½•"
scores = model.predict([(user_query, doc)
                       for doc in search_results['documents'][0]])
# æŒ‰å¾—åˆ†æ’åº
sorted_list = sorted(
    zip(scores, search_results['documents'][0]), key=lambda x: x[0], reverse=True)
for score, doc in sorted_list:
    print(f"{score}\t{doc}\n")
```

**Output:**
```
0.918857753276825	In this work, we develop and release Llama 2, a family of pretrained and ï¬ne-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models.

0.7791304588317871	We believe that the open release of LLMs, when done safely, will be a net beneï¬t to society. Like all LLMs, Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021; Solaiman et al., 2023).

0.47571462392807007	We provide a responsible use guideÂ¶ and code examplesâ€– to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of our responsible release strategy can be found in Section 5.3.

0.47421783208847046	We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge. Figure 3: Safety human evaluation results for Llama 2-Chat compared to other open-source and closed source models.

0.16011707484722137	Additionally, these safety evaluations are performed using content standards that are likely to be biased towards the Llama 2-Chat models. We are releasing the following models to the general public for research and commercial useâ€¡: 1.
```

#### ä¸€äº› Rerank çš„ API æœåŠ¡ 

- [Cohere Rerank](https://cohere.com/rerank)ï¼šæ”¯æŒå¤šè¯­è¨€
- [Jina Rerank](https://jina.ai/reranker/)ï¼šç›®å‰åªæ”¯æŒè‹±æ–‡

### 5.3ã€æ··åˆæ£€ç´¢ï¼ˆHybrid Searchï¼‰ï¼ˆé€‰ï¼‰

åœ¨**å®é™…ç”Ÿäº§**ä¸­ï¼Œä¼ ç»Ÿçš„å…³é”®å­—æ£€ç´¢ï¼ˆç¨€ç–è¡¨ç¤ºï¼‰ä¸å‘é‡æ£€ç´¢ï¼ˆç¨ å¯†è¡¨ç¤ºï¼‰å„æœ‰ä¼˜åŠ£ã€‚

ä¸¾ä¸ªå…·ä½“ä¾‹å­ï¼Œæ¯”å¦‚æ–‡æ¡£ä¸­åŒ…å«å¾ˆé•¿çš„ä¸“æœ‰åè¯ï¼Œå…³é”®å­—æ£€ç´¢å¾€å¾€æ›´ç²¾å‡†è€Œå‘é‡æ£€ç´¢å®¹æ˜“å¼•å…¥æ¦‚å¿µæ··æ·†ã€‚

```python
# èƒŒæ™¯è¯´æ˜ï¼šåœ¨åŒ»å­¦ä¸­â€œå°ç»†èƒè‚ºç™Œâ€å’Œâ€œéå°ç»†èƒè‚ºç™Œâ€æ˜¯ä¸¤ç§ä¸åŒçš„ç™Œç—‡

query = "éå°ç»†èƒè‚ºç™Œçš„æ‚£è€…"

documents = [
    "ç›ä¸½æ‚£æœ‰è‚ºç™Œï¼Œç™Œç»†èƒå·²è½¬ç§»",
    "åˆ˜æŸè‚ºç™ŒIæœŸ",
    "å¼ æŸç»è¯Šæ–­ä¸ºéå°ç»†èƒè‚ºç™ŒIIIæœŸ",
    "å°ç»†èƒè‚ºç™Œæ˜¯è‚ºç™Œçš„ä¸€ç§"
]

query_vec = get_embeddings([query])[0]
doc_vecs = get_embeddings(documents)

print("Cosine distance:")
for vec in doc_vecs:
    print(cos_sim(query_vec, vec))
```

**Output:**
```
Cosine distance:
0.8915268056308027
0.8895478505819983
0.9039165614288258
0.9131441645902685
```

æ‰€ä»¥ï¼Œæœ‰æ—¶å€™æˆ‘ä»¬éœ€è¦ç»“åˆä¸åŒçš„æ£€ç´¢ç®—æ³•ï¼Œæ¥è¾¾åˆ°æ¯”å•ä¸€æ£€ç´¢ç®—æ³•æ›´ä¼˜çš„æ•ˆæœã€‚è¿™å°±æ˜¯**æ··åˆæ£€ç´¢**ã€‚

æ··åˆæ£€ç´¢çš„æ ¸å¿ƒæ˜¯ï¼Œç»¼åˆæ–‡æ¡£ $d$ åœ¨ä¸åŒæ£€ç´¢ç®—æ³•ä¸‹çš„æ’åºåæ¬¡ï¼ˆrankï¼‰ï¼Œä¸ºå…¶ç”Ÿæˆæœ€ç»ˆæ’åºã€‚

ä¸€ä¸ªæœ€å¸¸ç”¨çš„ç®—æ³•å« **Reciprocal Rank Fusionï¼ˆRRFï¼‰**

$rrf(d)=\sum_{a\in A}\frac{1}{k+rank_a(d)}$

å…¶ä¸­ $A$ è¡¨ç¤ºæ‰€æœ‰ä½¿ç”¨çš„æ£€ç´¢ç®—æ³•çš„é›†åˆï¼Œ$rank_a(d)$ è¡¨ç¤ºä½¿ç”¨ç®—æ³• $a$ æ£€ç´¢æ—¶ï¼Œæ–‡æ¡£ $d$ çš„æ’åºï¼Œ$k$ æ˜¯ä¸ªå¸¸æ•°ã€‚

å¾ˆå¤šå‘é‡æ•°æ®åº“éƒ½æ”¯æŒæ··åˆæ£€ç´¢ï¼Œæ¯”å¦‚ [Weaviate](https://weaviate.io/blog/hybrid-search-explained)ã€[Pinecone](https://www.pinecone.io/learn/hybrid-search-intro/) ç­‰ã€‚ä¹Ÿå¯ä»¥æ ¹æ®ä¸Šè¿°åŸç†è‡ªå·±å®ç°ã€‚

### 5.3.1ã€æˆ‘ä»¬æ‰‹å†™ä¸ªç®€å•çš„ä¾‹å­

1. åŸºäºå…³é”®å­—æ£€ç´¢çš„æ’åº

```python
import time


class MyEsConnector:
    def __init__(self, es_client, index_name, keyword_fn):
        self.es_client = es_client
        self.index_name = index_name
        self.keyword_fn = keyword_fn

    def add_documents(self, documents):
        '''æ–‡æ¡£çŒåº“'''
        if self.es_client.indices.exists(index=self.index_name):
            self.es_client.indices.delete(index=self.index_name)
        self.es_client.indices.create(index=self.index_name)
        actions = [
            {
                "_index": self.index_name,
                "_source": {
                    "keywords": self.keyword_fn(doc),
                    "text": doc,
                    "id": f"doc_{i}"
                }
            }
            for i, doc in enumerate(documents)
        ]
        helpers.bulk(self.es_client, actions)
        time.sleep(1)

    def search(self, query_string, top_n=3):
        '''æ£€ç´¢'''
        search_query = {
            "match": {
                "keywords": self.keyword_fn(query_string)
            }
        }
        res = self.es_client.search(
            index=self.index_name, query=search_query, size=top_n)
        return {
            hit["_source"]["id"]: {
                "text": hit["_source"]["text"],
                "rank": i,
            }
            for i, hit in enumerate(res["hits"]["hits"])
        }
```

```python
from chinese_utils import to_keywords  # ä½¿ç”¨ä¸­æ–‡çš„å…³é”®å­—æå–å‡½æ•°

# å¼•å…¥é…ç½®æ–‡ä»¶
ELASTICSEARCH_BASE_URL = os.getenv('ELASTICSEARCH_BASE_URL')
ELASTICSEARCH_PASSWORD = os.getenv('ELASTICSEARCH_PASSWORD')
ELASTICSEARCH_NAME= os.getenv('ELASTICSEARCH_NAME')

es = Elasticsearch(
    hosts=[ELASTICSEARCH_BASE_URL],  # æœåŠ¡åœ°å€ä¸ç«¯å£
    http_auth=(ELASTICSEARCH_NAME, ELASTICSEARCH_PASSWORD),  # ç”¨æˆ·åï¼Œå¯†ç 
)


# åˆ›å»º ES è¿æ¥å™¨
es_connector = MyEsConnector(es, "demo_es_rrf", to_keywords)

# æ–‡æ¡£çŒåº“
es_connector.add_documents(documents)

# å…³é”®å­—æ£€ç´¢
keyword_search_results = es_connector.search(query, 3)

print(json.dumps(keyword_search_results, indent=4, ensure_ascii=False))
```

**Output:**
```
{
    "doc_2": {
        "text": "å¼ æŸç»è¯Šæ–­ä¸ºéå°ç»†èƒè‚ºç™ŒIIIæœŸ",
        "rank": 0
    },
    "doc_0": {
        "text": "ç›ä¸½æ‚£æœ‰è‚ºç™Œï¼Œç™Œç»†èƒå·²è½¬ç§»",
        "rank": 1
    },
    "doc_3": {
        "text": "å°ç»†èƒè‚ºç™Œæ˜¯è‚ºç™Œçš„ä¸€ç§",
        "rank": 2
    }
}
```

2. åŸºäºå‘é‡æ£€ç´¢çš„æ’åº

```python
# åˆ›å»ºå‘é‡æ•°æ®åº“è¿æ¥å™¨
vecdb_connector = MyVectorDBConnector("demo_vec_rrf", get_embeddings)

# æ–‡æ¡£çŒåº“
vecdb_connector.add_documents(documents)

# å‘é‡æ£€ç´¢
vector_search_results = {
    "doc_"+str(documents.index(doc)): {
        "text": doc,
        "rank": i
    }
    for i, doc in enumerate(
        vecdb_connector.search(query, 3)["documents"][0]
    )
}  # æŠŠç»“æœè½¬æˆè·Ÿä¸Šé¢å…³é”®å­—æ£€ç´¢ç»“æœä¸€æ ·çš„æ ¼å¼

print(json.dumps(vector_search_results, indent=4, ensure_ascii=False))
```

**Output:**
```
{
    "doc_3": {
        "text": "å°ç»†èƒè‚ºç™Œæ˜¯è‚ºç™Œçš„ä¸€ç§",
        "rank": 0
    },
    "doc_2": {
        "text": "å¼ æŸç»è¯Šæ–­ä¸ºéå°ç»†èƒè‚ºç™ŒIIIæœŸ",
        "rank": 1
    },
    "doc_0": {
        "text": "ç›ä¸½æ‚£æœ‰è‚ºç™Œï¼Œç™Œç»†èƒå·²è½¬ç§»",
        "rank": 2
    }
}
```

3. åŸºäº RRF çš„èåˆæ’åº

```python
def rrf(ranks, k=1):
    ret = {}
    # éå†æ¯æ¬¡çš„æ’åºç»“æœ
    for rank in ranks:
        # éå†æ’åºä¸­æ¯ä¸ªå…ƒç´ 
        for id, val in rank.items():
            if id not in ret:
                ret[id] = {"score": 0, "text": val["text"]}
            # è®¡ç®— RRF å¾—åˆ†
            ret[id]["score"] += 1.0/(k+val["rank"])
    # æŒ‰ RRF å¾—åˆ†æ’åºï¼Œå¹¶è¿”å›
    return dict(sorted(ret.items(), key=lambda item: item[1]["score"], reverse=True))
```

```python
import json

# èåˆä¸¤æ¬¡æ£€ç´¢çš„æ’åºç»“æœ
reranked = rrf([keyword_search_results, vector_search_results])

print(json.dumps(reranked, indent=4, ensure_ascii=False))
```

**Output:**
```
{
    "doc_2": {
        "score": 1.5,
        "text": "å¼ æŸç»è¯Šæ–­ä¸ºéå°ç»†èƒè‚ºç™ŒIIIæœŸ"
    },
    "doc_3": {
        "score": 1.3333333333333333,
        "text": "å°ç»†èƒè‚ºç™Œæ˜¯è‚ºç™Œçš„ä¸€ç§"
    },
    "doc_0": {
        "score": 0.8333333333333333,
        "text": "ç›ä¸½æ‚£æœ‰è‚ºç™Œï¼Œç™Œç»†èƒå·²è½¬ç§»"
    }
}
```

### 5.4ã€RAG-Fusionï¼ˆé€‰ï¼‰

RAG-Fusion å°±æ˜¯åˆ©ç”¨äº† RRF çš„åŸç†æ¥æå‡æ£€ç´¢çš„å‡†ç¡®æ€§ã€‚

![image](./assets/05-rag-embeddings/rag-fusion.jpeg)

åŸå§‹é¡¹ç›®ï¼ˆä¸€æ®µéå¸¸ç®€çŸ­çš„æ¼”ç¤ºä»£ç ï¼‰ï¼šhttps://github.com/Raudaschl/rag-fusion

## å…­ã€å‘é‡æ¨¡å‹çš„æœ¬åœ°åŠ è½½ä¸è¿è¡Œ

> âŒ **Warning:** ä»¥ä¸‹ä»£ç ä¸è¦åœ¨æœåŠ¡å™¨ä¸Šè¿è¡Œï¼Œä¼šæ­»æœºï¼å¯ä¸‹è½½å·¦ä¾§ bge.py åœ¨è‡ªå·±æœ¬åœ°è¿è¡Œã€‚

> âš ï¸ **Note:** å¤‡æ³¨ï¼š
ç”±äº huggingface è¢«å¢™ï¼Œæˆ‘ä»¬å·²ç»ä¸ºæ‚¨å‡†å¤‡å¥½äº†æœ¬ç« ç›¸å…³æ¨¡å‹ã€‚è¯·ç‚¹å‡»ä»¥ä¸‹ç½‘ç›˜é“¾æ¥è¿›è¡Œä¸‹è½½ï¼š
    
é“¾æ¥: https://pan.baidu.com/s/1X0kfNKasvWqCLUEEyAvO-Q?pwd=3v6y æå–ç : 3v6y


```python
from sentence_transformers import SentenceTransformer

model_name = 'BAAI/bge-large-zh-v1.5' #ä¸­æ–‡
# model_name = 'moka-ai/m3e-base'  # ä¸­è‹±åŒè¯­ï¼Œä½†æ•ˆæœä¸€èˆ¬
# model_name = 'BAAI/bge-m3' # å¤šè¯­è¨€ï¼Œä½†æ•ˆæœä¸€èˆ¬

model = SentenceTransformer(model_name)
```

```python
query = "å›½é™…äº‰ç«¯"
# query = "global conflicts"

documents = [
    "è”åˆå›½å°±è‹ä¸¹è¾¾å°”å¯Œå°”åœ°åŒºå¤§è§„æ¨¡æš´åŠ›äº‹ä»¶å‘å‡ºè­¦å‘Š",
    "åœŸè€³å…¶ã€èŠ¬å…°ã€ç‘å…¸ä¸åŒ—çº¦ä»£è¡¨å°†ç»§ç»­å°±ç‘å…¸â€œå…¥çº¦â€é—®é¢˜è¿›è¡Œè°ˆåˆ¤",
    "æ—¥æœ¬å²é˜œå¸‚é™†ä¸Šè‡ªå«é˜Ÿå°„å‡»åœºå†…å‘ç”Ÿæªå‡»äº‹ä»¶ 3äººå—ä¼¤",
    "å›½å®¶æ¸¸æ³³ä¸­å¿ƒï¼ˆæ°´ç«‹æ–¹ï¼‰ï¼šæ¢å¤æ¸¸æ³³ã€å¬‰æ°´ä¹å›­ç­‰æ°´ä¸Šé¡¹ç›®è¿è¥",
    "æˆ‘å›½é¦–æ¬¡åœ¨ç©ºé—´ç«™å¼€å±•èˆ±å¤–è¾å°„ç”Ÿç‰©å­¦æš´éœ²å®éªŒ",
]

query_vec = model.encode(query)

doc_vecs = [
    model.encode(doc)
    for doc in documents
]

print("Cosine distance:")  # è¶Šå¤§è¶Šç›¸ä¼¼
# print(cos_sim(query_vec, query_vec))
for vec in doc_vecs:
    print(cos_sim(query_vec, vec))
```

**Output:**
```
Cosine distance:
0.4727645
0.38867012
0.3285629
0.316192
0.30938625
```

> â„¹ï¸ **Info:** æ‰©å±•é˜…è¯»ï¼šhttps://github.com/FlagOpen/FlagEmbedding

> âœ… **Tip:** åˆ’é‡ç‚¹ï¼š
    
        ä¸æ˜¯æ¯ä¸ª Embedding æ¨¡å‹éƒ½å¯¹ä½™å¼¦è·ç¦»å’Œæ¬§æ°è·ç¦»åŒæ—¶æœ‰æ•ˆ
        å“ªç§ç›¸ä¼¼åº¦è®¡ç®—æœ‰æ•ˆè¦é˜…è¯»æ¨¡å‹çš„è¯´æ˜ï¼ˆé€šå¸¸éƒ½æ”¯æŒä½™å¼¦è·ç¦»è®¡ç®—ï¼‰

> âš ï¸ **Note:** æ³¨æ„ï¼š 
    
        æœ¬èŠ‚åªä»‹ç»äº†æ¨¡å‹åœ¨æœ¬åœ°å¦‚ä½•åŠ è½½ä¸è¿è¡Œã€‚
        å…³äºå¦‚ä½•å°†æ¨¡å‹éƒ¨ç½²æˆæ”¯æŒå¹¶å‘è¯·æ±‚çš„ HTTP æœåŠ¡ï¼Œå°†åœ¨ç¬¬15è¯¾ä¸­è®²è§£ã€‚

## ä¸ƒã€PDF æ–‡æ¡£ä¸­çš„è¡¨æ ¼æ€ä¹ˆå¤„ç†ï¼ˆé€‰ï¼‰

![image](./assets/05-rag-embeddings/table_rag.png)

1. å°†æ¯é¡µ PDF è½¬æˆå›¾ç‰‡

```python
# !pip install PyMuPDF
# !pip install matplotlib
```

```python
import os
import fitz
from PIL import Image

def pdf2images(pdf_file):
    '''å°† PDF æ¯é¡µè½¬æˆä¸€ä¸ª PNG å›¾åƒ'''
    # ä¿å­˜è·¯å¾„ä¸ºåŸ PDF æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
    output_directory_path, _ = os.path.splitext(pdf_file)
    
    if not os.path.exists(output_directory_path):
        os.makedirs(output_directory_path)
    
    # åŠ è½½ PDF æ–‡ä»¶
    pdf_document = fitz.open(pdf_file)
    
    # æ¯é¡µè½¬ä¸€å¼ å›¾
    for page_number in range(pdf_document.page_count):
        # å–ä¸€é¡µ
        page = pdf_document[page_number]
    
        # è½¬å›¾åƒ
        pix = page.get_pixmap()
    
        # ä»ä½å›¾åˆ›å»º PNG å¯¹è±¡
        image = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
    
        # ä¿å­˜ PNG æ–‡ä»¶
        image.save(f"./{output_directory_path}/page_{page_number + 1}.png")
    
    # å…³é—­ PDF æ–‡ä»¶
    pdf_document.close()
```

```python
from PIL import Image
import os
import matplotlib.pyplot as plt

def show_images(dir_path):
    '''æ˜¾ç¤ºç›®å½•ä¸‹çš„ PNG å›¾åƒ'''
    for file in os.listdir(dir_path):
        if file.endswith('.png'):
            # æ‰“å¼€å›¾åƒ
            img = Image.open(os.path.join(dir_path, file)) 

            # æ˜¾ç¤ºå›¾åƒ
            plt.imshow(img)
            plt.axis('off')  # ä¸æ˜¾ç¤ºåæ ‡è½´
            plt.show()
```

```python
pdf2images("llama2_page8.pdf")
show_images("llama2_page8")
```

2. è¯†åˆ«æ–‡æ¡£ï¼ˆå›¾ç‰‡ï¼‰ä¸­çš„è¡¨æ ¼

```python
class MaxResize(object):
    '''ç¼©æ”¾å›¾åƒ'''
    def __init__(self, max_size=800):
        self.max_size = max_size

    def __call__(self, image):
        width, height = image.size
        current_max_size = max(width, height)
        scale = self.max_size / current_max_size
        resized_image = image.resize(
            (int(round(scale * width)), int(round(scale * height)))
        )

        return resized_image
```

```python
import torchvision.transforms as transforms

# å›¾åƒé¢„å¤„ç†
detection_transform = transforms.Compose(
    [
        MaxResize(800),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    ]
)
```

> âŒ **Warning:** ä»¥ä¸‹ä»£ç ä¸è¦åœ¨æœåŠ¡å™¨ä¸Šè¿è¡Œï¼Œä¼šæ­»æœºï¼å¯ä¸‹è½½å·¦ä¾§ table_detection.py åœ¨è‡ªå·±æœ¬åœ°è¿è¡Œã€‚

> âš ï¸ **Note:** å¤‡æ³¨ï¼š
ç”±äº huggingface è¢«å¢™ï¼Œæˆ‘ä»¬å·²ç»ä¸ºæ‚¨å‡†å¤‡å¥½äº†æœ¬ç« ç›¸å…³æ¨¡å‹ã€‚è¯·ç‚¹å‡»ä»¥ä¸‹ç½‘ç›˜é“¾æ¥è¿›è¡Œä¸‹è½½ï¼š
    
é“¾æ¥: https://pan.baidu.com/s/1X0kfNKasvWqCLUEEyAvO-Q?pwd=3v6y æå–ç : 3v6y


```python
from transformers import AutoModelForObjectDetection

# åŠ è½½ TableTransformer æ¨¡å‹
model = AutoModelForObjectDetection.from_pretrained(
    "microsoft/table-transformer-detection"
)
```

```python
# è¯†åˆ«åçš„åæ ‡æ¢ç®—ä¸åå¤„ç†

def box_cxcywh_to_xyxy(x):
    '''åæ ‡è½¬æ¢'''
    x_c, y_c, w, h = x.unbind(-1)
    b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]
    return torch.stack(b, dim=1)


def rescale_bboxes(out_bbox, size):
    '''åŒºåŸŸç¼©æ”¾'''
    width, height = size
    boxes = box_cxcywh_to_xyxy(out_bbox)
    boxes = boxes * torch.tensor(
        [width, height, width, height], dtype=torch.float32
    )
    return boxes


def outputs_to_objects(outputs, img_size, id2label):
    '''ä»æ¨¡å‹è¾“å‡ºä¸­å–å®šä½æ¡†åæ ‡'''
    m = outputs.logits.softmax(-1).max(-1)
    pred_labels = list(m.indices.detach().cpu().numpy())[0]
    pred_scores = list(m.values.detach().cpu().numpy())[0]
    pred_bboxes = outputs["pred_boxes"].detach().cpu()[0]
    pred_bboxes = [
        elem.tolist() for elem in rescale_bboxes(pred_bboxes, img_size)
    ]

    objects = []
    for label, score, bbox in zip(pred_labels, pred_scores, pred_bboxes):
        class_label = id2label[int(label)]
        if not class_label == "no object":
            objects.append(
                {
                    "label": class_label,
                    "score": float(score),
                    "bbox": [float(elem) for elem in bbox],
                }
            )

    return objects
```

```python
import torch

# è¯†åˆ«è¡¨æ ¼ï¼Œå¹¶å°†è¡¨æ ¼éƒ¨åˆ†å•ç‹¬å­˜ä¸ºå›¾åƒæ–‡ä»¶

def detect_and_crop_save_table(file_path):
    # åŠ è½½å›¾åƒï¼ˆPDFé¡µï¼‰    
    image = Image.open(file_path)

    filename, _ = os.path.splitext(os.path.basename(file_path))

    # è¾“å‡ºè·¯å¾„
    cropped_table_directory = os.path.join(os.path.dirname(file_path), "table_images")

    if not os.path.exists(cropped_table_directory):
        os.makedirs(cropped_table_directory)

    # é¢„å¤„ç†
    pixel_values = detection_transform(image).unsqueeze(0)

    # è¯†åˆ«è¡¨æ ¼
    with torch.no_grad():
        outputs = model(pixel_values)

    # åå¤„ç†ï¼Œå¾—åˆ°è¡¨æ ¼å­åŒºåŸŸ
    id2label = model.config.id2label
    id2label[len(model.config.id2label)] = "no object"
    detected_tables = outputs_to_objects(outputs, image.size, id2label)

    print(f"number of tables detected {len(detected_tables)}")

    for idx in range(len(detected_tables)):
        # å°†è¯†åˆ«ä»çš„è¡¨æ ¼åŒºåŸŸå•ç‹¬å­˜ä¸ºå›¾åƒ
        cropped_table = image.crop(detected_tables[idx]["bbox"])
        cropped_table.save(os.path.join(cropped_table_directory,f"{filename}_{idx}.png"))
```

```python
detect_and_crop_save_table("llama2_page8/page_1.png")
show_images("llama2_page8/table_images")
```

**Output:**
```
number of tables detected 2
```

3. åŸºäº GPT-4 Vision API åšè¡¨æ ¼é—®ç­”

```python
import base64
from openai import OpenAI

client = OpenAI()

def encode_image(image_path):
  with open(image_path, "rb") as image_file:
    return base64.b64encode(image_file.read()).decode('utf-8')

def image_qa(query, image_path):
    base64_image = encode_image(image_path)
    response = client.chat.completions.create(
        model="gpt-4o",
        temperature=0,
        seed=42,
        messages=[{
            "role": "user",
              "content": [
                  {"type": "text", "text": query},
                  {
                      "type": "image_url",
                      "image_url": {
                          "url": f"data:image/jpeg;base64,{base64_image}",
                      },
                  },
              ],
        }],
    )

    return response.choices[0].message.content
```

```python
response = image_qa("å“ªä¸ªæ¨¡å‹åœ¨AGI Evalæ•°æ®é›†ä¸Šè¡¨ç°æœ€å¥½ã€‚å¾—åˆ†å¤šå°‘","llama2_page8/table_images/page_1_0.png")
print(response)
```

**Output:**
```
åœ¨AGI Evalæ•°æ®é›†ä¸Šè¡¨ç°æœ€å¥½çš„æ¨¡å‹æ˜¯LLaMA 2 70Bï¼Œå¾—åˆ†ä¸º54.2ã€‚
```

4. ç”¨ GPT-4 Vision ç”Ÿæˆè¡¨æ ¼ï¼ˆå›¾åƒï¼‰æè¿°ï¼Œå¹¶å‘é‡åŒ–ç”¨äºæ£€ç´¢

```python
import chromadb
from chromadb.config import Settings


class NewVectorDBConnector:
    def __init__(self, collection_name, embedding_fn):
        chroma_client = chromadb.Client(Settings(allow_reset=True))

        # ä¸ºäº†æ¼”ç¤ºï¼Œå®é™…ä¸éœ€è¦æ¯æ¬¡ reset()
        chroma_client.reset()

        # åˆ›å»ºä¸€ä¸ª collection
        self.collection = chroma_client.get_or_create_collection(
            name=collection_name)
        self.embedding_fn = embedding_fn

    def add_documents(self, documents):
        '''å‘ collection ä¸­æ·»åŠ æ–‡æ¡£ä¸å‘é‡'''
        self.collection.add(
            embeddings=self.embedding_fn(documents),  # æ¯ä¸ªæ–‡æ¡£çš„å‘é‡
            documents=documents,  # æ–‡æ¡£çš„åŸæ–‡
            ids=[f"id{i}" for i in range(len(documents))]  # æ¯ä¸ªæ–‡æ¡£çš„ id
        )

    def add_images(self, image_paths):
        '''å‘ collection ä¸­æ·»åŠ å›¾åƒ'''
        documents = [
            image_qa("è¯·ç®€è¦æè¿°å›¾ç‰‡ä¸­çš„ä¿¡æ¯",image)
            for image in image_paths
        ]
        self.collection.add(
            embeddings=self.embedding_fn(documents),  # æ¯ä¸ªæ–‡æ¡£çš„å‘é‡
            documents=documents,  # æ–‡æ¡£çš„åŸæ–‡
            ids=[f"id{i}" for i in range(len(documents))],  # æ¯ä¸ªæ–‡æ¡£çš„ id
            metadatas=[{"image": image} for image in image_paths] # ç”¨ metadata æ ‡è®°æºå›¾åƒè·¯å¾„
        )

    def search(self, query, top_n):
        '''æ£€ç´¢å‘é‡æ•°æ®åº“'''
        results = self.collection.query(
            query_embeddings=self.embedding_fn([query]),
            n_results=top_n
        )
        return results
```

```python
images = []
dir_path = "llama2_page8/table_images"
for file in os.listdir(dir_path):
    if file.endswith('.png'):
        # æ‰“å¼€å›¾åƒ
        images.append(os.path.join(dir_path, file))

new_db_connector = NewVectorDBConnector("table_demo",get_embeddings)
new_db_connector.add_images(images)
```

```python
query  = "å“ªä¸ªæ¨¡å‹åœ¨AGI Evalæ•°æ®é›†ä¸Šè¡¨ç°æœ€å¥½ã€‚å¾—åˆ†å¤šå°‘"

results = new_db_connector.search(query, 1)
metadata = results["metadatas"][0]
print("====æ£€ç´¢ç»“æœ====")
print(metadata)
print("====å›å¤====")
response = image_qa(query,metadata[0]["image"])
print(response)
```

**Output:**
```
====æ£€ç´¢ç»“æœ====
[{'image': 'llama2_page8/table_images/page_1_0.png'}]
====å›å¤====
åœ¨AGI Evalæ•°æ®é›†ä¸Šè¡¨ç°æœ€å¥½çš„æ¨¡å‹æ˜¯LLaMA 2 70Bï¼Œå¾—åˆ†ä¸º54.2ã€‚
```

### ä¸€äº›é¢å‘ RAG çš„æ–‡æ¡£è§£æè¾…åŠ©å·¥å…·

- [PyMuPDF](https://pymupdf.readthedocs.io/en/latest/): PDF æ–‡ä»¶å¤„ç†åŸºç¡€åº“ï¼Œå¸¦æœ‰åŸºäºè§„åˆ™çš„è¡¨æ ¼ä¸å›¾åƒæŠ½å–ï¼ˆä¸å‡†ï¼‰
- [RAGFlow](https://github.com/infiniflow/ragflow): ä¸€æ¬¾åŸºäºæ·±åº¦æ–‡æ¡£ç†è§£æ„å»ºçš„å¼€æº RAG å¼•æ“ï¼Œæ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼
- [Unstructured.io](https://unstructured.io/): ä¸€ä¸ªå¼€æº+SaaSå½¢å¼çš„æ–‡æ¡£è§£æåº“ï¼Œæ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼
- [LlamaParse](https://docs.llamaindex.ai/en/stable/llama_cloud/llama_parse/)ï¼šä»˜è´¹ API æœåŠ¡ï¼Œç”± LlamaIndex å®˜æ–¹æä¾›ï¼Œè§£æä¸ä¿è¯100%å‡†ç¡®ï¼Œå®æµ‹å¶æœ‰æ–‡å­—ä¸¢å¤±æˆ–é”™ä½å‘ç”Ÿ
- [Mathpix](https://mathpix.com/)ï¼šä»˜è´¹ API æœåŠ¡ï¼Œæ•ˆæœè¾ƒå¥½ï¼Œå¯è§£ææ®µè½ç»“æ„ã€è¡¨æ ¼ã€å…¬å¼ç­‰ï¼Œè´µï¼

åœ¨å·¥ç¨‹ä¸Šï¼ŒPDF è§£ææœ¬èº«æ˜¯ä¸ªå¤æ‚ä¸”çç¢çš„å·¥ä½œã€‚ä»¥ä¸Šå·¥å…·éƒ½ä¸å®Œç¾ï¼Œå»ºè®®åœ¨è‡ªå·±å®é™…åœºæ™¯æµ‹è¯•åé€‰æ‹©ä½¿ç”¨ã€‚

### å…«ã€è¯´è¯´ GraphRAG

![image](./assets/05-rag-embeddings/GraphRAG.png)

1. **ä»€ä¹ˆæ˜¯ GraphRAG**ï¼šæ ¸å¿ƒæ€æƒ³æ˜¯å°†çŸ¥è¯†é¢„å…ˆå¤„ç†æˆçŸ¥è¯†å›¾è°±
2. **ä¼˜ç‚¹**ï¼šé€‚åˆå¤æ‚é—®é¢˜ï¼Œå°¤å…¶æ˜¯ä»¥æŸ¥è¯¢ä¸ºä¸­å¿ƒçš„æ€»ç»“ï¼Œä¾‹å¦‚ï¼šâ€œXXXå›¢é˜Ÿå»å¹´æœ‰å“ªäº›è´¡çŒ®â€
3. **ç¼ºç‚¹**ï¼šçŸ¥è¯†å›¾è°±çš„æ„å»ºã€æ¸…æ´—ã€ç»´æŠ¤æ›´æ–°ç­‰éƒ½æœ‰å¯è§‚çš„æˆæœ¬
4. **å»ºè®®**ï¼š
   - GraphRAG ä¸æ˜¯ä¸‡èƒ½è‰¯è¯
   - é¢†ä¼šå…¶æ ¸å¿ƒæ€æƒ³
   - é‡åˆ°ä¼ ç»Ÿ RAG æ— è®ºå¦‚ä½•ä¼˜åŒ–éƒ½ä¸å¥½è§£å†³çš„é—®é¢˜æ—¶ï¼Œé…Œæƒ…ä½¿ç”¨

## æ€»ç»“

### RAG çš„æµç¨‹

- ç¦»çº¿æ­¥éª¤ï¼š
  1. æ–‡æ¡£åŠ è½½
  2. æ–‡æ¡£åˆ‡åˆ†
  3. å‘é‡åŒ–
  4. çŒå…¥å‘é‡æ•°æ®åº“
- åœ¨çº¿æ­¥éª¤ï¼š
  1. è·å¾—ç”¨æˆ·é—®é¢˜
  2. ç”¨æˆ·é—®é¢˜å‘é‡åŒ–
  3. æ£€ç´¢å‘é‡æ•°æ®åº“
  4. å°†æ£€ç´¢ç»“æœå’Œç”¨æˆ·é—®é¢˜å¡«å…¥ Prompt æ¨¡ç‰ˆ
  5. ç”¨æœ€ç»ˆè·å¾—çš„ Prompt è°ƒç”¨ LLM
  6. ç”± LLM ç”Ÿæˆå›å¤

### æˆ‘ç”¨äº†ä¸€ä¸ªå¼€æºçš„ RAGï¼Œä¸å¥½ä½¿æ€ä¹ˆåŠï¼Ÿ

1. æ£€æŸ¥é¢„å¤„ç†æ•ˆæœï¼šæ–‡æ¡£åŠ è½½æ˜¯å¦æ­£ç¡®ï¼Œåˆ‡å‰²çš„æ˜¯å¦åˆç†
2. æµ‹è¯•æ£€ç´¢æ•ˆæœï¼šé—®é¢˜æ£€ç´¢å›æ¥çš„æ–‡æœ¬ç‰‡æ®µæ˜¯å¦åŒ…å«ç­”æ¡ˆ
3. æµ‹è¯•å¤§æ¨¡å‹èƒ½åŠ›ï¼šç»™å®šé—®é¢˜å’ŒåŒ…å«ç­”æ¡ˆæ–‡æœ¬ç‰‡æ®µçš„å‰æä¸‹ï¼Œå¤§æ¨¡å‹èƒ½ä¸èƒ½æ­£ç¡®å›ç­”é—®é¢˜

## ä½œä¸š

åšä¸ªè‡ªå·±çš„ ChatPDFã€‚éœ€æ±‚ï¼š

1. ä»æœ¬åœ°åŠ è½½ PDF æ–‡ä»¶ï¼ŒåŸºäº PDF çš„å†…å®¹å¯¹è¯
2. å¯ä»¥æ— å‰ç«¯ï¼Œåªè¦èƒ½åœ¨å‘½ä»¤è¡Œè¿è¡Œå°±è¡Œ
3. å…¶å®ƒéšæ„å‘æŒ¥